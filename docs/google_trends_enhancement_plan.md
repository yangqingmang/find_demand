# Google Trends 数据采集优化方案

## 1. 背景与目标

当前的需求挖掘工作流（特别是 `--all` 完整流程）在获取初始热门关键词时，对 Google Trends API 存在高频率、深度的依赖。此模式虽然能深度挖掘单个种子词，但极易触发 Google 的速率限制（Rate Limiting），导致IP被临时屏蔽，从而严重影响数据采集的稳定性和关键词覆盖广度。

本方案旨在优化 Google Trends 的数据采集策略，核心目标是：

**在不牺牲（甚至提升）关键词数量和质量的前提下，显著降低 API 请求频率，规避速率限制，提升工作流的健壮性和效率。**

核心指导思想：**从“深度挖掘”转向“广度撒网”，然后进行本地化的高效过滤。**

---

## 2. 核心问题分析

当前流程的主要瓶颈在于其“串联深度挖掘”模式：

`种子词A -> [相关查询API -> 相关主题API -> 搜索建议API] -> 种子词B -> [相关查询API -> ...] `

这种模式下，API请求次数与种子词数量成正比关系，请求量巨大且在短时间内集中爆发，是触发速率限制的主要原因。

---

## 3. 四大优化策略

### 策略一：利用“每日/实时热搜榜”进行批量获取

*   **工作原理**：直接请求 Google Trends 提供的“每日搜索趋势” (Daily Search Trends) 或“实时搜索趋势” (Realtime Search Trends) 的API端点。这些端点通常一次性返回20-50个当前热度最高的关键词。
*   **优势**：
    *   **极高的请求效率**：一次API请求可换取数十个高质量、自带热度属性的关键词。
    *   **数据源质量高**：返回的词本身就是经过Google验证的公众热点。
*   **实施建议**：
    *   在 `src/collectors/trends_collector.py` 中，实现一个新方法，如 `fetch_daily_trends(region='US')`。

### 策略二：从“一个宽泛的种子词”扩展

*   **工作原理**：放弃对多个具体长尾词的迭代请求，转而对一个极其宽泛的行业核心词（例如，只用 "AI" 而不是 "AI image generator", "AI video editor" 等多个词）发起一次“相关查询” (Related Queries) 请求。返回的结果列表里，通常已经包含了我们需要的各种热门长尾词。
*   **优势**：
    *   **API请求降维**：将N次API请求缩减为1次。
    *   **关键词多样性**：能发现意料之外的相关词，扩大挖掘的广度。
*   **实施建议**：
    *   修改工作流，使其可以接受一个或几个宽泛的“行业词”作为初始种子。
    *   调用 `get_related_queries` 时，设置获取尽可能多的结果。
    *   获取到大的结果集后，在本地内存中进行二次处理和过滤。

### 策略三：丰富第三方趋势聚合源

*   **工作原理**：当前流程已经集成了 `TrendingKeywords.net`，这是一个很好的实践。我们可以进一步集成更多类似的第三方趋势聚合网站（如 Exploding Topics, Glimpse 等）。这些网站本身就在持续聚合和处理Google Trends数据。
*   **优势**：
    *   **零API消耗**：完全不消耗我们自己的Google API请求配额。
    *   **数据预处理**：获取到的数据通常是经过筛选和分类的，质量较高。
*   **实施建议**：
    *   在 `src/collectors/` 目录下，仿照 `trending_keywords_collector.py`，实现一到两个新的采集器，用于从其他趋势网站获取数据。

### 策略四：优化请求调度与缓存策略

这是对现有流程的直接加固。

*   **智能缓存**：
    *   **缓存目标**：应缓存**每一次原始的API请求结果**，而不仅仅是处理后的数据。
    *   **缓存逻辑**：在发起任何对Google的API请求前，先根据“请求URL和参数”生成一个唯一的缓存键（Cache Key），检查该键是否存在且未过期。如果存在，直接从缓存读取原始响应，避免网络请求。
    *   **缓存周期**：为与趋势相关的缓存设置一个合理的有效期（TTL, Time-To-Live），例如6-12小时。

*   **请求合并与延迟执行**：
    *   **工作原理**：在程序需要请求API时，不是立即执行，而是将请求任务（关键词、类型等）放入一个队列中。由一个统一的“请求调度器”来消费这个队列，该调度器严格遵守速率限制（例如，强制保证每秒最多发出N次请求）。
    *   **实施建议**：确保项目中所有的Google API调用都通过 `src/utils/request_rate_limiter.py` 来执行，并为其配置一个保守的全局速率（如：1-2次/秒）。

---

## 4. 建议的组合实施方案 (`--all` 流程重构)

建议将以上策略组合，对 `--all` 工作流进行如下重构：

1.  **第一阶段：广度撒网 (API请求 < 5次)**
    *   **主要来源**：调用一次“每日热搜榜”API（策略一），获取一批基础热词。
    *   **次要来源**：调用一到两次“第三方聚合网站”采集器（策略三），获取另一批趋势词。
    *   **可选补充**：允许用户传入1-2个“宽泛种子词”，调用一次“相关查询”API（策略二）。
    *   **合并与去重**：将所有来源的关键词在内存中合并，形成一个大的、无重复的初始候选池。

2.  **第二阶段：本地分析与种子筛选 (0次API请求)**
    *   对第一阶段产生的数百个候选词，在本地执行初步的 `analyze_keywords` 分析。**注意：此阶段的分析应禁用所有需要网络请求的子模块（如SERP分析、新词检测等）**，只进行基于本地规则的意图和价值评估。
    *   根据本地分析的机会分数，筛选出得分最高的 Top 10-20 个关键词，作为下一阶段的“黄金种子”。

3.  **第三阶段：深度挖掘与最终分析 (API请求集中于高价值目标)**
    *   使用“黄金种子”启动现有的多平台发现流程 (`MultiPlatformKeywordDiscovery`)。
    *   对多平台发现的结果，执行一次**完整且深入**的 `analyze_keywords` 分析，这次可以开启所有分析模块（包括新词检测和SERP分析），因为目标关键词已经经过了层层筛选，数量可控且价值更高。

---

## 5. 预期收益

*   **稳定性大幅提升**：API请求总数从可能的三位数降低到可控的个位数或两位数，极大降低了被速率限制的风险。
*   **效率提升**：通过批量获取和内存处理，减少了大量的网络延迟和不必要的磁盘I/O。
*   **关键词质量和广度提升**：数据来源更加多样化，更容易发现意想不到的跨界热点和需求。
*   **流程更健壮**：即使某个数据源暂时失效，其他数据源仍能保证工作流产出基础结果。