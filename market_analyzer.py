# market_analyzer.py
# å¸‚åœºéœ€æ±‚åˆ†æå·¥å…·é›†ä¸»è„šæœ¬
import argparse
import os
import sys
import time
import json
from datetime import datetime
import pandas as pd

# å¯¼å…¥å„ä¸ªæ¨¡å—
try:
    from trends_collector import TrendsCollector
    from keyword_scorer import KeywordScorer
    from intent_analyzer import IntentAnalyzer
except ImportError as e:
    print(f"é”™è¯¯: æ— æ³•å¯¼å…¥å¿…è¦æ¨¡å— - {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å—æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

class MarketAnalyzer:
    """å¸‚åœºéœ€æ±‚åˆ†æå·¥å…·é›†ä¸»ç±»"""
    
    def __init__(self, output_dir='data'):
        """
        åˆå§‹åŒ–å¸‚åœºåˆ†æå™¨
        
        å‚æ•°:
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.date_str = datetime.now().strftime('%Y-%m-%d')
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.trends_collector = TrendsCollector()
        self.keyword_scorer = KeywordScorer()
        self.intent_analyzer = IntentAnalyzer()
        
        # æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(output_dir, f'analysis_log_{self.date_str}.txt')
        
    def log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def run_analysis(self, keywords, geo='', timeframe='today 3-m', 
                    volume_weight=0.4, growth_weight=0.4, kd_weight=0.2,
                    min_score=None, enrich=True):
        """
        è¿è¡Œå®Œæ•´çš„å¸‚åœºéœ€æ±‚åˆ†ææµç¨‹
        
        å‚æ•°:
            keywords (list): è¦åˆ†æçš„å…³é”®è¯åˆ—è¡¨
            geo (str): åœ°åŒºä»£ç 
            timeframe (str): æ—¶é—´èŒƒå›´
            volume_weight (float): æœç´¢é‡æƒé‡
            growth_weight (float): å¢é•¿ç‡æƒé‡
            kd_weight (float): å…³é”®è¯éš¾åº¦æƒé‡
            min_score (int): æœ€ä½è¯„åˆ†è¿‡æ»¤
            enrich (bool): æ˜¯å¦ä¸°å¯Œæ•°æ®
            
        è¿”å›:
            dict: åˆ†æç»“æœæ‘˜è¦
        """
        self.log(f"å¼€å§‹å¸‚åœºéœ€æ±‚åˆ†æ - å…³é”®è¯: {keywords}, åœ°åŒº: {geo or 'å…¨çƒ'}")
        start_time = time.time()
        
        # æ­¥éª¤1: è·å–Google Trendsæ•°æ®
        self.log("æ­¥éª¤1: è·å–Google Trendsæ•°æ®")
        trends_results = self.trends_collector.fetch_multiple_keywords(keywords, geo, timeframe)
        
        if not trends_results:
            self.log("è­¦å‘Š: æœªè·å–åˆ°ä»»ä½•Google Trendsæ•°æ®")
            return {"error": "æœªè·å–åˆ°Google Trendsæ•°æ®"}
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        trends_df = pd.concat(trends_results.values(), ignore_index=True) if trends_results else pd.DataFrame()
        
        if trends_df.empty:
            self.log("è­¦å‘Š: åˆå¹¶åçš„Google Trendsæ•°æ®ä¸ºç©º")
            return {"error": "Google Trendsæ•°æ®ä¸ºç©º"}
        
        # ä¿å­˜Trendsç»“æœ
        trends_file = os.path.join(self.output_dir, f'trends_all_{self.date_str}.csv')
        trends_df.to_csv(trends_file, index=False, encoding='utf-8-sig')
        self.log(f"å·²ä¿å­˜Google Trendsæ•°æ®åˆ°: {trends_file}")
        
        # æ­¥éª¤2: å…³é”®è¯è¯„åˆ†
        self.log("æ­¥éª¤2: å…³é”®è¯è¯„åˆ†")
        
        # ä¸°å¯Œæ•°æ®ï¼ˆå¯é€‰ï¼‰
        if enrich:
            self.log("æ­£åœ¨ä¸°å¯Œå…³é”®è¯æ•°æ®...")
            trends_df = self.keyword_scorer.enrich_with_ads_data(trends_df)
        
        # è¯„åˆ†
        scored_df = self.keyword_scorer.score_keywords(
            trends_df, 
            volume_col='value', 
            growth_col='growth' if 'growth' in trends_df.columns else None
        )
        
        # è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        if min_score:
            scored_df = self.keyword_scorer.filter_keywords(scored_df, min_score=min_score)
            self.log(f"è¿‡æ»¤åå‰©ä½™ {len(scored_df)} æ¡å…³é”®è¯")
        
        # ä¿å­˜è¯„åˆ†ç»“æœ
        scored_file = os.path.join(self.output_dir, f'scored_{self.date_str}.csv')
        scored_df.to_csv(scored_file, index=False, encoding='utf-8-sig')
        self.log(f"å·²ä¿å­˜è¯„åˆ†ç»“æœåˆ°: {scored_file}")
        
        # ä¿å­˜é«˜åˆ†å…³é”®è¯
        high_score_df = scored_df[scored_df['score'] >= 70].sort_values('score', ascending=False)
        if not high_score_df.empty:
            high_score_file = os.path.join(self.output_dir, f'scored_high_score_{self.date_str}.csv')
            high_score_df.to_csv(high_score_file, index=False, encoding='utf-8-sig')
            self.log(f"å·²ä¿å­˜é«˜åˆ†å…³é”®è¯ ({len(high_score_df)}ä¸ª) åˆ°: {high_score_file}")
        
        # æ­¥éª¤3: æœç´¢æ„å›¾åˆ†æ
        self.log("æ­¥éª¤3: æœç´¢æ„å›¾åˆ†æ")
        result_df = self.intent_analyzer.analyze_keywords(scored_df)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self.intent_analyzer.generate_intent_summary(result_df)
        
        # ä¿å­˜æ„å›¾åˆ†æç»“æœ
        intent_file = os.path.join(self.output_dir, f'intent_{self.date_str}.csv')
        result_df.to_csv(intent_file, index=False, encoding='utf-8-sig')
        self.log(f"å·²ä¿å­˜æ„å›¾åˆ†æç»“æœåˆ°: {intent_file}")
        
        # ä¿å­˜æ‘˜è¦ä¸ºJSON
        summary_file = os.path.join(self.output_dir, f'intent_summary_{self.date_str}.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        self.log(f"å·²ä¿å­˜æ„å›¾åˆ†ææ‘˜è¦åˆ°: {summary_file}")
        
        # æŒ‰æ„å›¾åˆ†ç»„ä¿å­˜
        for intent, keywords in summary['intent_keywords'].items():
            if keywords:
                intent_df = result_df[result_df['intent'] == intent]
                intent_path = os.path.join(self.output_dir, f'intent_{intent}_{self.date_str}.csv')
                intent_df.to_csv(intent_path, index=False, encoding='utf-8-sig')
                self.log(f"å·²ä¿å­˜ {intent} ({self.intent_analyzer.INTENT_TYPES[intent]}) æ„å›¾å…³é”®è¯åˆ°: {intent_path}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        end_time = time.time()
        duration = round(end_time - start_time, 2)
        
        report = {
            "åˆ†ææ—¥æœŸ": self.date_str,
            "åˆ†æå…³é”®è¯": keywords,
            "åœ°åŒº": geo or "å…¨çƒ",
            "æ—¶é—´èŒƒå›´": timeframe,
            "åˆ†æè€—æ—¶(ç§’)": duration,
            "å…³é”®è¯æ€»æ•°": len(result_df),
            "é«˜åˆ†å…³é”®è¯æ•°": len(high_score_df) if 'high_score_df' in locals() else 0,
            "æ„å›¾åˆ†å¸ƒ": summary['intent_percentages'],
            "Top5å…³é”®è¯": result_df.sort_values('score', ascending=False).head(5)[['query', 'score', 'intent']].to_dict('records'),
            "è¾“å‡ºæ–‡ä»¶": {
                "Google Trendsæ•°æ®": trends_file,
                "è¯„åˆ†ç»“æœ": scored_file,
                "æ„å›¾åˆ†æç»“æœ": intent_file,
                "æ„å›¾åˆ†ææ‘˜è¦": summary_file
            }
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f'analysis_report_{self.date_str}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        self.log(f"å·²ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°: {report_file}")
        
        # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
        self.print_final_summary(report, summary)
        
        return report
    
    def print_final_summary(self, report, summary):
        """æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ¯ å¸‚åœºéœ€æ±‚åˆ†æå®Œæˆ!")
        print("="*80)
        
        print(f"ğŸ“Š åˆ†ææ¦‚è§ˆ:")
        print(f"   â€¢ åˆ†æå…³é”®è¯: {', '.join(report['åˆ†æå…³é”®è¯'])}")
        print(f"   â€¢ ç›®æ ‡åœ°åŒº: {report['åœ°åŒº']}")
        print(f"   â€¢ åˆ†æè€—æ—¶: {report['åˆ†æè€—æ—¶(ç§’)']} ç§’")
        print(f"   â€¢ å‘ç°å…³é”®è¯: {report['å…³é”®è¯æ€»æ•°']} ä¸ª")
        print(f"   â€¢ é«˜åˆ†å…³é”®è¯: {report['é«˜åˆ†å…³é”®è¯æ•°']} ä¸ª")
        
        print(f"\nğŸ¯ æœç´¢æ„å›¾åˆ†å¸ƒ:")
        intent_names = {
            'I': 'ä¿¡æ¯å‹', 'N': 'å¯¼èˆªå‹', 'C': 'å•†ä¸šå‹', 
            'E': 'äº¤æ˜“å‹', 'B': 'è¡Œä¸ºå‹'
        }
        for intent, percentage in report['æ„å›¾åˆ†å¸ƒ'].items():
            intent_name = intent_names.get(intent, intent)
            bar_length = int(percentage / 5)  # æ¯5%ä¸€ä¸ªå­—ç¬¦
            bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            print(f"   {intent} ({intent_name:4s}): {bar} {percentage:5.1f}%")
        
        print(f"\nğŸ† Top 5 é«˜åˆ†å…³é”®è¯:")
        for i, kw in enumerate(report["Top5å…³é”®è¯"]):
            intent_name = intent_names.get(kw['intent'], kw['intent'])
            print(f"   {i+1}. {kw['query']:<40} åˆ†æ•°: {kw['score']:3d} | æ„å›¾: {intent_name}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for desc, path in report['è¾“å‡ºæ–‡ä»¶'].items():
            print(f"   â€¢ {desc}: {path}")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        self.print_recommendations(report, summary)
        
        print("="*80)
    
    def print_recommendations(self, report, summary):
        """æ‰“å°åˆ†æå»ºè®®"""
        intent_dist = report['æ„å›¾åˆ†å¸ƒ']
        top_keywords = report["Top5å…³é”®è¯"]
        
        # åŸºäºæ„å›¾åˆ†å¸ƒçš„å»ºè®®
        if intent_dist.get('I', 0) > 60:
            print("   ğŸ” ä¿¡æ¯å‹å…³é”®è¯å ä¸»å¯¼ï¼Œå»ºè®®åˆ›å»ºæ•™è‚²æ€§å†…å®¹å’ŒæŒ‡å—")
        if intent_dist.get('C', 0) > 30:
            print("   ğŸ’° å•†ä¸šå‹å…³é”®è¯è¾ƒå¤šï¼Œå»ºè®®ä¼˜åŒ–äº§å“é¡µé¢å’Œæ¯”è¾ƒå†…å®¹")
        if intent_dist.get('E', 0) > 20:
            print("   ğŸ›’ äº¤æ˜“å‹å…³é”®è¯è¾ƒå¤šï¼Œå»ºè®®ä¼˜åŒ–è´­ä¹°æµç¨‹å’Œç€é™†é¡µ")
        
        # åŸºäºé«˜åˆ†å…³é”®è¯çš„å»ºè®®
        if report['é«˜åˆ†å…³é”®è¯æ•°'] > 0:
            print(f"   â­ å‘ç° {report['é«˜åˆ†å…³é”®è¯æ•°']} ä¸ªé«˜æ½œåŠ›å…³é”®è¯ï¼Œå»ºè®®ä¼˜å…ˆæŠ•å…¥èµ„æº")
        
        # åŸºäºTopå…³é”®è¯çš„å»ºè®®
        if top_keywords:
            top_intent = max(set(kw['intent'] for kw in top_keywords), 
                           key=lambda x: sum(1 for kw in top_keywords if kw['intent'] == x))
            intent_names = {'I': 'ä¿¡æ¯å‹', 'N': 'å¯¼èˆªå‹', 'C': 'å•†ä¸šå‹', 'E': 'äº¤æ˜“å‹', 'B': 'è¡Œä¸ºå‹'}
            print(f"   ğŸ¯ Topå…³é”®è¯ä¸»è¦ä¸º{intent_names.get(top_intent, top_intent)}ï¼Œå»ºè®®é’ˆå¯¹æ€§ä¼˜åŒ–å†…å®¹ç­–ç•¥")
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¸‚åœºéœ€æ±‚åˆ†æå·¥å…·é›†')
    parser.add_argument('--keywords', nargs='+', required=True, help='è¦åˆ†æçš„å…³é”®è¯åˆ—è¡¨')
    parser.add_argument('--geo', default='', help='åœ°åŒºä»£ç ï¼Œå¦‚USã€GBç­‰ï¼Œé»˜è®¤ä¸ºå…¨çƒ')
    parser.add_argument('--timeframe', default='today 3-m', help='æ—¶é—´èŒƒå›´ï¼Œé»˜è®¤ä¸ºè¿‡å»3ä¸ªæœˆ')
    parser.add_argument('--output', default='data', help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºdata')
    parser.add_argument('--volume-weight', type=float, default=0.4, help='æœç´¢é‡æƒé‡ï¼Œé»˜è®¤0.4')
    parser.add_argument('--growth-weight', type=float, default=0.4, help='å¢é•¿ç‡æƒé‡ï¼Œé»˜è®¤0.4')
    parser.add_argument('--kd-weight', type=float, default=0.2, help='å…³é”®è¯éš¾åº¦æƒé‡ï¼Œé»˜è®¤0.2')
    parser.add_argument('--min-score', type=int, help='æœ€ä½è¯„åˆ†è¿‡æ»¤')
    parser.add_argument('--no-enrich', action='store_true', help='ä¸ä¸°å¯Œå…³é”®è¯æ•°æ®')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¸‚åœºåˆ†æå™¨
    analyzer = MarketAnalyzer(args.output)
    
    # è¿è¡Œåˆ†æ
    analyzer.run_analysis(
        keywords=args.keywords,
        geo=args.geo,
        timeframe=args.timeframe,
        volume_weight=args.volume_weight,
        growth_weight=args.growth_weight,
        kd_weight=args.kd_weight,
        min_score=args.min_score,
        enrich=not args.no_enrich
    )


if __name__ == "__main__":
    main()