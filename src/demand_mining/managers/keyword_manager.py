#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…³é”®è¯ç®¡ç†å™¨ - è´Ÿè´£å…³é”®è¯åˆ†æåŠŸèƒ½
"""

import os
import sys
import json
import re
import copy
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from src.demand_mining.analyzers.intent_analyzer_v2 import IntentAnalyzerV2 as IntentAnalyzer
from src.demand_mining.analyzers.market_analyzer import MarketAnalyzer
from src.demand_mining.analyzers.keyword_analyzer import KeywordAnalyzer
from src.demand_mining.analyzers.comprehensive_analyzer import ComprehensiveAnalyzer
from src.demand_mining.analyzers.serp_analyzer import SerpAnalyzer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)

from .base_manager import BaseManager


class KeywordManager(BaseManager):
    """å…³é”®è¯åˆ†æç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = None):
        super().__init__(config_path)
        
        # å»¶è¿Ÿå¯¼å…¥åˆ†æå™¨ï¼Œé¿å…å¾ªç¯å¯¼å…¥
        self._intent_analyzer = None
        self._market_analyzer = None
        self._keyword_analyzer = None
        self._comprehensive_analyzer = None
        self._serp_analyzer = None
        
        print("ğŸ” å…³é”®è¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        default_weights = {
            'intent_confidence': 0.17,
            'search_volume': 0.18,
            'competition': 0.14,
            'ai_bonus': 0.07,
            'commercial_value': 0.16,
            'serp_purchase_intent': 0.05,
            'serp_ads_presence': 0.05,
            'long_tail': 0.1,
            'brand_penalty': 0.08
        }

        keyword_scoring_cfg = {}
        if hasattr(self, 'config') and self.config:
            attr = getattr(self.config, 'keyword_scoring', None)
            if isinstance(attr, dict):
                keyword_scoring_cfg = attr

        combined = default_weights.copy()
        weights_cfg = keyword_scoring_cfg.get('weights', {}) if isinstance(keyword_scoring_cfg, dict) else {}
        if isinstance(weights_cfg, dict):
            combined.update({k: float(v) for k, v in weights_cfg.items() if isinstance(v, (int, float))})

        env_weights = os.getenv('KEYWORD_SCORING_WEIGHTS')
        if env_weights:
            try:
                env_data = json.loads(env_weights)
                combined.update({k: float(v) for k, v in env_data.items()})
            except Exception:
                pass

        self.scoring_weights = combined
        self._normalize_scoring_weights()

        default_cost_penalty = 0.15
        if isinstance(keyword_scoring_cfg, dict):
            try:
                default_cost_penalty = float(keyword_scoring_cfg.get('cost_penalty', default_cost_penalty))
            except Exception:
                pass
        env_cost_penalty = os.getenv('KEYWORD_SCORING_COST_PENALTY')
        if env_cost_penalty:
            try:
                default_cost_penalty = float(env_cost_penalty)
            except Exception:
                pass
        self.cost_penalty = max(0.0, min(default_cost_penalty, 1.0))

        filters_cfg = self.config.get('keyword_filters', {}) if isinstance(self.config, dict) else {}

        def _extract_values(raw_value):
            if raw_value is None:
                return []
            if isinstance(raw_value, dict):
                for key in ('values', 'items', 'terms'):
                    if key in raw_value and isinstance(raw_value[key], (list, tuple, set)):
                        return [str(v).strip() for v in raw_value[key] if isinstance(v, str) and v.strip()]
                if 'value' in raw_value and isinstance(raw_value['value'], str):
                    return [raw_value['value'].strip()]
                return []
            if isinstance(raw_value, (list, tuple, set)):
                return [str(v).strip() for v in raw_value if isinstance(v, str) and v.strip()]
            if isinstance(raw_value, str) and raw_value.strip():
                return [raw_value.strip()]
            return []

        def _resolve_terms(default_values, list_key, mode_key, default_mode='extend'):
            base_list = [str(v).strip().lower() for v in default_values if isinstance(v, str) and v.strip()]
            raw_entries = filters_cfg.get(list_key)
            mode_candidate = filters_cfg.get(mode_key, default_mode)
            if isinstance(raw_entries, dict) and 'mode' in raw_entries:
                mode_candidate = raw_entries['mode']
            mode = str(mode_candidate).lower() if isinstance(mode_candidate, str) else default_mode
            if mode not in ('extend', 'replace', 'disable'):
                mode = default_mode
            if mode == 'replace':
                base_list = []
            elif mode == 'disable':
                return []

            for value in _extract_values(raw_entries):
                lowered = value.lower()
                if lowered not in base_list:
                    base_list.append(lowered)
            return base_list

        default_brand_phrases = [
            'chatgpt', 'openai', 'gpt', 'gpt-4', 'gpt4', 'gpt5', 'claude',
            'midjourney', 'deepseek', 'hix bypass', 'undetectable ai', 'turnitin',
            'copilot'
        ]
        default_brand_modifiers = [
            'login', 'logins', 'signup', 'sign', 'account', 'app', 'apps', 'download',
            'downloads', 'premium', 'price', 'prices', 'pricing', 'cost', 'costs',
            'free', 'trial', 'promo', 'discount', 'code', 'codes', 'coupon', 'review',
            'reviews', 'vs', 'versus', 'alternative', 'alternatives', 'compare',
            'comparison', 'checker', 'detector', 'essay', 'humanizer', 'turnitin',
            'unlimited', 'pro', 'plus', 'plan', 'plans', 'tier', 'tiers', 'lifetime',
            'prompt', 'prompts'
        ]
        default_long_tail_modifiers = [
            'workflow', 'strategy', 'ideas', 'guide', 'guides', 'step', 'steps',
            'framework', 'frameworks', 'template', 'templates', 'checklist',
            'automation', 'process', 'plan', 'plans', 'blueprint', 'examples',
            'case', 'cases', 'study', 'studies', 'tutorial', 'tutorials', 'use',
            'uses', 'stack', 'stacks', 'integration', 'integrations', 'workflows',
            'niche', 'niches', 'system', 'systems'
        ]
        default_generic_heads = [
            'service', 'services', 'software', 'platform', 'platforms', 'solution',
            'solutions', 'application', 'applications', 'tool', 'tools',
            'machine learning', 'artificial intelligence', 'automation', 'ai',
            'technology', 'technologies', 'gpt'
        ]
        default_question_prefixes = [
            'how to', 'how do', 'how can', 'what is', 'what are', 'why', 'should i',
            'can i', 'is there', 'best way', 'ways to'
        ]

        brand_phrases_list = _resolve_terms(default_brand_phrases, 'brand_phrases', 'brand_phrases_mode')
        brand_modifier_list = _resolve_terms(default_brand_modifiers, 'brand_modifiers', 'brand_modifier_mode')
        long_tail_list = _resolve_terms(default_long_tail_modifiers, 'long_tail_tokens', 'long_tail_mode')
        generic_head_list = _resolve_terms(default_generic_heads, 'generic_head_terms', 'generic_head_mode')
        question_prefix_list = _resolve_terms(default_question_prefixes, 'question_prefixes', 'question_prefix_mode')

        self.brand_phrases = set(brand_phrases_list)
        self.brand_token_set = {token for phrase in self.brand_phrases for token in phrase.split()}
        self.brand_token_set.update({'chatgpt', 'openai', 'gpt', 'claude', 'midjourney', 'deepseek', 'hix', 'bypass', 'turnitin', 'copilot'})
        self.brand_modifier_tokens = set(brand_modifier_list)
        self.long_tail_modifiers = set(long_tail_list)
        self.generic_head_terms = set(generic_head_list)
        self.question_prefixes = tuple(question_prefix_list)

        keyword_analysis_cfg = self.config.get('keyword_analysis', {})
        if not isinstance(keyword_analysis_cfg, dict):
            keyword_analysis_cfg = {}
        self.keyword_analysis_config = keyword_analysis_cfg

        def _resolve_positive_int(value, fallback):
            try:
                candidate = int(value)
            except (TypeError, ValueError):
                return fallback
            return candidate if candidate > 0 else fallback

        base_batch = _resolve_positive_int(keyword_analysis_cfg.get('batch_size', 25), 25)
        self.intent_batch_size = _resolve_positive_int(keyword_analysis_cfg.get('intent_batch_size', base_batch), base_batch)
        self.market_batch_size = _resolve_positive_int(keyword_analysis_cfg.get('market_batch_size', base_batch), base_batch)

        self._cache_enabled = bool(keyword_analysis_cfg.get('enable_cache', True))
        ttl_hours = keyword_analysis_cfg.get('cache_ttl_hours', 12)
        try:
            ttl_hours_value = float(ttl_hours)
        except (TypeError, ValueError):
            ttl_hours_value = 12.0

        if ttl_hours_value <= 0:
            self.cache_ttl = None
            self._cache_enabled = False
        else:
            self.cache_ttl = timedelta(hours=ttl_hours_value)

        cache_dir_cfg = keyword_analysis_cfg.get('cache_dir') or os.path.join(self.output_dir, 'keyword_cache')
        self.cache_dir = Path(cache_dir_cfg)
        self.intent_cache_path = self.cache_dir / 'intent_cache.json'
        self.market_cache_path = self.cache_dir / 'market_cache.json'
        self.intent_cache: Dict[str, Dict[str, Any]] = {}
        self.market_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_dirty: Dict[str, bool] = {'intent': False, 'market': False}

        if self._cache_enabled:
            self._load_caches()

    
    @property
    def intent_analyzer(self):
        """å»¶è¿ŸåŠ è½½æ„å›¾åˆ†æå™¨"""
        if self._intent_analyzer is None:
            self._intent_analyzer = IntentAnalyzer()
        return self._intent_analyzer
    
    @property
    def market_analyzer(self):
        """å»¶è¿ŸåŠ è½½å¸‚åœºåˆ†æå™¨"""
        if self._market_analyzer is None:
            self._market_analyzer = MarketAnalyzer()
        return self._market_analyzer
    
    @property
    def keyword_analyzer(self):
        """å»¶è¿ŸåŠ è½½å…³é”®è¯åˆ†æå™¨"""
        if self._keyword_analyzer is None:
            self._keyword_analyzer = KeywordAnalyzer()
        return self._keyword_analyzer
    
    @property
    def comprehensive_analyzer(self):
        """å»¶è¿ŸåŠ è½½ç»¼åˆåˆ†æå™¨"""
        if self._comprehensive_analyzer is None:
            self._comprehensive_analyzer = ComprehensiveAnalyzer()
        return self._comprehensive_analyzer

    @property
    def serp_analyzer(self):
        """å»¶è¿ŸåŠ è½½SERPåˆ†æå™¨"""
        if getattr(self, '_serp_analyzer', None) is False:
            return None

        if self._serp_analyzer is None:
            demand_cfg = self.config.get('demand_mining', {})
            use_serp = demand_cfg.get('use_serp_signals', self.config.get('intent_analysis', {}).get('enable_serp_analysis', False))
            if not use_serp:
                self._serp_analyzer = False
                return None
            try:
                analyzer = SerpAnalyzer(use_proxy=self.config.get('serp', {}).get('use_proxy', True))
                if not getattr(analyzer, 'credentials_available', True):
                    warning = getattr(analyzer, 'credential_warning', None) or 'æœªæ£€æµ‹åˆ°SERPç›¸å…³APIå‡­è¯ï¼Œè·³è¿‡SERPä¿¡å·æå–'
                    print(f"âš ï¸ {warning}")
                    self._serp_analyzer = False
                    return None
                self._serp_analyzer = analyzer
            except Exception as exc:
                print(f"âš ï¸ SERPåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {exc}")
                self._serp_analyzer = False
        return self._serp_analyzer or None

    
    def analyze(self, input_source, analysis_type: str = 'file', 
                output_dir: str = None, use_comprehensive: bool = False) -> Dict[str, Any]:
        """
        åˆ†æå…³é”®è¯
        
        Args:
            input_source: è¾“å…¥æºï¼ˆæ–‡ä»¶è·¯å¾„æˆ–å…³é”®è¯åˆ—è¡¨ï¼‰
            analysis_type: åˆ†æç±»å‹ ('file' æˆ– 'keywords')
            output_dir: è¾“å‡ºç›®å½•
            use_comprehensive: æ˜¯å¦ä½¿ç”¨ç»¼åˆåˆ†æå™¨
            
        Returns:
            åˆ†æç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹å…³é”®è¯åˆ†æ - ç±»å‹: {analysis_type}, ç»¼åˆåˆ†æ: {use_comprehensive}")
        
        if use_comprehensive:
            return self._comprehensive_analyze(input_source, analysis_type, output_dir)
        
        if analysis_type == 'file':
            return self._analyze_from_file(input_source, output_dir)
        elif analysis_type == 'keywords':
            if isinstance(input_source, (list, tuple, set)):
                keywords = [str(kw) for kw in input_source if kw]
            elif hasattr(input_source, 'tolist'):
                keywords = [str(kw) for kw in input_source.tolist() if kw]
            else:
                keywords = [str(input_source)] if input_source else []
            return self._analyze_from_keywords(keywords, output_dir)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
    
    def _analyze_from_file(self, file_path: str, output_dir: str = None) -> Dict[str, Any]:
        """ä»æ–‡ä»¶åˆ†æå…³é”®è¯"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–æ•°æ®
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith('.json'):
            df = pd.read_json(file_path)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVæˆ–JSONæ–‡ä»¶")
        
        print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡å…³é”®è¯æ•°æ®")
        
        # æå–å…³é”®è¯åˆ—è¡¨
        keywords = []
        for col in ['query', 'keyword', 'term']:
            if col in df.columns:
                keywords = df[col].dropna().tolist()
                break
        
        if not keywords:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å…³é”®è¯åˆ—")
        
        return self._perform_analysis(keywords, output_dir)
    
    def _analyze_from_keywords(self, keywords: List[str], output_dir: str = None) -> Dict[str, Any]:
        """ä»å…³é”®è¯åˆ—è¡¨åˆ†æ"""
        if not keywords:
            raise ValueError("å…³é”®è¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        print(f"âœ… æ¥æ”¶åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
        return self._perform_analysis(keywords, output_dir)
    
    def _perform_analysis(self, keywords: List[str], output_dir: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œå…³é”®è¯åˆ†æ"""
        normalized_keywords = self._prepare_keywords(keywords)
        results = {
            'total_keywords': len(normalized_keywords),
            'analysis_time': datetime.now().isoformat(),
            'keywords': [],
            'intent_summary': {},
            'market_insights': {},
            'recommendations': [],
            'processing_summary': {}
        }

        if not normalized_keywords:
            results['intent_summary'] = self._generate_intent_summary([])
            results['market_insights'] = self._generate_market_insights([])
            results['recommendations'] = []
            results['processing_summary'] = {
                'total_keywords': 0,
                'unique_keywords': 0,
                'intent_cache_hits': 0,
                'intent_batches': 0,
                'intent_computed': 0,
                'market_cache_hits': 0,
                'market_batches': 0,
                'market_computed': 0
            }
            if output_dir:
                output_path = self._save_analysis_results(results, output_dir)
                results['output_path'] = output_path
            return results

        unique_keywords = list(dict.fromkeys(normalized_keywords))
        intent_map, intent_stats = self._collect_intent_results(unique_keywords)
        market_map, market_stats = self._collect_market_results(unique_keywords)

        print(f"ğŸ§® å…³é”®è¯æ‰¹å¤„ç†: æ€»è®¡ {len(normalized_keywords)} ä¸ªï¼Œå”¯ä¸€ {len(unique_keywords)} ä¸ª")
        print(
            f"ğŸ§  æ„å›¾åˆ†æ â†’ ç¼“å­˜å‘½ä¸­ {intent_stats['cache_hits']}ï¼Œæ–°è®¡ç®— {intent_stats['computed']}ï¼Œæ‰¹æ¬¡æ•° {intent_stats['batches']}"
        )
        print(
            f"ğŸ“Š å¸‚åœºåˆ†æ â†’ ç¼“å­˜å‘½ä¸­ {market_stats['cache_hits']}ï¼Œæ–°è®¡ç®— {market_stats['computed']}ï¼Œæ‰¹æ¬¡æ•° {market_stats['batches']}"
        )

        for keyword in normalized_keywords:
            intent_result = copy.deepcopy(intent_map.get(keyword, self._get_default_intent_result()))
            market_result = copy.deepcopy(market_map.get(keyword, self._get_default_market_result(keyword)))
            serp_signals = market_result.get('serp_signals')

            keyword_result = {
                'keyword': keyword,
                'intent': intent_result,
                'market': market_result,
                'opportunity_score': self._calculate_opportunity_score(
                    intent_result, market_result, serp_signals, keyword
                ),
                'execution_cost': market_result.get('execution_cost', 0.0)
            }

            if serp_signals:
                keyword_result['serp_signals'] = serp_signals

            results['keywords'].append(keyword_result)

        results['intent_summary'] = self._generate_intent_summary(results['keywords'])
        results['market_insights'] = self._generate_market_insights(results['keywords'])
        results['recommendations'] = self._generate_recommendations(results['keywords'])
        results['processing_summary'] = {
            'total_keywords': len(normalized_keywords),
            'unique_keywords': len(unique_keywords),
            'intent_cache_hits': intent_stats['cache_hits'],
            'intent_batches': intent_stats['batches'],
            'intent_computed': intent_stats['computed'],
            'market_cache_hits': market_stats['cache_hits'],
            'market_batches': market_stats['batches'],
            'market_computed': market_stats['computed']
        }

        if output_dir:
            output_path = self._save_analysis_results(results, output_dir)
            results['output_path'] = output_path

        if self._cache_enabled:
            self._flush_caches()

        return results

    @staticmethod
    def _prepare_keywords(keywords: List[str]) -> List[str]:
        """æ¸…ç†è¾“å…¥å…³é”®è¯åˆ—è¡¨"""
        prepared: List[str] = []
        if not keywords:
            return prepared

        for keyword in keywords:
            if keyword is None:
                continue
            text = str(keyword).strip()
            if text:
                prepared.append(text)
        return prepared

    def _collect_intent_results(self, keywords: List[str]) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, int]]:
        """æ‰¹é‡æ”¶é›†æ„å›¾åˆ†æç»“æœï¼Œå¸¦ç¼“å­˜"""
        results: Dict[str, Dict[str, Any]] = {}
        stats = {'cache_hits': 0, 'computed': 0, 'batches': 0}

        if not keywords:
            return results, stats

        pending: List[str] = []
        for keyword in keywords:
            cached = self._get_cached_intent_result(keyword)
            if cached is not None:
                results[keyword] = cached
                stats['cache_hits'] += 1
            else:
                pending.append(keyword)

        if not pending:
            return results, stats

        analyzer = self.intent_analyzer
        if analyzer is None:
            for keyword in pending:
                results[keyword] = self._get_default_intent_result()
            return results, stats

        for batch in self._chunked(pending, self.intent_batch_size):
            if not batch:
                continue
            stats['batches'] += 1
            try:
                df = pd.DataFrame({'query': batch})
                payload = analyzer.analyze_keywords(df)
            except Exception as exc:
                print(f"âš ï¸ æ„å›¾åˆ†ææ‰¹å¤„ç†å¤±è´¥: {exc}")
                payload = {}

            batch_results = self._extract_intent_results(batch, payload)
            stats['computed'] += len(batch)
            for keyword, intent_result in batch_results.items():
                results[keyword] = intent_result
                self._set_cached_intent_result(keyword, intent_result)

        return results, stats

    def _extract_intent_results(self, batch: List[str], analyzer_payload: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """ä»æ„å›¾åˆ†æå™¨è¿”å›å€¼ä¸­æå–ç»“æ„åŒ–ç»“æœ"""
        normalized: Dict[str, Dict[str, Any]] = {}
        payload = analyzer_payload or {}
        raw_results = payload.get('results') if isinstance(payload, dict) else None
        raw_results = raw_results or []

        indexed: Dict[str, Dict[str, Any]] = {}
        for item in raw_results:
            if not isinstance(item, dict):
                continue
            query = item.get('query')
            if not query:
                continue
            formatted = self._format_intent_result(item)
            indexed[str(query)] = formatted

        for keyword in batch:
            normalized[keyword] = copy.deepcopy(indexed.get(keyword, self._get_default_intent_result()))

        return normalized

    def _format_intent_result(self, intent_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–å•ä¸ªæ„å›¾åˆ†æç»“æœ"""
        if not isinstance(intent_data, dict):
            return self._get_default_intent_result()

        website_recs = intent_data.get('website_recommendations')
        if not isinstance(website_recs, dict):
            website_recs = {}

        return {
            'primary_intent': intent_data.get('intent_primary', 'Unknown') or 'Unknown',
            'confidence': intent_data.get('probability', 0.0) or 0.0,
            'secondary_intent': intent_data.get('intent_secondary') or None,
            'intent_description': intent_data.get('intent_primary', 'Unknown') or 'Unknown',
            'website_recommendations': {
                'website_type': website_recs.get('website_type', intent_data.get('website_type', 'Unknown')),
                'ai_tool_category': website_recs.get('ai_tool_category', intent_data.get('ai_tool_category', 'General')),
                'domain_suggestions': website_recs.get('domain_suggestions', intent_data.get('domain_suggestions', [])),
                'monetization_strategy': website_recs.get('monetization_strategy', intent_data.get('monetization_strategy', [])),
                'technical_requirements': website_recs.get('technical_requirements', intent_data.get('technical_requirements', [])),
                'competition_analysis': website_recs.get('competition_analysis', intent_data.get('competition_analysis', {})),
                'development_priority': website_recs.get('development_priority', intent_data.get('development_priority', {})),
                'content_strategy': website_recs.get('content_strategy', intent_data.get('content_strategy', []))
            }
        }

    def _collect_market_results(self, keywords: List[str]) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, int]]:
        """æ‰¹é‡æ”¶é›†å¸‚åœºåˆ†æç»“æœï¼Œå¸¦ç¼“å­˜"""
        results: Dict[str, Dict[str, Any]] = {}
        stats = {'cache_hits': 0, 'computed': 0, 'batches': 0}

        if not keywords:
            return results, stats

        pending: List[str] = []
        for keyword in keywords:
            cached = self._get_cached_market_result(keyword)
            if cached is not None:
                results[keyword] = cached
                stats['cache_hits'] += 1
            else:
                pending.append(keyword)

        if not pending:
            return results, stats

        analyzer = self.market_analyzer
        if analyzer is None:
            for keyword in pending:
                results[keyword] = self._get_default_market_result(keyword)
            return results, stats

        for batch in self._chunked(pending, self.market_batch_size):
            if not batch:
                continue
            stats['batches'] += 1
            try:
                base_payload = analyzer.analyze_market_data(batch)
            except Exception as exc:
                print(f"âš ï¸ å¸‚åœºåˆ†ææ‰¹å¤„ç†å¤±è´¥: {exc}")
                base_payload = {}

            stats['computed'] += len(batch)
            for keyword in batch:
                raw_market = base_payload.get(keyword, {}) if isinstance(base_payload, dict) else {}
                try:
                    enriched = self._enrich_market_result(keyword, raw_market)
                except Exception as exc:
                    print(f"âš ï¸ å¸‚åœºåˆ†æå¤„ç†å¤±è´¥ ({keyword}): {exc}")
                    enriched = self._get_default_market_result(keyword)
                results[keyword] = enriched
                self._set_cached_market_result(keyword, enriched)

        return results, stats

    def _enrich_market_result(self, keyword: str, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¡¥å…¨å¸‚åœºåˆ†æä¿¡æ¯ï¼ˆå« SERP ä¿¡å·ï¼‰"""
        if not isinstance(market_data, dict):
            market_data = {}

        base_data = {
            'search_volume': market_data.get('search_volume', 1000),
            'competition': market_data.get('competition', 0.5),
            'cpc': market_data.get('cpc', 2.0),
            'trend': market_data.get('trend', 'stable'),
            'seasonality': market_data.get('seasonality', 'low'),
            'execution_cost': market_data.get('execution_cost', 0.3)
        }

        base_data.update({
            'ai_bonus': self._calculate_ai_bonus(keyword),
            'commercial_value': self._assess_commercial_value(keyword),
            'opportunity_indicators': self._get_opportunity_indicators(keyword),
            'keyword': keyword
        })

        serp_signals = market_data.get('serp_signals')
        if not serp_signals:
            serp_signals = self._gather_serp_signals(keyword)

        if serp_signals:
            base_data['serp_signals'] = serp_signals

        return base_data

    def _get_cached_intent_result(self, keyword: str) -> Optional[Dict[str, Any]]:
        """è¯»å–æ„å›¾ç¼“å­˜"""
        if not self._cache_enabled:
            return None

        entry = self.intent_cache.get(keyword)
        if entry is None or not self._is_cache_entry_valid(entry):
            if entry is not None:
                self.intent_cache.pop(keyword, None)
                self._cache_dirty['intent'] = True
            return None

        return copy.deepcopy(entry.get('value'))

    def _set_cached_intent_result(self, keyword: str, value: Dict[str, Any]) -> None:
        """å†™å…¥æ„å›¾ç¼“å­˜"""
        if not self._cache_enabled:
            return

        self.intent_cache[keyword] = {
            'value': copy.deepcopy(value),
            'timestamp': datetime.now().isoformat()
        }
        self._cache_dirty['intent'] = True

    def _get_cached_market_result(self, keyword: str) -> Optional[Dict[str, Any]]:
        """è¯»å–å¸‚åœºç¼“å­˜"""
        if not self._cache_enabled:
            return None

        entry = self.market_cache.get(keyword)
        if entry is None or not self._is_cache_entry_valid(entry):
            if entry is not None:
                self.market_cache.pop(keyword, None)
                self._cache_dirty['market'] = True
            return None

        return copy.deepcopy(entry.get('value'))

    def _set_cached_market_result(self, keyword: str, value: Dict[str, Any]) -> None:
        """å†™å…¥å¸‚åœºç¼“å­˜"""
        if not self._cache_enabled:
            return

        self.market_cache[keyword] = {
            'value': copy.deepcopy(value),
            'timestamp': datetime.now().isoformat()
        }
        self._cache_dirty['market'] = True

    def _is_cache_entry_valid(self, entry: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ¡ç›®æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…"""
        if not isinstance(entry, dict) or 'value' not in entry:
            return False

        if not self.cache_ttl:
            return True

        timestamp = entry.get('timestamp')
        if not timestamp:
            return False

        try:
            cached_at = datetime.fromisoformat(timestamp)
        except Exception:
            return False

        return datetime.now() - cached_at <= self.cache_ttl

    def _load_caches(self) -> None:
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        except Exception as exc:
            print(f"âš ï¸ æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½• {self.cache_dir}: {exc}")
            self._cache_enabled = False
            return

        self.intent_cache = self._read_cache_file(self.intent_cache_path, 'intent')
        self.market_cache = self._read_cache_file(self.market_cache_path, 'market')

    def _read_cache_file(self, path: Path, cache_type: str) -> Dict[str, Any]:
        """è¯»å–å•ä¸ªç¼“å­˜æ–‡ä»¶"""
        if not path.exists():
            return {}

        try:
            with path.open('r', encoding='utf-8') as fh:
                raw_data = json.load(fh)
        except Exception as exc:
            print(f"âš ï¸ æ— æ³•åŠ è½½ç¼“å­˜ {path}: {exc}")
            return {}

        if not isinstance(raw_data, dict):
            return {}

        cleaned: Dict[str, Any] = {}
        removed = False
        for keyword, entry in raw_data.items():
            if not isinstance(keyword, str) or not isinstance(entry, dict):
                removed = True
                continue
            if not self._is_cache_entry_valid(entry):
                removed = True
                continue
            cleaned[keyword] = entry

        if removed:
            self._cache_dirty[cache_type] = True

        return cleaned

    def _write_cache_file(self, path: Path, payload: Dict[str, Any]) -> None:
        """å†™å…¥ç¼“å­˜æ–‡ä»¶"""
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            with path.open('w', encoding='utf-8') as fh:
                json.dump(payload, fh, ensure_ascii=False, indent=2)
        except Exception as exc:
            print(f"âš ï¸ ç¼“å­˜å†™å…¥å¤±è´¥ ({path}): {exc}")

    def _flush_caches(self) -> None:
        """å°†å†…å­˜ç¼“å­˜å†™å›ç£ç›˜"""
        if not self._cache_enabled:
            return

        if self._cache_dirty.get('intent'):
            self._write_cache_file(self.intent_cache_path, self.intent_cache)
            self._cache_dirty['intent'] = False

        if self._cache_dirty.get('market'):
            self._write_cache_file(self.market_cache_path, self.market_cache)
            self._cache_dirty['market'] = False

    @staticmethod
    def _chunked(items: List[str], chunk_size: int):
        """æŒ‰æ‰¹æ¬¡æ‹†åˆ†åˆ—è¡¨"""
        if not items:
            return

        try:
            chunk_size = int(chunk_size)
        except (TypeError, ValueError):
            chunk_size = len(items)

        if chunk_size <= 0:
            chunk_size = len(items)

        for idx in range(0, len(items), chunk_size):
            yield items[idx: idx + chunk_size]

    @staticmethod
    def _get_default_market_result(keyword: str = '') -> Dict[str, Any]:
        """è·å–é»˜è®¤å¸‚åœºåˆ†æç»“æœ"""
        return {
            'search_volume': 0,
            'competition': 0.0,
            'cpc': 0.0,
            'trend': 'unknown',
            'seasonality': 'unknown',
            'execution_cost': 0.5,
            'ai_bonus': 0,
            'commercial_value': 0,
            'opportunity_indicators': [],
            'keyword': keyword
        }

    def _analyze_keyword_intent(self, keyword: str) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯æ„å›¾"""
        try:
            intent_map, _ = self._collect_intent_results([keyword])
            return intent_map.get(keyword, self._get_default_intent_result())
        except Exception as exc:
            print(f"âš ï¸ æ„å›¾åˆ†æå¤±è´¥ ({keyword}): {exc}")
            return self._get_default_intent_result()


    def _gather_serp_signals(self, keyword: str) -> Dict[str, Any]:
        """æå–SERPä¿¡å·"""
        serp_analyzer = self.serp_analyzer
        if not serp_analyzer:
            return {}
        try:
            serp_result = serp_analyzer.analyze_keyword_serp(keyword)
        except Exception as exc:
            print(f"âš ï¸ SERPä¿¡å·æå–å¤±è´¥ ({keyword}): {exc}")
            return {}

        features = serp_result.get('serp_features', {}) or {}
        if not features:
            return {}

        ads_reference = features.get('ads_reference_window', features.get('analyzed_results', 4))
        ads_reference = max(ads_reference or 0, 1)

        return {
            'ads_count': features.get('ads_count', 0),
            'analyzed_results': features.get('analyzed_results', 0),
            'purchase_intent_hits': features.get('purchase_intent_hits', 0),
            'purchase_intent_ratio': features.get('purchase_intent_ratio', 0.0),
            'ads_reference_window': ads_reference,
            'purchase_terms': features.get('purchase_intent_terms', []),
        }
    def _analyze_keyword_market(self, keyword: str) -> Dict[str, Any]:
        """Analyze keyword market data"""
        try:
            market_map, _ = self._collect_market_results([keyword])
            return market_map.get(keyword, self._get_default_market_result(keyword))
        except Exception as exc:
            print(f"âš ï¸ å¸‚åœºåˆ†æå¤±è´¥ ({keyword}): {exc}")
            return self._get_default_market_result(keyword)

    def _normalize_scoring_weights(self):
        """Ensure opportunity scoring weights are normalized"""
        total = sum(self.scoring_weights.values()) if getattr(self, 'scoring_weights', None) else 0
        if not total:
            self.scoring_weights = {
                'intent_confidence': 0.2,
                'search_volume': 0.22,
                'competition': 0.15,
                'ai_bonus': 0.1,
                'commercial_value': 0.18,
                'serp_purchase_intent': 0.05,
                'serp_ads_presence': 0.05
            }
            total = 1.0
        if total <= 0:
            total = 1.0
        self.scoring_weights = {k: (v / total) for k, v in self.scoring_weights.items()}

    def _calculate_opportunity_score(self, intent_result: Dict, market_result: Dict, serp_signals: Optional[Dict] = None, keyword: Optional[str] = None) -> float:
        """Calculate overall opportunity score"""
        try:
            serp_signals = serp_signals or market_result.get('serp_signals') or {}
            weights = getattr(self, 'scoring_weights', None) or {
                'intent_confidence': 0.17,
                'search_volume': 0.18,
                'competition': 0.14,
                'ai_bonus': 0.07,
                'commercial_value': 0.16,
                'serp_purchase_intent': 0.05,
                'serp_ads_presence': 0.05,
                'long_tail': 0.1,
                'brand_penalty': 0.08
            }

            def _weight(key: str, default: float = 0.0) -> float:
                try:
                    return float(weights.get(key, default))
                except Exception:
                    return default

            intent_score = intent_result.get('confidence', 0) * 100
            volume_score = min(market_result.get('search_volume', 0) / 800, 1) * 100
            competition_raw = market_result.get('competition', 1)
            competition_score = max(0.0, min(1 - competition_raw, 1)) * 100

            ai_bonus_raw = market_result.get('ai_bonus', 0)
            commercial_value_raw = market_result.get('commercial_value', 0)
            ai_bonus_score = min(ai_bonus_raw * 2.5, 100)
            commercial_value_score = min(commercial_value_raw * 2.0, 100)

            purchase_ratio = serp_signals.get('purchase_intent_ratio')
            if purchase_ratio is None:
                purchase_hits = serp_signals.get('purchase_intent_hits', 0)
                analyzed_results = serp_signals.get('analyzed_results', 0)
                purchase_ratio = (purchase_hits / analyzed_results) if analyzed_results else 0.0
            serp_purchase_score = min(purchase_ratio * 100, 100)

            ads_count = serp_signals.get('ads_count', 0)
            ads_reference = max(serp_signals.get('ads_reference_window', 4), 1)
            serp_ads_score = min((ads_count / ads_reference) * 100, 100) if ads_reference else 0.0

            total_score = (
                intent_score * _weight('intent_confidence') +
                volume_score * _weight('search_volume') +
                competition_score * _weight('competition') +
                ai_bonus_score * _weight('ai_bonus') +
                commercial_value_score * _weight('commercial_value') +
                serp_purchase_score * _weight('serp_purchase_intent') +
                serp_ads_score * _weight('serp_ads_presence')
            )

            keyword_text = (keyword or market_result.get('keyword') or '').lower()
            if keyword_text:
                long_tail_signal = self._calculate_long_tail_signal(keyword_text)
                if long_tail_signal > 0:
                    total_score += long_tail_signal * _weight('long_tail', 0.1)

                brand_penalty = self._calculate_brand_penalty(keyword_text)
                if brand_penalty > 0:
                    penalty_weight = _weight('brand_penalty', 0.08)
                    total_score -= brand_penalty * penalty_weight

            execution_cost = market_result.get('execution_cost')
            if execution_cost is None:
                execution_cost = serp_signals.get('execution_cost')
            cost_penalty_weight = getattr(self, 'cost_penalty', 0.15)
            if execution_cost is not None and cost_penalty_weight:
                try:
                    normalized_cost = min(max(float(execution_cost), 0.0), 1.0)
                except Exception:
                    normalized_cost = 0.0
                total_score -= normalized_cost * cost_penalty_weight * 100

            return round(max(0.0, min(total_score, 100.0)), 2)

        except Exception as e:
            print(f"âš ï¸ æœºä¼šåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    @staticmethod
    def _get_default_intent_result() -> Dict[str, Any]:
        """Return a default intent analysis result"""
        return {
            'primary_intent': 'Unknown',
            'confidence': 0.0,
            'secondary_intent': None,
            'intent_description': 'åˆ†æå¤±è´¥',
            'website_recommendations': {}
        }

    @staticmethod
    def _calculate_ai_bonus(keyword: str) -> float:
        """Calculate extra score for AI-related keywords"""
        keyword_lower = keyword.lower()
        ai_score = 0

        ai_prefixes = ['ai', 'artificial intelligence', 'machine learning', 'deep learning']
        if any(prefix in keyword_lower for prefix in ai_prefixes):
            ai_score += 20

        ai_tool_types = ['generator', 'detector', 'writer', 'assistant', 'chatbot', 'humanizer']
        if any(tool_type in keyword_lower for tool_type in ai_tool_types):
            ai_score += 15

        brand_tokens = ['bypass', 'undetectable', 'deepseek', 'chatgpt', 'gpt', 'llm']
        if any(token in keyword_lower for token in brand_tokens):
            ai_score += 10

        return min(ai_score, 60)

    def _calculate_long_tail_signal(self, keyword: str) -> float:
        """Estimate long-tail strength to reward descriptive queries"""
        if not keyword:
            return 0.0

        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword) if t]
        if not tokens:
            return 0.0

        score = 0.0

        if len(tokens) >= 4:
            score += 55
        elif len(tokens) == 3:
            score += 35
        elif len(tokens) == 2:
            score += 15

        unique_tokens = len(set(tokens))
        if unique_tokens >= 3:
            score += 10

        if any(mod in tokens for mod in self.long_tail_modifiers):
            score += 20

        if any(keyword.startswith(prefix) for prefix in self.question_prefixes):
            score += 20

        if 'for' in tokens and tokens.index('for') < len(tokens) - 1:
            score += 15

        if self._is_generic_head_term(keyword):
            score -= 40

        return max(0.0, min(score, 100.0))

    def _calculate_brand_penalty(self, keyword: str) -> float:
        """Apply penalty for branded or vendor-owned terms without unique angle"""
        if not keyword:
            return 0.0

        keyword_lower = keyword.lower()
        raw_tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        if not raw_tokens:
            return 0.0

        brand_present = any(phrase in keyword_lower for phrase in self.brand_phrases)
        if not brand_present:
            brand_present = any(token in self.brand_token_set for token in raw_tokens)
        if not brand_present:
            return 0.0

        non_brand_tokens = [t for t in raw_tokens if t not in self.brand_token_set]
        if not non_brand_tokens:
            return 80.0

        if all(token in self.brand_modifier_tokens for token in non_brand_tokens):
            return 65.0

        if len(non_brand_tokens) == 1 and non_brand_tokens[0] in self.brand_modifier_tokens:
            return 55.0

        if self._is_generic_head_term(keyword_lower):
            return 45.0

        return 25.0

    def _is_generic_head_term(self, keyword: str) -> bool:
        """Detect overly generic head terms that lack modifiers"""
        if not keyword:
            return False

        keyword_lower = keyword.lower()
        if keyword_lower in self.generic_head_terms:
            return True

        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        if not tokens:
            return True

        if len(tokens) == 1:
            token = tokens[0]
            return token in self.generic_head_terms or len(token) <= 3

        if len(tokens) == 2:
            joined = " ".join(tokens)
            generic_pairs = {'ai', 'machine', 'software', 'platform', 'service', 'tool'}
            if joined in self.generic_head_terms:
                return True
            if tokens[0] in self.generic_head_terms and tokens[1] in self.generic_head_terms:
                return True
            if tokens[0] in generic_pairs and tokens[1] in self.generic_head_terms:
                return True
            if tokens[1] in generic_pairs and tokens[0] in self.generic_head_terms:
                return True

        return False

    @staticmethod
    def _assess_commercial_value(keyword: str) -> float:
        """Estimate commercial value of a keyword"""
        keyword_lower = keyword.lower()
        commercial_score = 0

        high_value_types = [
            'generator', 'converter', 'editor', 'maker', 'creator',
            'optimizer', 'enhancer', 'analyzer', 'detector'
        ]
        if any(value_type in keyword_lower for value_type in high_value_types):
            commercial_score += 25

        high_payment_domains = [
            'business', 'marketing', 'seo', 'content', 'design',
            'video', 'image', 'writing', 'coding', 'academic'
        ]
        if any(domain in keyword_lower for domain in high_payment_domains):
            commercial_score += 20

        monetization_terms = [
            'pricing', 'price', 'plan', 'plans', 'premium', 'pro', 'subscription',
            'license', 'licence', 'login', 'account', 'service', 'software',
            'discount', 'promo', 'alternative'
        ]
        if any(term in keyword_lower for term in monetization_terms):
            commercial_score += 20

        return min(commercial_score, 60)

    @staticmethod
    def _get_opportunity_indicators(keyword: str) -> List[str]:
        """è·å–æœºä¼šæŒ‡æ ‡"""
        indicators = []
        keyword_lower = keyword.lower()
        
        # AIç›¸å…³
        if any(prefix in keyword_lower for prefix in ['ai', 'artificial intelligence']):
            indicators.append("AIç›¸å…³éœ€æ±‚")
        
        # å·¥å…·ç±»
        if any(tool in keyword_lower for tool in ['generator', 'converter', 'editor']):
            indicators.append("å·¥å…·ç±»éœ€æ±‚")
        
        # æ–°å…´æ¦‚å¿µ
        if any(concept in keyword_lower for concept in ['gpt', 'chatbot', 'neural']):
            indicators.append("æ–°å…´æŠ€æœ¯")
        
        # å‡ºæµ·å‹å¥½
        if not any(ord(char) > 127 for char in keyword):
            indicators.append("å‡ºæµ·å‹å¥½")
        
        return indicators
    
    @staticmethod
    def _generate_intent_summary(keywords: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ„å›¾æ‘˜è¦"""
        intent_counts = {}
        total_keywords = len(keywords)
        
        for kw in keywords:
            intent = kw['intent']['primary_intent']
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        intent_percentages = {
            intent: round(count / total_keywords * 100, 1)
            for intent, count in intent_counts.items()
        }
        
        return {
            'total_keywords': total_keywords,
            'intent_distribution': intent_counts,
            'intent_percentages': intent_percentages,
            'dominant_intent': max(intent_counts.items(), key=lambda x: x[1])[0] if intent_counts else 'Unknown'
        }

    @staticmethod
    def _generate_market_insights(keywords: List[Dict]) -> Dict[str, Any]:
        """Generate market insight summary"""
        high_opportunity = [kw for kw in keywords if kw['opportunity_score'] >= 70]
        medium_opportunity = [kw for kw in keywords if 40 <= kw['opportunity_score'] < 70]
        low_opportunity = [kw for kw in keywords if kw['opportunity_score'] < 40]

        avg_cost = 0.0
        if keywords:
            total_cost = sum((kw.get('execution_cost') or kw.get('market', {}).get('execution_cost', 0.0)) for kw in keywords)
            avg_cost = round(total_cost / len(keywords), 2)

        low_cost_keywords = [
            kw for kw in keywords
            if (kw.get('execution_cost') or kw.get('market', {}).get('execution_cost', 1.0)) <= 0.35
        ]

        return {
            'high_opportunity_count': len(high_opportunity),
            'medium_opportunity_count': len(medium_opportunity),
            'low_opportunity_count': len(low_opportunity),
            'top_opportunities': sorted(keywords, key=lambda x: x['opportunity_score'], reverse=True)[:10],
            'avg_opportunity_score': round(sum(kw['opportunity_score'] for kw in keywords) / len(keywords), 2) if keywords else 0,
            'avg_execution_cost': avg_cost,
            'low_cost_candidates': low_cost_keywords[:5]
        }

    @staticmethod
    def _generate_recommendations(keywords: List[Dict]) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        # åˆ†æå…³é”®è¯åˆ†å¸ƒ
        high_opportunity = [kw for kw in keywords if kw['opportunity_score'] >= 70]
        ai_keywords = [kw for kw in keywords if kw['market'].get('ai_bonus', 0) > 0]
        
        # é«˜æœºä¼šå…³é”®è¯å»ºè®®
        if high_opportunity:
            recommendations.append(f"ğŸ¯ å‘ç° {len(high_opportunity)} ä¸ªé«˜æœºä¼šå…³é”®è¯ï¼Œå»ºè®®ç«‹å³å¼€å‘MVPäº§å“")
            top_3 = sorted(high_opportunity, key=lambda x: x['opportunity_score'], reverse=True)[:3]
            for i, kw in enumerate(top_3, 1):
                recommendations.append(f"   {i}. {kw['keyword']} (æœºä¼šåˆ†æ•°: {kw['opportunity_score']})")
        
        # AIç›¸å…³å»ºè®®
        if ai_keywords:
            recommendations.append(f"ğŸ¤– å‘ç° {len(ai_keywords)} ä¸ªAIç›¸å…³å…³é”®è¯ï¼Œç¬¦åˆå‡ºæµ·AIå·¥å…·æ–¹å‘")
        
        return recommendations
    
    @staticmethod
    def _save_analysis_results(results: Dict[str, Any], output_dir: str) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        from src.utils.file_utils import save_results_with_timestamp
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = save_results_with_timestamp(results, output_dir, 'keyword_analysis')
        
        # ä¿å­˜CSVæ ¼å¼ï¼ˆå…³é”®è¯è¯¦æƒ…ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_path = os.path.join(output_dir, f'keywords_detail_{timestamp}.csv')
        keywords_df = pd.DataFrame([
            {
                'keyword': kw['keyword'],
                'primary_intent': kw['intent']['primary_intent'],
                'confidence': kw['intent']['confidence'],
                'search_volume': kw['market']['search_volume'],
                'competition': kw['market']['competition'],
                'opportunity_score': kw['opportunity_score']
            }
            for kw in results['keywords']
        ])
        keywords_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        return json_path
    
    def _comprehensive_analyze(self, input_source: str, analysis_type: str, output_dir: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨ç»¼åˆåˆ†æå™¨è¿›è¡Œå…¨é¢åˆ†æ"""
        print("ğŸ”§ å¯åŠ¨ç»¼åˆåˆ†ææ¨¡å¼...")
        
        # å‡†å¤‡æ•°æ®
        if analysis_type == 'file':
            if not os.path.exists(input_source):
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_source}")
            
            # è¯»å–æ•°æ®
            if input_source.endswith('.csv'):
                df = pd.read_csv(input_source)
            elif input_source.endswith('.json'):
                df = pd.read_json(input_source)
            else:
                raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVæˆ–JSONæ–‡ä»¶")
            
            print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡å…³é”®è¯æ•°æ®")
            
        elif analysis_type == 'keywords':
            keywords = [input_source] if isinstance(input_source, str) else input_source
            df = pd.DataFrame({'query': keywords})
            print(f"âœ… æ¥æ”¶åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
        
        # æ‰§è¡Œç»¼åˆåˆ†æ
        try:
            analysis_result = self.comprehensive_analyzer.analyze(df)
            
            # ä¿å­˜ç»“æœ
            if output_dir:
                output_path = self.comprehensive_analyzer.export_comprehensive_report(
                    analysis_result, output_dir
                )
                analysis_result['output_path'] = output_path
                print(f"ğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_dir}")
            
            # ç”Ÿæˆç®€åŒ–çš„è¿”å›ç»“æœï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
            simplified_result = self._convert_to_legacy_format(analysis_result)
            
            return simplified_result
            
        except Exception as e:
            print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
            # é™çº§åˆ°åŸæœ‰åˆ†ææ–¹æ³•
            print("ğŸ”„ é™çº§åˆ°åŸºç¡€åˆ†ææ¨¡å¼...")
            if analysis_type == 'file':
                return self._analyze_from_file(input_source, output_dir)
            else:
                return self._analyze_from_keywords([input_source], output_dir)
    
    def _convert_to_legacy_format(self, comprehensive_result: Dict[str, Any]) -> Dict[str, Any]:
        """å°†ç»¼åˆåˆ†æç»“æœè½¬æ¢ä¸ºå…¼å®¹åŸæœ‰æ¥å£çš„æ ¼å¼"""
        df = comprehensive_result['results']
        summary = comprehensive_result['summary']
        
        # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼
        legacy_result = {
            'total_keywords': summary['total_keywords'],
            'analysis_time': comprehensive_result['analysis_time'],
            'keywords': [],
            'intent_summary': {},
            'market_insights': {},
            'recommendations': [],
            'comprehensive_summary': summary  # æ–°å¢ç»¼åˆæ‘˜è¦
        }
        
        # è½¬æ¢å…³é”®è¯è¯¦æƒ…
        for _, row in df.iterrows():
            keyword_result = {
                'keyword': row['query'],
                'comprehensive_score': row.get('comprehensive_score', 0),
                'comprehensive_grade': row.get('comprehensive_grade', 'C'),
                'intent': {
                    'primary_intent': row.get('intent_intent', 'Unknown'),
                    'confidence': row.get('intent_confidence', 0.0),
                    'intent_description': row.get('intent_intent_description', '')
                },
                'market': {
                    'search_volume': row.get('market_search_volume', 0),
                    'competition': row.get('market_competition', 0.5),
                    'cpc': row.get('market_cpc', 0.0)
                },
                'timeliness': {
                    'score': row.get('timeliness_score', 50.0),
                    'grade': row.get('timeliness_grade', 'C'),
                    'trend_direction': row.get('timeliness_trend_direction', 'stable')
                },
                'scorer': {
                    'total_score': row.get('scorer_total_score', 50.0),
                    'pray_score': row.get('scorer_pray_score', 50.0),
                    'commercial_score': row.get('scorer_commercial_score', 30.0)
                },
                'opportunity_score': row.get('comprehensive_score', 50.0)  # ä½¿ç”¨ç»¼åˆè¯„åˆ†ä½œä¸ºæœºä¼šåˆ†æ•°
            }
            legacy_result['keywords'].append(keyword_result)
        
        # ç”Ÿæˆæ„å›¾æ‘˜è¦
        if 'intent_intent' in df.columns:
            intent_counts = df['intent_intent'].value_counts().to_dict()
            total = len(df)
            legacy_result['intent_summary'] = {
                'total_keywords': total,
                'intent_distribution': intent_counts,
                'intent_percentages': {k: round(v/total*100, 1) for k, v in intent_counts.items()},
                'dominant_intent': max(intent_counts.items(), key=lambda x: x[1])[0] if intent_counts else 'Unknown'
            }
        
        # ç”Ÿæˆå¸‚åœºæ´å¯Ÿ
        high_opportunity = df[df['comprehensive_score'] >= 80] if 'comprehensive_score' in df.columns else pd.DataFrame()
        medium_opportunity = df[(df['comprehensive_score'] >= 60) & (df['comprehensive_score'] < 80)] if 'comprehensive_score' in df.columns else pd.DataFrame()
        low_opportunity = df[df['comprehensive_score'] < 60] if 'comprehensive_score' in df.columns else pd.DataFrame()
        
        legacy_result['market_insights'] = {
            'high_opportunity_count': len(high_opportunity),
            'medium_opportunity_count': len(medium_opportunity),
            'low_opportunity_count': len(low_opportunity),
            'top_opportunities': df.nlargest(10, 'comprehensive_score')[['query', 'comprehensive_score']].to_dict('records') if 'comprehensive_score' in df.columns else [],
            'avg_opportunity_score': round(df['comprehensive_score'].mean(), 2) if 'comprehensive_score' in df.columns else 50.0
        }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        if len(high_opportunity) > 0:
            recommendations.append(f"ğŸ¯ å‘ç° {len(high_opportunity)} ä¸ªé«˜ä»·å€¼å…³é”®è¯ (ç»¼åˆè¯„åˆ†â‰¥80)ï¼Œå»ºè®®ä¼˜å…ˆå¼€å‘")
            top_3 = high_opportunity.nlargest(3, 'comprehensive_score')
            for i, (_, row) in enumerate(top_3.iterrows(), 1):
                recommendations.append(f"   {i}. {row['query']} (ç»¼åˆè¯„åˆ†: {row['comprehensive_score']})")
        
        if 'timeliness_grade' in df.columns:
            high_timeliness = df[df['timeliness_grade'].isin(['A', 'B'])]
            if len(high_timeliness) > 0:
                recommendations.append(f"â° å‘ç° {len(high_timeliness)} ä¸ªé«˜æ—¶æ•ˆæ€§å…³é”®è¯ï¼Œå»ºè®®å¿«é€Ÿè¡ŒåŠ¨")
        
        if 'scorer_commercial_score' in df.columns:
            high_commercial = df[df['scorer_commercial_score'] >= 70]
            if len(high_commercial) > 0:
                recommendations.append(f"ğŸ’° å‘ç° {len(high_commercial)} ä¸ªé«˜å•†ä¸šä»·å€¼å…³é”®è¯ï¼Œå˜ç°æ½œåŠ›å¤§")
        
        legacy_result['recommendations'] = recommendations
        
        return legacy_result
