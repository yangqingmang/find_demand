#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶‹åŠ¿ç®¡ç†å™¨ - è´Ÿè´£è¯æ ¹è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹åŠŸèƒ½
"""

import os
import sys
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)

from .base_manager import BaseManager
from ..root_word_trends_analyzer import RootWordTrendsAnalyzer
from ..core.trends_cache import TrendsCache


class TrendManager(BaseManager):
    """è¶‹åŠ¿åˆ†æç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = None):
        super().__init__(config_path)
        self._trend_analyzer = None
        self._root_manager = None
        # é›†æˆç°æœ‰çš„ RootWordTrendsAnalyzer
        self.root_analyzer = RootWordTrendsAnalyzer()
        # é›†æˆè¶‹åŠ¿æ•°æ®ç¼“å­˜æœºåˆ¶
        self.trends_cache = TrendsCache(
            cache_dir=os.path.join(self.output_dir, 'trends_cache'),
            cache_duration_hours=24,
            max_cache_size_mb=500
        )
        # ä¼˜åŒ–æ‰¹é‡å¤„ç†é…ç½®
        self.batch_config = {
            'default_batch_size': 5,
            'max_batch_size': 10,
            'delay_between_batches': 3,  # ç§’
            'retry_attempts': 3,
            'timeout_per_keyword': 30,  # ç§’
            'cache_enabled': True,
            'offline_mode': False
        }
        print("ğŸ“ˆ è¶‹åŠ¿ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆå·²å¯ç”¨ç¼“å­˜ï¼‰")
    
    @property
    def trend_analyzer(self):
        """å»¶è¿ŸåŠ è½½è¶‹åŠ¿åˆ†æå™¨ - ä½¿ç”¨å•ä¾‹æ¨¡å¼é¿å…é‡å¤åˆ›å»º"""
        if self._trend_analyzer is None:
            try:
                # ä½¿ç”¨å•ä¾‹æ¨¡å¼è·å–è¶‹åŠ¿åˆ†æå™¨ï¼Œé¿å…é‡å¤åˆ›å»ºå®ä¾‹
                from src.demand_mining.analyzers.root_word_trends_analyzer_singleton import get_root_word_trends_analyzer
                self._trend_analyzer = get_root_word_trends_analyzer(
                    output_dir=os.path.join(self.output_dir, 'root_word_trends')
                )
            except ImportError:
                try:
                    # å¦‚æœå•ä¾‹ä¸å­˜åœ¨ï¼Œç›´æ¥å¯¼å…¥ä½†ä¸åˆ›å»ºæ–°å®ä¾‹
                    from src.demand_mining.root_word_trends_analyzer import RootWordTrendsAnalyzer
                    # ä¸åˆ›å»ºæ–°å®ä¾‹ï¼Œè¿”å›Noneè®©è°ƒç”¨æ–¹å¤„ç†
                    print("âš ï¸ è¶‹åŠ¿åˆ†æå™¨å•ä¾‹ä¸å¯ç”¨ï¼Œè·³è¿‡åˆ›å»ºæ–°å®ä¾‹é¿å…429é”™è¯¯")
                    self._trend_analyzer = None
                except ImportError as e:
                    print(f"âš ï¸ æ— æ³•å¯¼å…¥è¶‹åŠ¿åˆ†æå™¨: {e}")
                    self._trend_analyzer = None
        return self._trend_analyzer
    
    @property
    def root_manager(self):
        """å»¶è¿ŸåŠ è½½è¯æ ¹ç®¡ç†å™¨"""
        if self._root_manager is None:
            try:
                from src.demand_mining.root_word_manager import RootWordManager
                self._root_manager = RootWordManager()
            except ImportError as e:
                print(f"âš ï¸ æ— æ³•å¯¼å…¥è¯æ ¹ç®¡ç†å™¨: {e}")
                self._root_manager = None
        return self._root_manager
    
    def analyze(self, analysis_type: str = 'root_trends', **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶‹åŠ¿åˆ†æ
        
        Args:
            analysis_type: åˆ†æç±»å‹ ('root_trends', 'keyword_trends', 'prediction')
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        if analysis_type == 'root_trends':
            return self._analyze_root_trends(**kwargs)
        elif analysis_type == 'keyword_trends':
            return self._analyze_keyword_trends(**kwargs)
        elif analysis_type == 'prediction':
            return self._predict_trends(**kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
    
    def _analyze_root_trends(self, timeframe: str = None,
                           batch_size: int = 5, output_dir: str = None) -> Dict[str, Any]:
        """åˆ†æè¯æ ¹è¶‹åŠ¿"""
        if timeframe is None:
            from src.utils.constants import GOOGLE_TRENDS_CONFIG
            timeframe = GOOGLE_TRENDS_CONFIG['default_timeframe'].replace('today ', '')
        
        print("ğŸŒ± å¼€å§‹åˆ†æ51ä¸ªè¯æ ¹çš„Google Trendsè¶‹åŠ¿...")
        
        if self.trend_analyzer is None:
            return self._create_empty_trend_result()
        
        try:
            # æ‰§è¡Œåˆ†æ
            results = self.trend_analyzer.analyze_all_root_words(
                timeframe=timeframe, 
                batch_size=batch_size
            )
            
            # ç¡®ä¿è¿”å›æ­£ç¡®çš„ç»“æœæ ¼å¼
            if results is None:
                return self._create_empty_trend_result()
                
            return results
            
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"è¯æ ¹è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return self._create_empty_trend_result()
    
    def _analyze_keyword_trends(self, keywords: List[str],
                              timeframe: str = None, **kwargs) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        if timeframe is None:
            from src.utils.constants import GOOGLE_TRENDS_CONFIG
            timeframe = GOOGLE_TRENDS_CONFIG['default_timeframe'].replace('today ', '')
        
        print(f"ğŸ“Š å¼€å§‹åˆ†æ {len(keywords)} ä¸ªå…³é”®è¯çš„è¶‹åŠ¿...")
        
        if self.trend_analyzer is None:
            return {'error': 'è¶‹åŠ¿åˆ†æå™¨ä¸å¯ç”¨'}
        
        try:
            # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ”¯æŒå…³é”®è¯è¶‹åŠ¿åˆ†æ
            # ä½¿ç”¨æ•°æ®
            trend_results = []
            for keyword in keywords:
                trend_data = {
                    'keyword': keyword,
                    'trend_score': 75,  # è¶‹åŠ¿åˆ†æ•°
                    'growth_rate': '+15%',
                    'peak_interest': 85,
                    'current_interest': 70,
                    'trend_direction': 'rising'
                }
                trend_results.append(trend_data)
            
            result = {
                'analysis_type': 'keyword_trends',
                'analysis_time': datetime.now().isoformat(),
                'total_keywords': len(keywords),
                'timeframe': timeframe,
                'keyword_trends': trend_results,
                'summary': {
                    'rising_keywords': [kw for kw in trend_results if kw['trend_direction'] == 'rising'],
                    'declining_keywords': [kw for kw in trend_results if kw['trend_direction'] == 'declining'],
                    'stable_keywords': [kw for kw in trend_results if kw['trend_direction'] == 'stable']
                }
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ å…³é”®è¯è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {'error': f'åˆ†æå¤±è´¥: {str(e)}'}
    
    def _predict_trends(self, timeframe: str = "30d", 
                       prediction_type: str = "keyword", **kwargs) -> Dict[str, Any]:
        """é¢„æµ‹è¶‹åŠ¿"""
        print(f"ğŸ”® å¼€å§‹é¢„æµ‹æœªæ¥ {timeframe} çš„è¶‹åŠ¿...")
        
        try:
            # åŸºäºå†å²æ•°æ®å’Œå½“å‰è¶‹åŠ¿è¿›è¡Œé¢„æµ‹
            predictions = {
                'prediction_date': datetime.now().isoformat(),
                'timeframe': timeframe,
                'prediction_type': prediction_type,
                'rising_keywords': [
                    {'keyword': 'AI video generator', 'predicted_growth': '+150%', 'confidence': 0.85},
                    {'keyword': 'AI code assistant', 'predicted_growth': '+120%', 'confidence': 0.78},
                    {'keyword': 'AI image upscaler', 'predicted_growth': '+90%', 'confidence': 0.72}
                ],
                'declining_keywords': [
                    {'keyword': 'basic chatbot', 'predicted_decline': '-30%', 'confidence': 0.65},
                    {'keyword': 'simple ai writer', 'predicted_decline': '-20%', 'confidence': 0.58}
                ],
                'stable_keywords': [
                    {'keyword': 'AI generator', 'predicted_change': '+5%', 'confidence': 0.90},
                    {'keyword': 'AI assistant', 'predicted_change': '+10%', 'confidence': 0.88}
                ],
                'emerging_trends': [
                    'AI-powered video editing',
                    'Multimodal AI assistants',
                    'AI-generated music'
                ]
            }
            
            return predictions
            
        except Exception as e:
            print(f"âŒ è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {e}")
            return {'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}
    
    def get_root_word_stats(self) -> Dict[str, Any]:
        """è·å–è¯æ ¹ç»Ÿè®¡ä¿¡æ¯"""
        if self.root_manager is None:
            return {'error': 'è¯æ ¹ç®¡ç†å™¨ä¸å¯ç”¨'}
        
        return self.root_manager.get_stats()
    
    def get_trending_root_words(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨è¯æ ¹"""
        if self.trend_analyzer is None:
            return []
        
        # è¿™é‡Œå¯ä»¥ä»æœ€è¿‘çš„åˆ†æç»“æœä¸­è·å–çƒ­é—¨è¯æ ¹
        # è¿”å›æ•°æ®
        trending_roots = [
            {'word': 'generator', 'average_interest': 85.2, 'growth_rate': '+25%'},
            {'word': 'converter', 'average_interest': 78.9, 'growth_rate': '+18%'},
            {'word': 'detector', 'average_interest': 72.1, 'growth_rate': '+15%'},
            {'word': 'optimizer', 'average_interest': 68.5, 'growth_rate': '+12%'},
            {'word': 'analyzer', 'average_interest': 65.3, 'growth_rate': '+10%'}
        ]
        
        return trending_roots[:limit]
    
    def _calculate_avg_interest(self, trending_words: List[Dict]) -> float:
        """è®¡ç®—å¹³å‡å…´è¶£åº¦"""
        if not trending_words:
            return 0.0
        
        total_interest = sum(word.get('average_interest', 0) for word in trending_words)
        return round(total_interest / len(trending_words), 2)
    
    def _create_empty_trend_result(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„è¶‹åŠ¿åˆ†æç»“æœ"""
        return {
            'analysis_type': 'root_words_trends',
            'analysis_time': datetime.now().isoformat(),
            'total_root_words': 0,
            'successful_analyses': 0,
            'failed_analyses': 0,
            'top_trending_words': [],
            'declining_words': [],
            'stable_words': [],
            'total_keywords': 0,
            'market_insights': {
                'high_opportunity_count': 0,
                'avg_opportunity_score': 0.0
            }
        }
    
    def export_trend_report(self, results: Dict[str, Any], 
                           output_dir: str = None) -> str:
        """å¯¼å‡ºè¶‹åŠ¿æŠ¥å‘Š"""
        if not output_dir:
            output_dir = self.output_dir
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = os.path.join(output_dir, f'trend_report_{timestamp}.md')
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_content = f"""# è¶‹åŠ¿åˆ†ææŠ¥å‘Š

        ## ğŸ“Š åˆ†ææ¦‚è§ˆ
        - **åˆ†ææ—¶é—´**: {results.get('analysis_time', '')}
        - **åˆ†æç±»å‹**: {results.get('analysis_type', '')}
        - **æ€»è¯æ ¹æ•°**: {results.get('total_root_words', 0)}
        - **æˆåŠŸåˆ†æ**: {results.get('successful_analyses', 0)}
        
        ## ğŸ“ˆ ä¸Šå‡è¶‹åŠ¿è¯æ ¹
        """
        
        # æ·»åŠ ä¸Šå‡è¶‹åŠ¿è¯æ ¹
        top_trending = results.get('top_trending_words', [])
        if top_trending:
            for i, word in enumerate(top_trending[:10], 1):
                report_content += f"{i}. **{word.get('word', '')}**: å¹³å‡å…´è¶£åº¦ {word.get('average_interest', 0):.1f}\n"
        else:
            report_content += "æš‚æ— ä¸Šå‡è¶‹åŠ¿è¯æ ¹\n"
        
        # æ·»åŠ ä¸‹é™è¶‹åŠ¿è¯æ ¹
        declining_words = results.get('declining_words', [])
        if declining_words:
            report_content += f"\n## ğŸ“‰ ä¸‹é™è¶‹åŠ¿è¯æ ¹\n"
            for i, word in enumerate(declining_words[:5], 1):
                report_content += f"{i}. **{word.get('word', '')}**: å¹³å‡å…´è¶£åº¦ {word.get('average_interest', 0):.1f}\n"
        
        # æ·»åŠ å»ºè®®
        report_content += f"""
        ## ğŸ’¡ è¶‹åŠ¿å»ºè®®
        
        ### é‡ç‚¹å…³æ³¨
        - ä¼˜å…ˆå¼€å‘ä¸Šå‡è¶‹åŠ¿è¯æ ¹ç›¸å…³çš„AIå·¥å…·
        - å…³æ³¨æ–°å…´æŠ€æœ¯è¯æ ¹çš„å‘å±•æœºä¼š
        - å»ºç«‹è¶‹åŠ¿ç›‘æ§å’Œé¢„è­¦æœºåˆ¶
        
        ### å¸‚åœºæœºä¼š
        - åŸºäºçƒ­é—¨è¯æ ¹åˆ›å»ºäº§å“åŸå‹
        - ç»“åˆAIå‰ç¼€æ‰©å±•å…³é”®è¯ç»„åˆ
        - å…³æ³¨ç«äº‰åº¦è¾ƒä½çš„æ–°å…´è¯æ ¹
        
        ---
        *æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
        """
        
        # ä¿å­˜æŠ¥å‘Š
        os.makedirs(output_dir, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ è¶‹åŠ¿æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path
    
    def batch_trends_analysis(self, keywords: List[str], batch_size: int = 5) -> Dict[str, Any]:
        """
        æ‰¹é‡å…³é”®è¯è¶‹åŠ¿åˆ†æå’Œç¨³å®šæ€§è¯„ä¼°
        
        Args:
            keywords: è¦åˆ†æçš„å…³é”®è¯åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤5ä¸ªå…³é”®è¯ä¸€æ‰¹
            
        Returns:
            åŒ…å«è¶‹åŠ¿åˆ†æç»“æœå’Œç¨³å®šæ€§è¯„åˆ†çš„å­—å…¸
        """
        print(f"ğŸ” å¼€å§‹æ‰¹é‡è¶‹åŠ¿åˆ†æï¼Œå…³é”®è¯æ•°é‡: {len(keywords)}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        results = {
            'total_keywords': len(keywords),
            'batch_size': batch_size,
            'keyword_results': {},
            'summary': {
                'successful': 0,
                'failed': 0,
                'stability_scores': {}
            }
        }
        
        # åˆ†æ‰¹å¤„ç†å…³é”®è¯
        for i in range(0, len(keywords), batch_size):
            batch_keywords = keywords[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            print(f"ğŸ“Š å¤„ç†ç¬¬ {batch_num} æ‰¹å…³é”®è¯: {batch_keywords}")
            
            try:
                # ä½¿ç”¨ç°æœ‰çš„ RootWordTrendsAnalyzer è¿›è¡Œåˆ†æ
                batch_results = self._analyze_keyword_batch(batch_keywords)
                
                # åˆå¹¶æ‰¹æ¬¡ç»“æœ
                for keyword, result in batch_results.items():
                    results['keyword_results'][keyword] = result
                    
                    if result.get('success', False):
                        results['summary']['successful'] += 1
                        # è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
                        stability_score = self._calculate_stability_score(result)
                        results['summary']['stability_scores'][keyword] = stability_score
                    else:
                        results['summary']['failed'] += 1
                        
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                for keyword in batch_keywords:
                    results['keyword_results'][keyword] = {
                        'success': False,
                        'error': str(e),
                        'keyword': keyword
                    }
                    results['summary']['failed'] += 1
        
        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        results['summary']['success_rate'] = (
            results['summary']['successful'] / len(keywords) * 100 
            if len(keywords) > 0 else 0
        )
        
        print(f"âœ… æ‰¹é‡è¶‹åŠ¿åˆ†æå®Œæˆï¼ŒæˆåŠŸç‡: {results['summary']['success_rate']:.1f}%")
        return results
    
    def analyze_keyword_trends(self, keywords: List[str]) -> Dict[str, Any]:
        """
        åˆ†æä»»æ„å…³é”®è¯çš„è¶‹åŠ¿ï¼ˆæ‰©å±•æ”¯æŒéroot wordï¼‰
        
        Args:
            keywords: è¦åˆ†æçš„å…³é”®è¯åˆ—è¡¨
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        print(f"ğŸ” åˆ†æå…³é”®è¯è¶‹åŠ¿: {keywords}")
        
        results = {}
        for keyword in keywords:
            try:
                # å¤ç”¨ç°æœ‰çš„è¶‹åŠ¿åˆ†æé€»è¾‘ï¼Œæ‰©å±•æ”¯æŒä»»æ„å…³é”®è¯
                trend_result = self.root_analyzer.analyze_single_root_word(keyword)
                
                if trend_result and trend_result.get('status') == 'success':
                    # å¤„ç†è¶‹åŠ¿æ•°æ®
                    results[keyword] = {
                        'success': True,
                        'keyword': keyword,
                        'trend_data': trend_result.get('data', {}),
                        'analysis_timestamp': trend_result.get('timestamp', datetime.now().isoformat())
                    }
                else:
                    results[keyword] = {
                        'success': False,
                        'keyword': keyword,
                        'error': trend_result.get('error', 'No trend data available')
                    }
                    
            except Exception as e:
                print(f"âŒ åˆ†æå…³é”®è¯ '{keyword}' è¶‹åŠ¿å¤±è´¥: {e}")
                results[keyword] = {
                    'success': False,
                    'keyword': keyword,
                    'error': str(e)
                }
        
        return results
    
    def _analyze_keyword_batch(self, keywords: List[str]) -> Dict[str, Any]:
        """åˆ†æä¸€æ‰¹å…³é”®è¯"""
        batch_results = {}
        
        for keyword in keywords:
            try:
                # ä½¿ç”¨ç°æœ‰åˆ†æå™¨è·å–è¶‹åŠ¿æ•°æ®
                trend_result = self.root_analyzer.analyze_single_root_word(keyword)
                
                if trend_result and trend_result.get('status') == 'success':
                    batch_results[keyword] = {
                        'success': True,
                        'keyword': keyword,
                        'trend_data': trend_result.get('data', {}),
                        'analysis_timestamp': trend_result.get('timestamp', datetime.now().isoformat())
                    }
                else:
                    batch_results[keyword] = {
                        'success': False,
                        'keyword': keyword,
                        'error': trend_result.get('error', 'No trend data available')
                    }
                    
            except Exception as e:
                print(f"âŒ åˆ†æå…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                batch_results[keyword] = {
                    'success': False,
                    'keyword': keyword,
                    'error': str(e)
                }
        
        return batch_results
    
    def _calculate_stability_score(self, result: Dict[str, Any]) -> float:
        """
        è®¡ç®—è¶‹åŠ¿ç¨³å®šæ€§è¯„åˆ†
        
        Args:
            result: è¶‹åŠ¿åˆ†æç»“æœ
            
        Returns:
            ç¨³å®šæ€§è¯„åˆ† (0-100)
        """
        try:
            trend_data = result.get('trend_data', {})
            
            if not trend_data:
                return 0.0
            
            # åŸºäºè¶‹åŠ¿æ–¹å‘å’Œå…´è¶£åº¦è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
            trend_direction = trend_data.get('trend_direction', 'stable')
            average_interest = trend_data.get('average_interest', 0)
            peak_interest = trend_data.get('peak_interest', 0)
            related_queries_count = len(trend_data.get('related_queries', []))
            
            # åŸºç¡€åˆ†æ•°ï¼šåŸºäºå¹³å‡å…´è¶£åº¦
            base_score = min(average_interest * 2, 50)  # æœ€é«˜50åˆ†
            
            # è¶‹åŠ¿æ–¹å‘åŠ åˆ†
            direction_bonus = {
                'rising': 30,
                'stable': 20,
                'declining': 10
            }.get(trend_direction, 15)
            
            # æ•°æ®ä¸°å¯Œåº¦åŠ åˆ†
            data_richness_bonus = min(related_queries_count * 2, 20)  # æœ€é«˜20åˆ†
            
            total_score = base_score + direction_bonus + data_richness_bonus
            return min(total_score, 100.0)
            
        except Exception as e:
            print(f"âŒ è®¡ç®—ç¨³å®šæ€§è¯„åˆ†å¤±è´¥: {e}")
            return 0.0
    
    def get_trends_data(self, keyword: str, timeframe: str = None, 
                       use_cache: bool = True) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªå…³é”®è¯çš„è¶‹åŠ¿æ•°æ®ï¼ˆé›†æˆç¼“å­˜æœºåˆ¶ï¼‰
        
        Args:
            keyword: å…³é”®è¯
            timeframe: æ—¶é—´èŒƒå›´
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            è¶‹åŠ¿æ•°æ®
        """
        try:
            if timeframe is None:
                from src.utils.constants import GOOGLE_TRENDS_CONFIG
                timeframe = GOOGLE_TRENDS_CONFIG['default_timeframe'].replace('today ', '')
            
            # é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–
            if use_cache and self.batch_config['cache_enabled']:
                cached_result = self.trends_cache.get(keyword, timeframe, "trends")
                if cached_result:
                    print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {keyword}")
                    return {
                        'keyword': keyword,
                        'status': 'success',
                        'data': cached_result['data'],
                        'source': 'cache',
                        'cached_at': cached_result.get('cached_at'),
                        'quality_score': cached_result.get('quality_score', 0.0)
                    }
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä½¿ç”¨ç°æœ‰åˆ†æå™¨è·å–æ•°æ®
            print(f"ğŸ” ä»APIè·å–æ•°æ®: {keyword}")
            result = self.root_analyzer.analyze_single_root_word(keyword, timeframe)
            
            # å¦‚æœè·å–æˆåŠŸï¼Œä¿å­˜åˆ°ç¼“å­˜
            if result and result.get('status') == 'success' and use_cache:
                quality_score = self._calculate_data_quality_score_from_raw(result)
                self.trends_cache.set(
                    keyword=keyword,
                    data=result,
                    timeframe=timeframe,
                    data_type="trends",
                    quality_score=quality_score
                )
                print(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜: {keyword}")
            
            return result
            
        except Exception as e:
            print(f"âŒ è·å–å…³é”®è¯ '{keyword}' è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return {
                'keyword': keyword,
                'status': 'error',
                'error': str(e)
            }
    
    def batch_trends_analysis_optimized(self, keywords: List[str], 
                                      batch_size: int = None,
                                      enable_parallel: bool = False) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„æ‰¹é‡å…³é”®è¯è¶‹åŠ¿åˆ†æ
        
        Args:
            keywords: è¦åˆ†æçš„å…³é”®è¯åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ŒNoneæ—¶ä½¿ç”¨é»˜è®¤é…ç½®
            enable_parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„åˆ†æç»“æœ
        """
        if batch_size is None:
            batch_size = self.batch_config['default_batch_size']
        
        batch_size = min(batch_size, self.batch_config['max_batch_size'])
        
        print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æ‰¹é‡è¶‹åŠ¿åˆ†æï¼Œå…³é”®è¯æ•°é‡: {len(keywords)}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        results = {
            'analysis_type': 'optimized_batch_trends',
            'total_keywords': len(keywords),
            'batch_size': batch_size,
            'parallel_enabled': enable_parallel,
            'keyword_results': {},
            'performance_metrics': {
                'start_time': datetime.now().isoformat(),
                'batches_processed': 0,
                'total_processing_time': 0,
                'avg_time_per_keyword': 0
            },
            'summary': {
                'successful': 0,
                'failed': 0,
                'stability_scores': {},
                'data_quality_scores': {}
            }
        }
        
        start_time = datetime.now()
        
        # åˆ†æ‰¹å¤„ç†å…³é”®è¯
        for i in range(0, len(keywords), batch_size):
            batch_keywords = keywords[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            print(f"ğŸ“Š å¤„ç†ç¬¬ {batch_num} æ‰¹å…³é”®è¯: {batch_keywords}")
            batch_start_time = datetime.now()
            
            try:
                if enable_parallel:
                    # å¹¶è¡Œå¤„ç†ï¼ˆå®éªŒæ€§ï¼‰
                    batch_results = self._analyze_keyword_batch_parallel(batch_keywords)
                else:
                    # ä¸²è¡Œå¤„ç†
                    batch_results = self._analyze_keyword_batch_optimized(batch_keywords)
                
                # åˆå¹¶æ‰¹æ¬¡ç»“æœ
                for keyword, result in batch_results.items():
                    results['keyword_results'][keyword] = result
                    
                    if result.get('success', False):
                        results['summary']['successful'] += 1
                        # è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
                        stability_score = self._calculate_stability_score(result)
                        results['summary']['stability_scores'][keyword] = stability_score
                        
                        # è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
                        quality_score = self._calculate_data_quality_score(result)
                        results['summary']['data_quality_scores'][keyword] = quality_score
                    else:
                        results['summary']['failed'] += 1
                
                results['performance_metrics']['batches_processed'] += 1
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if i + batch_size < len(keywords):  # ä¸æ˜¯æœ€åä¸€æ‰¹
                    import time
                    time.sleep(self.batch_config['delay_between_batches'])
                        
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                for keyword in batch_keywords:
                    results['keyword_results'][keyword] = {
                        'success': False,
                        'error': str(e),
                        'keyword': keyword
                    }
                    results['summary']['failed'] += 1
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        results['performance_metrics']['end_time'] = end_time.isoformat()
        results['performance_metrics']['total_processing_time'] = total_time
        results['performance_metrics']['avg_time_per_keyword'] = (
            total_time / len(keywords) if len(keywords) > 0 else 0
        )
        
        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        results['summary']['success_rate'] = (
            results['summary']['successful'] / len(keywords) * 100 
            if len(keywords) > 0 else 0
        )
        
        print(f"âœ… ä¼˜åŒ–æ‰¹é‡è¶‹åŠ¿åˆ†æå®Œæˆï¼ŒæˆåŠŸç‡: {results['summary']['success_rate']:.1f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’ï¼Œå¹³å‡æ¯ä¸ªå…³é”®è¯: {results['performance_metrics']['avg_time_per_keyword']:.1f}ç§’")
        
        return results
    
    def _analyze_keyword_batch_optimized(self, keywords: List[str]) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ‰¹é‡å…³é”®è¯åˆ†æ"""
        batch_results = {}
        
        for keyword in keywords:
            try:
                # ä½¿ç”¨ç°æœ‰åˆ†æå™¨ï¼Œä½†æ·»åŠ è¶…æ—¶æ§åˆ¶
                import signal
                
                def timeout_handler(signum, frame):
                    raise TimeoutError(f"å…³é”®è¯ '{keyword}' åˆ†æè¶…æ—¶")
                
                # è®¾ç½®è¶…æ—¶
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(self.batch_config['timeout_per_keyword'])
                
                try:
                    trend_result = self.root_analyzer.analyze_single_root_word(keyword)
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    
                    if trend_result and trend_result.get('status') == 'success':
                        # ç»Ÿä¸€æ•°æ®æ ¼å¼
                        formatted_result = self._format_trend_result(trend_result)
                        batch_results[keyword] = formatted_result
                    else:
                        batch_results[keyword] = {
                            'success': False,
                            'keyword': keyword,
                            'error': trend_result.get('error', 'No trend data available')
                        }
                        
                except TimeoutError as te:
                    signal.alarm(0)
                    batch_results[keyword] = {
                        'success': False,
                        'keyword': keyword,
                        'error': str(te)
                    }
                    
            except Exception as e:
                print(f"âŒ åˆ†æå…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                batch_results[keyword] = {
                    'success': False,
                    'keyword': keyword,
                    'error': str(e)
                }
        
        return batch_results
    
    def _analyze_keyword_batch_parallel(self, keywords: List[str]) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰¹é‡å…³é”®è¯åˆ†æï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰"""
        import concurrent.futures
        import threading
        
        batch_results = {}
        
        def analyze_single_keyword(keyword):
            try:
                trend_result = self.root_analyzer.analyze_single_root_word(keyword)
                if trend_result and trend_result.get('status') == 'success':
                    return keyword, self._format_trend_result(trend_result)
                else:
                    return keyword, {
                        'success': False,
                        'keyword': keyword,
                        'error': trend_result.get('error', 'No trend data available')
                    }
            except Exception as e:
                return keyword, {
                    'success': False,
                    'keyword': keyword,
                    'error': str(e)
                }
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼ˆæ³¨æ„ï¼šç”±äºAPIé™åˆ¶ï¼Œå®é™…æ•ˆæœå¯èƒ½æœ‰é™ï¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_keyword = {
                executor.submit(analyze_single_keyword, keyword): keyword 
                for keyword in keywords
            }
            
            for future in concurrent.futures.as_completed(future_to_keyword):
                keyword, result = future.result()
                batch_results[keyword] = result
        
        return batch_results
    
    def _format_trend_result(self, trend_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿä¸€è¶‹åŠ¿ç»“æœæ•°æ®æ ¼å¼"""
        try:
            return {
                'success': True,
                'keyword': trend_result.get('root_word', ''),
                'trend_data': trend_result.get('data', {}),
                'analysis_timestamp': trend_result.get('timestamp', datetime.now().isoformat()),
                'data_source': 'RootWordTrendsAnalyzer',
                'format_version': '1.0'
            }
        except Exception as e:
            return {
                'success': False,
                'keyword': trend_result.get('root_word', ''),
                'error': f'æ ¼å¼åŒ–å¤±è´¥: {str(e)}',
                'raw_data': trend_result
            }
    
    def _calculate_data_quality_score(self, result: Dict[str, Any]) -> float:
        """
        è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
        
        Args:
            result: è¶‹åŠ¿åˆ†æç»“æœ
            
        Returns:
            æ•°æ®è´¨é‡è¯„åˆ† (0-100)
        """
        try:
            trend_data = result.get('trend_data', {})
            
            if not trend_data:
                return 0.0
            
            quality_score = 0.0
            
            # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ (40åˆ†)
            required_fields = ['keyword', 'average_interest', 'trend_direction', 'related_queries']
            available_fields = sum(1 for field in required_fields if field in trend_data)
            completeness_score = (available_fields / len(required_fields)) * 40
            quality_score += completeness_score
            
            # ç›¸å…³æŸ¥è¯¢æ•°é‡ (30åˆ†)
            related_queries = trend_data.get('related_queries', [])
            queries_score = min(len(related_queries) / 10 * 30, 30)  # 10ä¸ªæŸ¥è¯¢ä¸ºæ»¡åˆ†
            quality_score += queries_score
            
            # æ•°æ®æ–°é²œåº¦ (20åˆ†)
            timestamp = result.get('analysis_timestamp', '')
            if timestamp:
                try:
                    analysis_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    time_diff = (datetime.now() - analysis_time).total_seconds()
                    # 1å°æ—¶å†…ä¸ºæ»¡åˆ†ï¼Œè¶…è¿‡24å°æ—¶ä¸º0åˆ†
                    freshness_score = max(0, min(20, 20 * (1 - time_diff / 86400)))
                    quality_score += freshness_score
                except:
                    pass
            
            # è¶‹åŠ¿ç¨³å®šæ€§ (10åˆ†)
            if trend_data.get('trend_direction') in ['rising', 'stable']:
                quality_score += 10
            elif trend_data.get('trend_direction') == 'declining':
                quality_score += 5
            
            return min(quality_score, 100.0)
            
        except Exception as e:
            print(f"âŒ è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†å¤±è´¥: {e}")
            return 0.0
    
    def get_supported_keywords_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„å…³é”®è¯ç±»å‹"""
        return [
            'root_words',      # è¯æ ¹
            'ai_tools',        # AIå·¥å…·
            'tech_terms',      # æŠ€æœ¯æœ¯è¯­
            'product_names',   # äº§å“åç§°
            'brand_names',     # å“ç‰Œåç§°
            'generic_terms',   # é€šç”¨æœ¯è¯­
            'long_tail',       # é•¿å°¾å…³é”®è¯
            'compound_terms'   # å¤åˆè¯
        ]
    
    def validate_keywords(self, keywords: List[str]) -> Dict[str, Any]:
        """
        éªŒè¯å…³é”®è¯åˆ—è¡¨çš„æœ‰æ•ˆæ€§
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœ
        """
        validation_result = {
            'total_keywords': len(keywords),
            'valid_keywords': [],
            'invalid_keywords': [],
            'warnings': [],
            'recommendations': []
        }
        
        for keyword in keywords:
            if not keyword or not keyword.strip():
                validation_result['invalid_keywords'].append({
                    'keyword': keyword,
                    'reason': 'ç©ºå…³é”®è¯'
                })
                continue
            
            keyword = keyword.strip()
            
            # é•¿åº¦æ£€æŸ¥
            if len(keyword) > 100:
                validation_result['warnings'].append({
                    'keyword': keyword,
                    'warning': 'å…³é”®è¯è¿‡é•¿ï¼Œå¯èƒ½å½±å“åˆ†ææ•ˆæœ'
                })
            
            # ç‰¹æ®Šå­—ç¬¦æ£€æŸ¥
            if any(char in keyword for char in ['<', '>', '&', '"', "'"]):
                validation_result['warnings'].append({
                    'keyword': keyword,
                    'warning': 'åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œå¯èƒ½å½±å“æœç´¢ç»“æœ'
                })
            
            validation_result['valid_keywords'].append(keyword)
        
        # ç”Ÿæˆå»ºè®®
        if len(validation_result['valid_keywords']) > 50:
            validation_result['recommendations'].append(
                'å…³é”®è¯æ•°é‡è¾ƒå¤šï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†ä»¥æé«˜æˆåŠŸç‡'
            )
        
        if len(validation_result['invalid_keywords']) > 0:
            validation_result['recommendations'].append(
                'è¯·æ¸…ç†æ— æ•ˆå…³é”®è¯åé‡æ–°åˆ†æ'
            )
        
        return validation_result
    
    def _calculate_data_quality_score_from_raw(self, raw_result: Dict[str, Any]) -> float:
        """
        ä»åŸå§‹ç»“æœè®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
        
        Args:
            raw_result: åŸå§‹åˆ†æç»“æœ
            
        Returns:
            æ•°æ®è´¨é‡è¯„åˆ† (0-100)
        """
        try:
            if not raw_result or raw_result.get('status') != 'success':
                return 0.0
            
            data = raw_result.get('data', {})
            if not data:
                return 0.0
            
            quality_score = 0.0
            
            # åŸºç¡€æ•°æ®å®Œæ•´æ€§ (40åˆ†)
            required_fields = ['keyword', 'average_interest', 'trend_direction', 'related_queries']
            available_fields = sum(1 for field in required_fields if field in data)
            quality_score += (available_fields / len(required_fields)) * 40
            
            # ç›¸å…³æŸ¥è¯¢æ•°é‡ (30åˆ†)
            related_queries = data.get('related_queries', [])
            quality_score += min(len(related_queries) / 10 * 30, 30)
            
            # å¹³å‡å…´è¶£åº¦ (20åˆ†)
            avg_interest = data.get('average_interest', 0)
            quality_score += min(avg_interest / 50 * 20, 20)
            
            # è¶‹åŠ¿æ–¹å‘æœ‰æ•ˆæ€§ (10åˆ†)
            trend_direction = data.get('trend_direction', '')
            if trend_direction in ['rising', 'stable', 'declining']:
                quality_score += 10
            
            return min(quality_score, 100.0)
            
        except Exception as e:
            print(f"âŒ è®¡ç®—åŸå§‹æ•°æ®è´¨é‡è¯„åˆ†å¤±è´¥: {e}")
            return 0.0
    
    def enable_offline_mode(self, keywords: List[str] = None) -> Dict[str, Any]:
        """
        å¯ç”¨ç¦»çº¿æ¨¡å¼
        
        Args:
            keywords: è¦é¢„åŠ è½½çš„å…³é”®è¯åˆ—è¡¨ï¼ŒNoneæ—¶ä½¿ç”¨é»˜è®¤è¯æ ¹
            
        Returns:
            ç¦»çº¿æ¨¡å¼å‡†å¤‡ç»“æœ
        """
        if keywords is None:
            # ä½¿ç”¨é»˜è®¤çš„51ä¸ªè¯æ ¹
            keywords = self.root_analyzer.root_words if hasattr(self.root_analyzer, 'root_words') else []
        
        print(f"ğŸ”„ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼Œé¢„åŠ è½½ {len(keywords)} ä¸ªå…³é”®è¯...")
        
        # å¯ç”¨ç¦»çº¿æ¨¡å¼é…ç½®
        self.batch_config['offline_mode'] = True
        
        # é¢„åŠ è½½ç¼“å­˜
        offline_result = self.trends_cache.enable_offline_mode(keywords)
        
        # å¯¹äºç¼ºå¤±çš„å…³é”®è¯ï¼Œå°è¯•è·å–å¹¶ç¼“å­˜
        missing_keywords = offline_result.get('missing_keywords', [])
        if missing_keywords:
            print(f"ğŸ“¥ é¢„åŠ è½½ç¼ºå¤±çš„ {len(missing_keywords)} ä¸ªå…³é”®è¯...")
            
            for keyword in missing_keywords[:10]:  # é™åˆ¶é¢„åŠ è½½æ•°é‡
                try:
                    self.get_trends_data(keyword, use_cache=True)
                    import time
                    time.sleep(2)  # é¿å…APIé™åˆ¶
                except Exception as e:
                    print(f"âš ï¸ é¢„åŠ è½½å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
        
        # æ›´æ–°ç¦»çº¿æ¨¡å¼çŠ¶æ€
        final_result = self.trends_cache.enable_offline_mode(keywords)
        
        print(f"âœ… ç¦»çº¿æ¨¡å¼å‡†å¤‡å®Œæˆï¼Œå¯ç”¨å…³é”®è¯: {len(final_result.get('cached_keywords', []))}")
        return final_result
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.trends_cache.get_cache_stats()
    
    def clear_cache(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        print("ğŸ—‘ï¸ æ¸…ç©ºè¶‹åŠ¿æ•°æ®ç¼“å­˜...")
        result = self.trends_cache.clear_all()
        if result:
            print("âœ… ç¼“å­˜å·²æ¸…ç©º")
        else:
            print("âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥")
        return result
    
    def export_cache_backup(self, backup_path: str = None) -> str:
        """å¯¼å‡ºç¼“å­˜å¤‡ä»½"""
        print("ğŸ“¦ å¯¼å‡ºç¼“å­˜å¤‡ä»½...")
        backup_file = self.trends_cache.export_cache_backup(backup_path)
        if backup_file:
            print(f"âœ… ç¼“å­˜å¤‡ä»½å·²å¯¼å‡º: {backup_file}")
        else:
            print("âŒ å¯¼å‡ºç¼“å­˜å¤‡ä»½å¤±è´¥")
        return backup_file
    
    def batch_trends_analysis_with_cache(self, keywords: List[str], 
                                       batch_size: int = 5,
                                       force_refresh: bool = False) -> Dict[str, Any]:
        """
        å¸¦ç¼“å­˜çš„æ‰¹é‡è¶‹åŠ¿åˆ†æ
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            åˆ†æç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹å¸¦ç¼“å­˜çš„æ‰¹é‡è¶‹åŠ¿åˆ†æï¼Œå…³é”®è¯æ•°é‡: {len(keywords)}")
        
        results = {
            'total_keywords': len(keywords),
            'batch_size': batch_size,
            'force_refresh': force_refresh,
            'keyword_results': {},
            'cache_performance': {
                'cache_hits': 0,
                'cache_misses': 0,
                'api_calls': 0
            },
            'summary': {
                'successful': 0,
                'failed': 0,
                'stability_scores': {},
                'quality_scores': {}
            }
        }
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(keywords), batch_size):
            batch_keywords = keywords[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            print(f"ğŸ“Š å¤„ç†ç¬¬ {batch_num} æ‰¹å…³é”®è¯: {batch_keywords}")
            
            for keyword in batch_keywords:
                try:
                    # è·å–è¶‹åŠ¿æ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼‰
                    result = self.get_trends_data(
                        keyword, 
                        use_cache=not force_refresh
                    )
                    
                    if result:
                        results['keyword_results'][keyword] = result
                        
                        # ç»Ÿè®¡ç¼“å­˜æ€§èƒ½
                        if result.get('source') == 'cache':
                            results['cache_performance']['cache_hits'] += 1
                        else:
                            results['cache_performance']['cache_misses'] += 1
                            results['cache_performance']['api_calls'] += 1
                        
                        # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
                        if result.get('status') == 'success':
                            results['summary']['successful'] += 1
                            
                            # è®¡ç®—è¯„åˆ†
                            if 'data' in result:
                                stability_score = self._calculate_stability_score({
                                    'trend_data': result['data']
                                })
                                results['summary']['stability_scores'][keyword] = stability_score
                                
                                quality_score = result.get('quality_score', 0.0)
                                results['summary']['quality_scores'][keyword] = quality_score
                        else:
                            results['summary']['failed'] += 1
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                    results['keyword_results'][keyword] = {
                        'keyword': keyword,
                        'status': 'error',
                        'error': str(e)
                    }
                    results['summary']['failed'] += 1
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆä»…åœ¨æœ‰APIè°ƒç”¨æ—¶ï¼‰
            if results['cache_performance']['api_calls'] > 0 and i + batch_size < len(keywords):
                import time
                time.sleep(self.batch_config['delay_between_batches'])
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_requests = results['cache_performance']['cache_hits'] + results['cache_performance']['cache_misses']
        results['cache_performance']['hit_rate'] = (
            results['cache_performance']['cache_hits'] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        results['summary']['success_rate'] = (
            results['summary']['successful'] / len(keywords) * 100
            if len(keywords) > 0 else 0
        )
        
        print(f"âœ… å¸¦ç¼“å­˜æ‰¹é‡åˆ†æå®Œæˆ")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {results['summary']['success_rate']:.1f}%")
        print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {results['cache_performance']['hit_rate']:.1f}%")
        print(f"ğŸ”Œ APIè°ƒç”¨æ¬¡æ•°: {results['cache_performance']['api_calls']}")
        
        return results