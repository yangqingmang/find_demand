#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Â§öÂπ≥Âè∞ÂÖ≥ÈîÆËØçÂèëÁé∞Â∑•ÂÖ∑
Êï¥ÂêàReddit„ÄÅHacker News„ÄÅProduct HuntÁ≠âÂπ≥Âè∞ÁöÑÂÖ≥ÈîÆËØçÂèëÁé∞
"""

import asyncio
import json
import random
import re
import time
import hashlib
import os
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any, Awaitable, Callable, Dict, List, Optional, Tuple
from urllib.parse import quote, urlparse

import aiohttp
import pandas as pd
from threading import Lock

from src.utils.telemetry import telemetry_manager
from src.utils.reddit_client import RedditOAuthClient
from src.utils.rss_parser import parse_rss


_EMBEDDING_MODEL_CACHE: Dict[str, Any] = {}
_EMBEDDING_MODEL_LOCK = Lock()


_DEFAULT_BRAND_PHRASES = {
    'chatgpt', 'openai', 'gpt', 'gpt-4', 'gpt-3.5', 'claude', 'anthropic',
    'midjourney', 'deepseek', 'stable diffusion', 'runwayml', 'runway',
    'perplexity ai', 'perplexity', 'janitor ai', 'hix bypass', 'undetectable ai',
    'turnitin', 'google gemini', 'gemini ai', 'bard ai', 'character ai',
    'microsoft copilot', 'bing copilot', 'copilot', 'notion', 'notion ai',
    'zapier', 'make.com', 'ifttt', 'adobe', 'photoshop', 'illustrator', 'canva',
    'figma', 'loom', 'descript', 'salesforce', 'hubspot', 'oracle', 'sap', 'aws',
    'aws bedrock', 'azure', 'azure openai', 'google cloud', 'meta ai', 'tesla',
    'bmw', 'mercedes', 'toyota', 'volkswagen', 'nio', 'xpeng', 'xiaomi',
    'huawei', 'apple', 'iphone', 'macbook', 'samsung', 'sony', 'dji', 'nvidia',
    'geforce', 'rtx', 'playstation', 'xbox', 'openpilot', 'gpt4', 'gpt5'
}

_DEFAULT_BRAND_MODIFIERS = {
    'lifetime deal', 'affiliate', 'pricing plan', 'pricing', 'free trial',
    'discount', 'discount code', 'coupon', 'coupon code', 'redeem', 'official',
    'official site', 'review', 'reviews', 'vs', 'alternative', 'alternatives',
    'compare', 'comparison', 'login', 'sign up', 'signup', 'account',
    'download', 'downloads', 'app', 'apps', 'premium', 'pro', 'plus',
    'enterprise', 'pricing tier', 'pricing tiers', 'starter plan', 'plan',
    'plans', 'lifetime', 'promo', 'deal', 'black friday', 'cyber monday'
}

_DEFAULT_BRAND_TOKENS = {
    'chatgpt', 'openai', 'gpt', 'gpt4', 'gpt-4', 'gpt5', 'gpt-5', 'gpt3',
    'gpt-3', 'claude', 'anthropic', 'midjourney', 'deepseek', 'runwayml',
    'runway', 'perplexity', 'janitor', 'turnitin', 'hix', 'bypass',
    'undetectable', 'gemini', 'bard', 'character', 'copilot', 'notion', 'zapier',
    'ifttt', 'adobe', 'photoshop', 'illustrator', 'canva', 'figma', 'loom',
    'descript', 'salesforce', 'hubspot', 'oracle', 'sap', 'aws', 'azure',
    'tesla', 'bmw', 'mercedes', 'toyota', 'volkswagen', 'nio', 'xpeng',
    'xiaomi', 'huawei', 'apple', 'iphone', 'macbook', 'samsung', 'sony', 'dji',
    'nvidia', 'geforce', 'rtx', 'playstation', 'xbox', 'openpilot'
}


class _AsyncRateLimiter:
    """Simple per-host async rate limiter using monotonic clocks."""

    def __init__(self, rate_limits: Dict[str, float], default_interval: float) -> None:
        self._rate_limits = {str(host).lower(): float(interval) for host, interval in rate_limits.items()}
        self._default_interval = max(float(default_interval), 0.0)
        self._locks: Dict[str, asyncio.Lock] = {}
        self._last_hit: Dict[str, float] = {}

    def _get_interval(self, host: str) -> float:
        normalized = (host or '').lower()
        interval = self._rate_limits.get(normalized)
        if interval is None:
            interval = self._rate_limits.get('*', self._default_interval)
        return max(float(interval), 0.0)

    async def throttle(self, host: str) -> None:
        interval = self._get_interval(host)
        if interval <= 0:
            return

        key = (host or '*').lower()
        lock = self._locks.setdefault(key, asyncio.Lock())
        async with lock:
            now = time.monotonic()
            last_time = self._last_hit.get(key, 0.0)
            wait_time = interval - (now - last_time)
            if wait_time > 0:
                await asyncio.sleep(wait_time)
                now = time.monotonic()
            self._last_hit[key] = now


class _TTLCache:
    """Disk-backed TTL cache used by keyword discovery."""

    def __init__(
        self,
        cache_dir: Path,
        ttl_seconds: int,
        enabled: bool,
        cache_store: Dict[str, Dict[str, Any]],
        normalizer: Callable[[str], str],
    ) -> None:
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl_seconds = max(int(ttl_seconds), 0)
        self.enabled = bool(enabled) and self.ttl_seconds != 0
        self._store = cache_store
        self._normalize = normalizer

    def _namespace_path(self, namespace: str) -> Path:
        path = self.cache_dir / namespace
        path.mkdir(parents=True, exist_ok=True)
        return path

    def get(self, namespace: str, key: str):
        if not self.enabled:
            return None

        now = time.time()
        store = self._store.setdefault(namespace, {})
        entry = store.get(key)
        if entry:
            timestamp = entry.get('timestamp', 0.0)
            if self.ttl_seconds <= 0 or now - timestamp <= self.ttl_seconds:
                return entry.get('payload')
            store.pop(key, None)

        path = self._namespace_path(namespace) / f"{self._normalize(key)}.json"
        if not path.exists():
            return None

        try:
            with path.open('r', encoding='utf-8') as fh:
                data = json.load(fh)
        except Exception:
            return None

        timestamp = data.get('timestamp', 0.0)
        if self.ttl_seconds > 0 and now - timestamp > self.ttl_seconds:
            try:
                path.unlink()
            except Exception:
                pass
            return None

        payload = data.get('payload')
        store[key] = {'payload': payload, 'timestamp': timestamp or now}
        return payload

    def set(self, namespace: str, key: str, payload: Any) -> None:
        if not self.enabled:
            return

        timestamp = time.time()
        store = self._store.setdefault(namespace, {})
        store[key] = {'payload': payload, 'timestamp': timestamp}

        path = self._namespace_path(namespace) / f"{self._normalize(key)}.json"
        try:
            with path.open('w', encoding='utf-8') as fh:
                json.dump({'timestamp': timestamp, 'payload': payload}, fh, ensure_ascii=False)
        except Exception as exc:
            print(f"‚ö†Ô∏è ÁºìÂ≠òÂÜôÂÖ•Â§±Ë¥• ({namespace}:{key}): {exc}")

# Ê∑ªÂä†configÁõÆÂΩïÂà∞Ë∑ØÂæÑ
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'config'))
from config.config_manager import get_config
from config.crypto_manager import ConfigCrypto
from src.pipeline.cleaning.cleaner import standardize_term, is_valid_term, CleaningConfig

class MultiPlatformKeywordDiscovery:
    """Â§öÂπ≥Âè∞ÂÖ≥ÈîÆËØçÂèëÁé∞Â∑•ÂÖ∑"""
    
    def __init__(self):
        # Âä†ËΩΩÈÖçÁΩÆ
        self.config = get_config()

        # Âü∫Á°ÄËØ∑Ê±Ç‰∏éÂπ∂ÂèëÊéßÂà∂
        base_timeout = getattr(self.config, 'DISCOVERY_REQUEST_TIMEOUT', 12) or 12
        base_max_retries = getattr(self.config, 'DISCOVERY_MAX_RETRIES', 3) or 3
        base_backoff = getattr(self.config, 'DISCOVERY_RETRY_BACKOFF', (1, 3))
        base_max_concurrency = getattr(self.config, 'DISCOVERY_MAX_CONCURRENCY', 5) or 5
        base_default_rate_interval = getattr(self.config, 'DISCOVERY_DEFAULT_RATE_INTERVAL', 0.35) or 0.35
        base_cache_dir = getattr(self.config, 'DISCOVERY_CACHE_DIR', 'output/cache/discovery') or 'output/cache/discovery'
        base_cache_enabled = getattr(self.config, 'DISCOVERY_CACHE_ENABLED', True)
        base_cache_ttl = getattr(self.config, 'DISCOVERY_CACHE_TTL', 3600)
        rate_override = getattr(self.config, 'DISCOVERY_RATE_LIMITS', None)

        runtime_cfg = self._load_discovery_runtime_config()
        if isinstance(runtime_cfg, dict):
            base_timeout = runtime_cfg.get('request_timeout', base_timeout)
            base_max_retries = runtime_cfg.get('max_retries', base_max_retries)
            if runtime_cfg.get('retry_backoff') is not None:
                base_backoff = runtime_cfg.get('retry_backoff')
            base_max_concurrency = runtime_cfg.get('max_concurrency', base_max_concurrency)
            base_default_rate_interval = runtime_cfg.get('default_rate_interval', base_default_rate_interval)
            base_cache_dir = runtime_cfg.get('cache_dir', base_cache_dir)
            if 'cache_enabled' in runtime_cfg:
                base_cache_enabled = runtime_cfg.get('cache_enabled')
            base_cache_ttl = runtime_cfg.get('cache_ttl_seconds', base_cache_ttl)
            if runtime_cfg.get('rate_limits') is not None:
                rate_override = runtime_cfg.get('rate_limits')

        embedding_cfg = {}
        if isinstance(runtime_cfg, dict):
            embedding_cfg = runtime_cfg.get('embeddings', {}) or {}
        base_embedding_enabled = getattr(self.config, 'DISCOVERY_EMBEDDINGS_ENABLED', True)
        base_embedding_min_keywords = getattr(self.config, 'DISCOVERY_EMBEDDINGS_MIN_KEYWORDS', 5)
        base_embedding_model = getattr(self.config, 'DISCOVERY_EMBEDDINGS_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')
        self.embedding_enabled = bool(embedding_cfg.get('enabled', base_embedding_enabled))
        try:
            self.embedding_min_keywords = max(int(embedding_cfg.get('min_keywords', base_embedding_min_keywords)), 1)
        except (TypeError, ValueError):
            try:
                self.embedding_min_keywords = max(int(base_embedding_min_keywords), 1)
            except (TypeError, ValueError):
                self.embedding_min_keywords = 5
        self.embedding_model_name = str(embedding_cfg.get('model_name', base_embedding_model)) or base_embedding_model
        self._embedding_model = None

        self.user_agent = (
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
            'AppleWebKit/537.36 (KHTML, like Gecko)'
        )

        try:
            self.request_timeout = int(float(base_timeout))
        except (TypeError, ValueError):
            self.request_timeout = 12

        try:
            self.max_retries = max(int(base_max_retries), 1)
        except (TypeError, ValueError):
            self.max_retries = 3

        backoff_cfg = base_backoff
        if isinstance(backoff_cfg, (list, tuple)) and len(backoff_cfg) == 2:
            try:
                self.retry_backoff = (float(backoff_cfg[0]), float(backoff_cfg[1]))
            except (TypeError, ValueError):
                self.retry_backoff = (1.0, 3.0)
        else:
            self.retry_backoff = (1.0, 3.0)

        try:
            self.max_concurrency = max(int(base_max_concurrency), 1)
        except (TypeError, ValueError):
            self.max_concurrency = 5

        try:
            self.default_rate_interval = float(base_default_rate_interval)
        except (TypeError, ValueError):
            self.default_rate_interval = 0.35

        cache_dir_value = str(base_cache_dir)
        self.cache_dir = Path(cache_dir_value)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self._cache_enabled = bool(base_cache_enabled)
        try:
            self._cache_ttl = int(base_cache_ttl)
        except (TypeError, ValueError):
            self._cache_ttl = 3600
        if self._cache_ttl <= 0:
            self._cache_enabled = False

        self._cache_store: Dict[str, Dict[str, Any]] = {}
        self._reconfigure_cache_backend()

        self._per_host_rate_limits = self._build_rate_limit_map(rate_override)
        self._rate_limiter = _AsyncRateLimiter(self._per_host_rate_limits, self.default_rate_interval)
        # Âπ≥Âè∞ÈÖçÁΩÆ
        self.platforms = {
            'reddit': {
                'base_url': 'https://www.reddit.com',
                'api_url': 'https://www.reddit.com/r/{}/search.json',
                'enabled': True
            },
            'hackernews': {
                'base_url': 'https://hacker-news.firebaseio.com/v0',
                'search_url': 'https://hn.algolia.com/api/v1/search',
                'enabled': True
            },
            'producthunt': {
                'base_url': 'https://api.producthunt.com/v2/api/graphql',
                'enabled': bool(self.config.PRODUCTHUNT_API_TOKEN)
            }
        }

        # ËÆæÁΩÆProductHuntËÆ§ËØÅÂ§¥
        self.ph_headers = {}
        if self.config.PRODUCTHUNT_API_TOKEN:
            self.ph_headers = {
                'Authorization': f'Bearer {self.config.PRODUCTHUNT_API_TOKEN}',
                'Content-Type': 'application/json',
                'Accept': 'application/json'
            }

        reddit_api_cfg = getattr(self.config, 'REDDIT_API', {}) if hasattr(self.config, 'REDDIT_API') else {}
        if not isinstance(reddit_api_cfg, dict):
            reddit_api_cfg = {}
        self.reddit_use_rss = bool(reddit_api_cfg.get('use_rss', True))
        self.reddit_client: Optional[RedditOAuthClient] = None
        if isinstance(reddit_api_cfg, dict) and reddit_api_cfg.get('enabled', False) and not self.reddit_use_rss:
            encrypted_payload = reddit_api_cfg.get('credentials_encrypted')
            decrypted_credentials = None
            if encrypted_payload:
                try:
                    decrypted_credentials = ConfigCrypto().decrypt_config(encrypted_payload)
                except Exception as exc:
                    print(f"‚ö†Ô∏è Reddit Âá≠ËØÅËß£ÂØÜÂ§±Ë¥•: {exc}")
            if isinstance(decrypted_credentials, dict):
                credentials = {k: v for k, v in decrypted_credentials.items() if v}
                user_agent_override = reddit_api_cfg.get('user_agent')
                if user_agent_override:
                    credentials.setdefault('user_agent', user_agent_override)
                token_cache_path = reddit_api_cfg.get('token_cache_path') or os.path.join(self.cache_dir, 'reddit_token.json')
                refresh_margin = int(reddit_api_cfg.get('token_refresh_margin', 300) or 300)
                try:
                    self.reddit_client = RedditOAuthClient(
                        credentials,
                        token_cache_path=Path(token_cache_path),
                        refresh_margin=refresh_margin,
                        verbose=bool(reddit_api_cfg.get('verbose', False))
                    )
                except Exception as exc:
                    print(f"‚ö†Ô∏è Reddit OAuth ÂàùÂßãÂåñÂ§±Ë¥•: {exc}")

        filters_cfg = self._load_discovery_filter_config()

        def _extract_filter_terms(payload: Any) -> List[str]:
            terms: List[str] = []
            if isinstance(payload, dict):
                for key in ('terms', 'values', 'items', 'entries', 'list', 'data'):
                    value = payload.get(key)
                    if isinstance(value, (list, tuple, set)):
                        terms.extend(value)
                if isinstance(payload.get('value'), str):
                    terms.append(payload['value'])
            elif isinstance(payload, (list, tuple, set)):
                terms.extend(payload)
            elif isinstance(payload, str):
                terms.append(payload)
            normalized: List[str] = []
            for term in terms:
                if isinstance(term, str):
                    cleaned = term.strip().lower()
                    if cleaned:
                        normalized.append(cleaned)
            return normalized

        def _resolve_filter_terms(defaults: set, list_key: str, mode_key: str) -> set:
            raw_values = filters_cfg.get(list_key)
            mode_candidate = filters_cfg.get(mode_key, 'extend')
            if isinstance(raw_values, dict) and 'mode' in raw_values:
                mode_candidate = raw_values.get('mode', mode_candidate)
            mode = str(mode_candidate).lower() if isinstance(mode_candidate, str) else 'extend'
            if mode not in ('extend', 'replace', 'disable'):
                mode = 'extend'
            if mode == 'disable':
                return set()
            resolved = set()
            if mode == 'extend':
                resolved.update(term.lower() for term in defaults if isinstance(term, str))
            values = _extract_filter_terms(raw_values)
            if mode == 'replace':
                resolved.clear()
            resolved.update(values)
            return {term for term in resolved if term}

        self.brand_filter_config = {
            'enabled': bool(filters_cfg.get('brand_filter_enabled', True)),
            'strict_brand_modifiers': bool(filters_cfg.get('strict_brand_modifiers', False)),
            'min_non_brand_tokens': max(int(filters_cfg.get('min_non_brand_tokens', 2) or 0), 0),
            'hard_exclude_brand_seeds': bool(filters_cfg.get('hard_exclude_brand_seeds', True)),
        }
        self.generic_filter_config = {
            'enabled': bool(filters_cfg.get('generic_filter_enabled', True))
        }

        resolved_brand_phrases = _resolve_filter_terms(_DEFAULT_BRAND_PHRASES, 'brand_phrases', 'brand_phrases_mode')
        if self.brand_filter_config.get('enabled', True) and not resolved_brand_phrases:
            resolved_brand_phrases = {term.lower() for term in _DEFAULT_BRAND_PHRASES}
        self.brand_phrases = resolved_brand_phrases

        resolved_brand_modifiers = _resolve_filter_terms(_DEFAULT_BRAND_MODIFIERS, 'brand_modifiers', 'brand_modifiers_mode')
        if self.brand_filter_config.get('enabled', True) and not resolved_brand_modifiers:
            resolved_brand_modifiers = {term.lower() for term in _DEFAULT_BRAND_MODIFIERS}
        self.brand_modifier_tokens = resolved_brand_modifiers

        brand_tokens = {str(token).strip().lower() for token in _DEFAULT_BRAND_TOKENS}
        generic_token_stops = {
            'ai', 'app', 'apps', 'plan', 'plans', 'code', 'codes', 'deal', 'deals',
            'free', 'trial', 'official', 'site', 'pricing', 'price', 'pro', 'plus',
            'vs', 'compare', 'comparison', 'review', 'reviews', 'login', 'signup',
            'sign', 'account', 'the', 'for', 'and', 'with', 'tool', 'tools',
            'software', 'platform', 'service', 'services'
        }
        for phrase in self.brand_phrases:
            normalized_phrase = phrase.strip().lower()
            if not normalized_phrase:
                continue
            if ' ' not in normalized_phrase:
                brand_tokens.add(normalized_phrase)
            for token in re.split(r'[^a-z0-9]+', normalized_phrase):
                token = token.strip()
                if len(token) < 3:
                    continue
                if token in generic_token_stops:
                    continue
                brand_tokens.add(token)
        self.brand_tokens = brand_tokens

        seed_cfg = self._load_discovery_seed_config()
        self.seed_profiles = seed_cfg.get('profiles', {}) if isinstance(seed_cfg.get('profiles'), dict) else {}
        self.default_seed_profile = seed_cfg.get('default_profile') or next(iter(self.seed_profiles.keys()), None)
        self.min_seed_terms = max(int(seed_cfg.get('min_terms', 3) or 0), 1)
        enrichment_cfg = seed_cfg.get('enrichment', {}) if isinstance(seed_cfg, dict) else {}
        if not isinstance(enrichment_cfg, dict):
            enrichment_cfg = {}
        self.seed_enrichment_cfg = enrichment_cfg
        self.brand_ratio_limit = float(enrichment_cfg.get('brand_ratio_limit', 0.3) or 0.3)
        self.manual_seed_backfill = max(int(enrichment_cfg.get('manual_seed_backfill', 5) or 0), 0)
        self.priority_seed_limit = int(enrichment_cfg.get('priority_seed_limit', 5) or 0)
        if self.priority_seed_limit < 0:
            self.priority_seed_limit = 0
        self.min_manual_seed_on_shortage = max(int(enrichment_cfg.get('min_manual_seed_on_shortage', 0) or 0), 0)

        manual_seed_source = enrichment_cfg.get('manual_seed_file') or enrichment_cfg.get('manual_terms')
        positive_seed_source = enrichment_cfg.get('positive_seed_file') or enrichment_cfg.get('positive_terms')
        self.manual_seed_terms = self._load_seed_source(
            manual_seed_source,
            fallback=enrichment_cfg.get('manual_terms'),
            label='ÊâãÂä®ÁßçÂ≠ê',
            metrics_prefix='discovery.seeds.preload.manual'
        )
        self.positive_seed_terms = self._load_seed_source(
            positive_seed_source,
            fallback=enrichment_cfg.get('positive_terms'),
            label='Ê≠£‰æãÁßçÂ≠ê',
            metrics_prefix='discovery.seeds.preload.positive'
        )

        # AIÁõ∏ÂÖ≥subredditÂàóË°®
        self.ai_subreddits = [
            'artificial', 'MachineLearning', 'deeplearning', 'ChatGPT',
            'OpenAI', 'artificial_intelligence', 'singularity', 'Futurology',
            'programming', 'webdev', 'entrepreneur', 'SaaS', 'startups'
        ]

        # ÂÖ≥ÈîÆËØçÊèêÂèñÊ®°ÂºèÔºàÁî®‰∫éËÆ∫Âùõ/Êñ∞ÈóªÂéüÂßãÊñáÊú¨Ôºâ
        self.keyword_patterns = [
            r'\b(?:ai|artificial intelligence|machine learning|deep learning|neural network)\b',
            r'\b(?:tool|software|app|platform|service|solution)\b'
        ]

        self.generic_head_terms = {
            'service', 'services', 'software', 'platform', 'platforms', 'solution',
            'solutions', 'application', 'applications', 'tool', 'tools',
            'machine learning', 'artificial intelligence', 'automation', 'ai',
            'technology', 'technologies', 'gpt'
        }
        self.generic_lead_tokens = {'ai', 'machine', 'software', 'platform', 'service', 'tool', 'technology', 'data'}
        self.generic_tail_tokens = {
            'tool', 'tools', 'software', 'platform', 'platforms', 'service',
            'services', 'application', 'applications', 'app', 'apps', 'solution',
            'solutions', 'system', 'systems', 'suite'
        }
        self.long_tail_tokens = {
            'workflow', 'workflows', 'strategy', 'strategies', 'ideas', 'guide',
            'guides', 'tutorial', 'tutorials', 'template', 'templates', 'checklist',
            'automation', 'process', 'processes', 'plan', 'plans', 'blueprint',
            'blueprints', 'examples', 'case', 'cases', 'study', 'studies', 'use',
            'uses', 'stack', 'stacks', 'integration', 'integrations', 'niche',
            'niches', 'system', 'systems', 'playbook', 'playbooks', 'framework',
            'frameworks', 'marketing', 'seo', 'content', 'workflow', 'roadmap',
            'roadmaps', 'setup', 'automation', 'builders', 'for', 'beginners',
            'advanced', 'agency', 'agencies', 'students', 'writers', 'designers',
            'developers', 'founders', 'startups', 'teams', 'checklists'
        }
        self.question_prefixes = (
            'how to', 'how do', 'how can', 'what is', 'what are', 'why', 'should i',
            'can i', 'is there', 'best way', 'ways to'
        )
        self.max_brand_variations = max(int(filters_cfg.get('max_brand_variations', 8) or 0), 0)
        self.lsh_similarity_threshold = float(filters_cfg.get('lsh_similarity_threshold', 0.9))
        self.cluster_distance_threshold = float(filters_cfg.get('cluster_distance_threshold', 0.2))
        self.lsh_similarity_threshold = min(max(self.lsh_similarity_threshold, 0.5), 0.98)
        self.cluster_distance_threshold = min(max(self.cluster_distance_threshold, 0.05), 0.5)
        self._cleaning_config = CleaningConfig()
        self._brand_filter_notice_shown = False
        self._generic_filter_notice_shown = False

    def _load_discovery_filter_config(self) -> Dict[str, Any]:
        config_path = os.path.join(os.path.dirname(__file__), '../../../config/integrated_workflow_config.json')
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as fh:
                    data = json.load(fh)
                filters_cfg = data.get('discovery_filters', {})
                return filters_cfg if isinstance(filters_cfg, dict) else {}
        except Exception:
            pass
        return {}

    def _load_discovery_runtime_config(self) -> Dict[str, Any]:
        config_path = os.path.join(os.path.dirname(__file__), '../../../config/integrated_workflow_config.json')
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as fh:
                    data = json.load(fh)
                runtime_cfg = data.get('discovery_runtime', {})
                return runtime_cfg if isinstance(runtime_cfg, dict) else {}
        except Exception:
            pass
        return {}

    def _load_discovery_seed_config(self) -> Dict[str, Any]:
        config_path = os.path.join(os.path.dirname(__file__), '../../../config/integrated_workflow_config.json')
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as fh:
                    data = json.load(fh)
                seed_cfg = data.get('discovery_seeds', {})
                return seed_cfg if isinstance(seed_cfg, dict) else {}
        except Exception:
            pass
        return {}

    def _load_seed_source(
        self,
        source: Any,
        *,
        fallback: Optional[Any] = None,
        label: str = 'Â§ñÈÉ®ÁßçÂ≠ê',
        metrics_prefix: Optional[str] = None
    ) -> List[str]:
        resolved_terms: List[str] = []

        def _collect(payload: Any) -> None:
            if payload is None:
                return
            if isinstance(payload, str):
                cleaned = payload.strip()
                if cleaned:
                    resolved_terms.append(cleaned)
            elif isinstance(payload, (list, tuple, set)):
                for item in payload:
                    _collect(item)
            elif isinstance(payload, dict):
                for item in payload.values():
                    _collect(item)

        path_candidate: Optional[Path] = None
        if isinstance(source, str):
            path_candidate = Path(source)
            if not path_candidate.is_absolute():
                project_root = Path(__file__).resolve().parents[3]
                path_candidate = project_root / source
            if path_candidate.exists():
                try:
                    with path_candidate.open('r', encoding='utf-8') as fh:
                        data = json.load(fh)
                    _collect(data)
                except Exception as exc:
                    print(f"‚ö†Ô∏è {label}Êñá‰ª∂Ëß£ÊûêÂ§±Ë¥•: {exc}")
            else:
                print(f"‚ö†Ô∏è {label}Êñá‰ª∂Êú™ÊâæÂà∞: {path_candidate}")
        else:
            _collect(source)

        if not resolved_terms and fallback is not None:
            _collect(fallback)

        deduped: List[str] = []
        seen: set = set()
        for term in resolved_terms:
            lowered = term.lower()
            if lowered not in seen:
                deduped.append(term)
                seen.add(lowered)

        filtered_terms, removed = self._filter_brand_terms(
            deduped,
            f'{label}Èò∂ÊÆµ',
            log=False,
            metrics_prefix=metrics_prefix
        )
        if filtered_terms and metrics_prefix:
            telemetry_manager.set_gauge(f'{metrics_prefix}.retained', len(filtered_terms))
        if removed:
            telemetry_manager.increment_counter('discovery.seeds.preload.brand_filtered', len(removed))
        return filtered_terms

    def _filter_brand_terms(
        self,
        terms: List[str],
        context: str,
        *,
        log: bool = True,
        metrics_prefix: Optional[str] = None
    ) -> Tuple[List[str], List[str]]:
        """Remove brand-heavy seed terms and optionally log the removal."""
        sanitized: List[str] = []
        filtered_out: List[str] = []

        if not terms:
            if metrics_prefix:
                telemetry_manager.set_gauge(f'{metrics_prefix}.total', 0)
                telemetry_manager.set_gauge(f'{metrics_prefix}.removed', 0)
                telemetry_manager.set_gauge(f'{metrics_prefix}.removed_ratio', 0.0)
            return sanitized, filtered_out

        # Normalize input values first
        for term in terms:
            if isinstance(term, str):
                normalized = term.strip()
                if normalized:
                    sanitized.append(normalized)

        total_candidates = len(sanitized)

        if not sanitized:
            if metrics_prefix:
                telemetry_manager.set_gauge(f'{metrics_prefix}.total', 0)
                telemetry_manager.set_gauge(f'{metrics_prefix}.removed', 0)
                telemetry_manager.set_gauge(f'{metrics_prefix}.removed_ratio', 0.0)
            return sanitized, filtered_out

        if not self.brand_filter_config.get('enabled', True) or not self.brand_filter_config.get('hard_exclude_brand_seeds', True):
            if metrics_prefix:
                telemetry_manager.set_gauge(f'{metrics_prefix}.total', total_candidates)
                telemetry_manager.set_gauge(f'{metrics_prefix}.removed', 0)
                telemetry_manager.set_gauge(f'{metrics_prefix}.removed_ratio', 0.0)
            return sanitized, filtered_out

        retained: List[str] = []
        for term in sanitized:
            if self._is_brand_heavy(term):
                filtered_out.append(term)
            else:
                retained.append(term)

        removed_count = len(filtered_out)

        if filtered_out and log:
            preview = ', '.join(filtered_out[:5])
            if len(filtered_out) > 5:
                preview += '...'
            print(f"üö´ {context}Êã¶Êà™ {len(filtered_out)} ‰∏™ÂìÅÁâåÁßçÂ≠ê: {preview}")

        if metrics_prefix:
            telemetry_manager.set_gauge(f'{metrics_prefix}.total', total_candidates)
            telemetry_manager.set_gauge(f'{metrics_prefix}.removed', removed_count)
            ratio = removed_count / total_candidates if total_candidates else 0.0
            telemetry_manager.set_gauge(f'{metrics_prefix}.removed_ratio', ratio)

        return retained, filtered_out

    def get_seed_profiles(self) -> List[str]:
        """ËøîÂõûÂèØÁî®ÁöÑÁßçÂ≠êÂÖ≥ÈîÆËØçÈÖçÁΩÆÂêçÁß∞ÂàóË°®"""
        return list(self.seed_profiles.keys())

    def get_seed_terms(self,
                        profile: Optional[str] = None,
                        limit: Optional[int] = None,
                        *,
                        apply_brand_filter: bool = True,
                        log_context: Optional[str] = None) -> List[str]:
        """Ê†πÊçÆÈÖçÁΩÆËé∑ÂèñÁßçÂ≠êÂÖ≥ÈîÆËØçÂàóË°®"""
        profile_name = profile or self.default_seed_profile
        terms: List[str] = []

        def _extract_terms(profile_key: Optional[str]) -> List[str]:
            if not profile_key:
                return []
            profile_cfg = self.seed_profiles.get(profile_key)
            if profile_cfg is None:
                return []
            if isinstance(profile_cfg, dict):
                source = profile_cfg.get('terms') or profile_cfg.get('keywords') or []
            elif isinstance(profile_cfg, (list, tuple, set)):
                source = profile_cfg
            else:
                source = []
            cleaned = []
            for term in source:
                if isinstance(term, str):
                    normalized = term.strip()
                    if normalized:
                        cleaned.append(normalized)
            return cleaned

        terms = _extract_terms(profile_name)
        if not terms and profile_name != self.default_seed_profile:
            terms = _extract_terms(self.default_seed_profile)
        if not terms and self.seed_profiles:
            # FallbackÂà∞Á¨¨‰∏Ä‰∏™ÂèØÁî®ÈÖçÁΩÆ
            fallback_profile = next(iter(self.seed_profiles.keys()))
            terms = _extract_terms(fallback_profile)

        # ÂéªÈáçÂêåÊó∂‰øùÁïôÈ°∫Â∫è
        unique_terms = []
        seen = set()
        for term in terms:
            lower_term = term.lower()
            if lower_term not in seen:
                unique_terms.append(term)
                seen.add(lower_term)

        if limit and limit > 0:
            unique_terms = unique_terms[:limit]

        if apply_brand_filter:
            filtered_terms, removed = self._filter_brand_terms(
                unique_terms,
                log_context or 'ÈÖçÁΩÆÁßçÂ≠êÈò∂ÊÆµ',
                log=bool(log_context),
                metrics_prefix='discovery.seeds.profile'
            )
            unique_terms = filtered_terms
            if log_context and removed:
                telemetry_manager.increment_counter('discovery.seeds.brand_filtered', len(removed))

        return unique_terms

    def prepare_search_terms(self,
                              seeds: Optional[List[str]] = None,
                              profile: Optional[str] = None,
                              limit: Optional[int] = None,
                              min_terms: Optional[int] = None,
                              *,
                              log_input_filter: bool = True) -> List[str]:
        """ÁªÑÂêàËæìÂÖ•ÁöÑÁßçÂ≠êÂÖ≥ÈîÆËØç‰∏éÈÖçÁΩÆ‰∏≠ÁöÑÈªòËÆ§ÁßçÂ≠ê"""
        cleaned: List[str] = []
        seen: set = set()

        def _append(term: str) -> bool:
            lower_term = term.lower()
            if lower_term not in seen:
                cleaned.append(term)
                seen.add(lower_term)
                return True
            return False

        def _prepend(term: str) -> bool:
            lower_term = term.lower()
            if lower_term not in seen:
                cleaned.insert(0, term)
                seen.add(lower_term)
                return True
            return False

        def _add_manual_seeds(reason_key: str, count: Optional[int] = None, message: Optional[str] = None) -> int:
            added = 0
            if not self.manual_seed_terms:
                return 0
            for term in self.manual_seed_terms:
                if count is not None and added >= count:
                    break
                if _append(term):
                    added += 1
            if added:
                telemetry_manager.increment_counter('discovery.seeds.manual_added', added)
                telemetry_manager.increment_counter(f'discovery.seeds.manual_added.{reason_key}', added)
                if message:
                    print(message.format(count=added))
            return added

        brand_checked_total = 0
        brand_removed_total = 0

        input_terms = seeds or []
        filtered_seeds, removed = self._filter_brand_terms(
            list(input_terms),
            'ËæìÂÖ•ÁßçÂ≠êÈò∂ÊÆµ',
            log=log_input_filter,
            metrics_prefix='discovery.seeds.input'
        )
        brand_checked_total += len(filtered_seeds) + len(removed)
        brand_removed_total += len(removed)
        for term in filtered_seeds:
            _append(term)

        target_min = max(min_terms or self.min_seed_terms, 1)
        if len(cleaned) < target_min:
            fallback_terms_raw = self.get_seed_terms(
                profile=profile,
                limit=None,
                apply_brand_filter=False
            )
            fallback_terms, removed = self._filter_brand_terms(
                fallback_terms_raw,
                'ÈÖçÁΩÆÁßçÂ≠êÈò∂ÊÆµ',
                log=True,
                metrics_prefix='discovery.seeds.profile'
            )
            brand_checked_total += len(fallback_terms) + len(removed)
            brand_removed_total += len(removed)
            for term in fallback_terms:
                if _append(term) and limit and len(cleaned) >= limit:
                    break

        if len(cleaned) < target_min:
            additional_terms_raw = self.get_seed_terms(
                profile=self.default_seed_profile,
                limit=None,
                apply_brand_filter=False
            )
            additional_terms, removed = self._filter_brand_terms(
                additional_terms_raw,
                'ÈªòËÆ§ÁßçÂ≠êÈò∂ÊÆµ',
                log=True,
                metrics_prefix='discovery.seeds.profile.default'
            )
            brand_checked_total += len(additional_terms) + len(removed)
            brand_removed_total += len(removed)
            for term in additional_terms:
                if _append(term) and limit and len(cleaned) >= limit:
                    break

        if self.positive_seed_terms and self.priority_seed_limit != 0:
            prioritized: List[str] = []
            for term in self.positive_seed_terms:
                lower = term.lower()
                if lower in seen:
                    continue
                prioritized.append(term)
                if 0 < self.priority_seed_limit <= len(prioritized):
                    break
            if prioritized:
                for term in reversed(prioritized):
                    _prepend(term)
                telemetry_manager.increment_counter('discovery.seeds.positive_prioritized', len(prioritized))

        if len(cleaned) < target_min and self.manual_seed_terms:
            shortage_needed = max(target_min - len(cleaned), self.min_manual_seed_on_shortage)
            if shortage_needed > 0:
                _add_manual_seeds('shortage', shortage_needed, message='‚ÑπÔ∏è ÁßçÂ≠êÊï∞Èáè‰∏çË∂≥ÔºåËøΩÂä† {count} ‰∏™ÁóõÁÇπÊâãÂä®ÁßçÂ≠ê')

        brand_ratio = 0.0
        if brand_checked_total > 0:
            brand_ratio = brand_removed_total / brand_checked_total
        telemetry_manager.set_gauge('discovery.seeds.brand_filtered_ratio', brand_ratio)

        if brand_ratio > self.brand_ratio_limit and self.manual_seed_terms:
            added = _add_manual_seeds(
                'brand_ratio',
                self.manual_seed_backfill or None,
                message=f"‚ÑπÔ∏è ÂìÅÁâåËøáÊª§ÂëΩ‰∏≠Áéá {brand_ratio:.2%} Ë∂ÖÂá∫ÈòàÂÄº {self.brand_ratio_limit:.0%}ÔºåËøΩÂä† {{count}} ‰∏™ÁóõÁÇπÊâãÂä®ÁßçÂ≠ê"
            )

        if limit and limit > 0 and len(cleaned) > limit:
            cleaned = cleaned[:limit]

        telemetry_manager.set_gauge('discovery.seeds.final_count', len(cleaned))
        return cleaned

    def _build_rate_limit_map(self, override: Optional[Any]) -> Dict[str, float]:
        """ÂêàÂπ∂ÈªòËÆ§‰∏éÈÖçÁΩÆÁöÑÈÄüÁéáÈôêÂà∂"""
        defaults = {
            'www.reddit.com': 1.1,
            'reddit.com': 1.1,
            'hn.algolia.com': 0.6,
            'news.ycombinator.com': 0.6,
            'suggestqueries.google.com': 0.25,
            'api.producthunt.com': 1.0,
            'www.producthunt.com': 1.0,
        }
        defaults['*'] = max(self.default_rate_interval, 0.1)

        if override:
            payload = override
            if isinstance(payload, str):
                try:
                    payload = json.loads(payload)
                except Exception:
                    payload = None
            if isinstance(payload, dict):
                for host, value in payload.items():
                    try:
                        interval = float(value)
                    except (TypeError, ValueError):
                        continue
                    normalized_host = str(host).lower().strip()
                    if normalized_host:
                        defaults[normalized_host] = max(interval, 0.05)
        return defaults

    def _build_headers(self, extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:
        headers = {'User-Agent': self.user_agent}
        if extra:
            for key, value in extra.items():
                if value:
                    headers[key] = value
        return headers

    def _normalize_cache_key(self, raw_key: str) -> str:
        normalized = re.sub(r'[^a-zA-Z0-9_-]+', '_', raw_key.lower()).strip('_')
        hash_suffix = hashlib.sha1(raw_key.encode('utf-8')).hexdigest()[:10]
        if normalized:
            normalized = normalized[:60]
            return f"{normalized}_{hash_suffix}"
        return hash_suffix

    def _reconfigure_cache_backend(self) -> None:
        self._cache_backend = _TTLCache(
            self.cache_dir,
            self._cache_ttl,
            self._cache_enabled,
            self._cache_store,
            self._normalize_cache_key,
        )

    def _ensure_cache_backend_current(self) -> None:
        target_dir = Path(self.cache_dir)
        target_ttl = max(int(self._cache_ttl), 0)
        target_enabled = bool(self._cache_enabled) and target_ttl != 0
        backend = getattr(self, '_cache_backend', None)
        if backend is None or backend.cache_dir != target_dir or backend.ttl_seconds != target_ttl or backend.enabled != target_enabled:
            self._cache_ttl = target_ttl
            self._cache_enabled = target_enabled
            self._reconfigure_cache_backend()
        else:
            backend.ttl_seconds = target_ttl
            backend.enabled = target_enabled

    def _load_cached_payload(self, namespace: str, key: str):
        self._ensure_cache_backend_current()
        return self._cache_backend.get(namespace, key)

    def _save_cached_payload(self, namespace: str, key: str, payload: Any) -> None:
        self._ensure_cache_backend_current()
        self._cache_backend.set(namespace, key, payload)

    def _run_async(self, coroutine):
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            return asyncio.run(coroutine)
        if loop.is_running():
            raise RuntimeError('discover_all_platforms ÈúÄÂú®ÂêåÊ≠•‰∏ä‰∏ãÊñáË∞ÉÁî®ÔºåÂèØÁõ¥Êé•‰ΩøÁî® await ÁâàÊú¨')
        return loop.run_until_complete(coroutine)

    async def _run_single_platform(
        self,
        builder: Callable[[aiohttp.ClientSession, asyncio.Semaphore], Awaitable[List[Dict[str, Any]]]]
    ) -> List[Dict[str, Any]]:
        semaphore = asyncio.Semaphore(self.max_concurrency)
        async with aiohttp.ClientSession(headers=self._build_headers()) as session:
            return await builder(session, semaphore)

    async def _fetch_json(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        method: str,
        url: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        json_payload: Optional[Any] = None
    ) -> Any:
        attempt = 0
        host = urlparse(url).netloc
        request_headers = self._build_headers(headers)
        while True:
            attempt += 1
            recorded_failure = False
            start_ts = None
            try:
                await self._rate_limiter.throttle(host)
                async with semaphore:
                    start_ts = time.perf_counter()
                    async with session.request(
                        method.upper(),
                        url,
                        params=params,
                        headers=request_headers,
                        json=json_payload,
                        timeout=self.request_timeout
                    ) as response:
                        elapsed = time.perf_counter() - start_ts
                        status = response.status
                        if status >= 400:
                            recorded_failure = True
                            telemetry_manager.record_request(
                                host=host,
                                method=method.upper(),
                                url=url,
                                status=status,
                                ok=False,
                                elapsed=elapsed,
                            )
                            response.raise_for_status()

                        data = await response.json()
                        telemetry_manager.record_request(
                            host=host,
                            method=method.upper(),
                            url=url,
                            status=status,
                            ok=True,
                            elapsed=elapsed,
                        )
                        return data
            except (
                aiohttp.ClientError,
                asyncio.TimeoutError,
                aiohttp.ContentTypeError,
                json.JSONDecodeError
            ) as exc:
                status = getattr(exc, 'status', None)
                elapsed = time.perf_counter() - start_ts if start_ts is not None else None
                if not recorded_failure:
                    telemetry_manager.record_request(
                        host=host,
                        method=method.upper(),
                        url=url,
                        status=status,
                        ok=False,
                        elapsed=elapsed,
                    )
                if attempt >= self.max_retries:
                    raise exc
                await asyncio.sleep(random.uniform(*self.retry_backoff))

    async def _fetch_text(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        method: str,
        url: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None
    ) -> str:
        attempt = 0
        host = urlparse(url).netloc
        request_headers = self._build_headers(headers)
        while True:
            attempt += 1
            recorded_failure = False
            start_ts = None
            try:
                await self._rate_limiter.throttle(host)
                async with semaphore:
                    start_ts = time.perf_counter()
                    async with session.request(
                        method.upper(),
                        url,
                        params=params,
                        headers=request_headers,
                        timeout=self.request_timeout
                    ) as response:
                        elapsed = time.perf_counter() - start_ts
                        status = response.status
                        if status >= 400:
                            recorded_failure = True
                            telemetry_manager.record_request(
                                host=host,
                                method=method.upper(),
                                url=url,
                                status=status,
                                ok=False,
                                elapsed=elapsed,
                            )
                            response.raise_for_status()

                        content = await response.text()
                        telemetry_manager.record_request(
                            host=host,
                            method=method.upper(),
                            url=url,
                            status=status,
                            ok=True,
                            elapsed=elapsed,
                        )
                        return content
            except (aiohttp.ClientError, asyncio.TimeoutError) as exc:
                status = getattr(exc, 'status', None)
                elapsed = time.perf_counter() - start_ts if start_ts is not None else None
                if not recorded_failure:
                    telemetry_manager.record_request(
                        host=host,
                        method=method.upper(),
                        url=url,
                        status=status,
                        ok=False,
                        elapsed=elapsed,
                    )
                if attempt >= self.max_retries:
                    raise exc
                await asyncio.sleep(random.uniform(*self.retry_backoff))

    def _get_embedding_model(self):
        cached = getattr(self, '_embedding_model', None)
        if cached is not None:
            return cached

        model_name = self.embedding_model_name
        shared = _EMBEDDING_MODEL_CACHE.get(model_name)
        if shared is not None:
            self._embedding_model = shared
            return shared

        with _EMBEDDING_MODEL_LOCK:
            shared = _EMBEDDING_MODEL_CACHE.get(model_name)
            if shared is None:
                from sentence_transformers import SentenceTransformer

                shared = SentenceTransformer(model_name)
                _EMBEDDING_MODEL_CACHE[model_name] = shared

        self._embedding_model = shared
        return shared

    async def _discover_reddit_keywords_async(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        subreddit: str,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        cache_key = f"{subreddit.lower()}_{int(limit)}"
        cached = self._load_cached_payload('reddit', cache_key)
        if cached is not None:
            print(f"‚ö° Reddit r/{subreddit} ‰ΩøÁî®ÁºìÂ≠òÁªìÊûú")
            return cached

        print(f"üîç Ê≠£Âú®ÂàÜÊûê Reddit r/{subreddit}...")

        if self.reddit_use_rss or self.reddit_client is None:
            feed_url = f"https://www.reddit.com/r/{subreddit}/.rss"
            try:
                feed_text = await self._fetch_text(session, semaphore, 'GET', feed_url)
            except Exception as exc:
                print(f"‚ùå Reddit r/{subreddit} RSS Ëé∑ÂèñÂ§±Ë¥•: {exc}")
                return []

            items: List[Dict[str, Any]] = []
            try:
                for entry in parse_rss(feed_text):
                    if not entry.get('title'):
                        continue
                    items.append({
                        'keyword': entry['title'],
                        'source': f'reddit_r_{subreddit}',
                        'title': entry['title'],
                        'score': 0,
                        'comments': 0,
                        'url': entry.get('link') or entry.get('guid') or '',
                        'platform': 'reddit',
                        'metadata': {
                            'published_at': entry.get('published'),
                            'author': entry.get('author'),
                        }
                    })
            except Exception as exc:
                print(f"‚ùå Reddit r/{subreddit} RSS Ëß£ÊûêÂ§±Ë¥•: {exc}")
                return []

            self._save_cached_payload('reddit', cache_key, items)
            return items

        params = {'limit': max(int(limit), 1)}
        base_public_url = f"https://www.reddit.com/r/{subreddit}/hot.json"
        base_oauth_url = f"https://oauth.reddit.com/r/{subreddit}/hot.json"
        data = None
        for attempt in range(2):
            headers: Optional[Dict[str, str]] = None
            current_url = base_public_url
            if self.reddit_client:
                force_refresh = attempt == 1
                token = await self.reddit_client.get_token(session, force_refresh=force_refresh)
                if token:
                    current_url = base_oauth_url
                    headers = {
                        'Authorization': f"Bearer {token}",
                        'User-Agent': self.reddit_client.credentials.user_agent,
                    }
            try:
                data = await self._fetch_json(
                    session,
                    semaphore,
                    'GET',
                    current_url,
                    params=params,
                    headers=headers,
                )
                break
            except aiohttp.ClientResponseError as exc:
                if self.reddit_client and exc.status in (401, 403) and attempt == 0:
                    await self.reddit_client.invalidate_token()
                    continue
                print(f"‚ùå Reddit r/{subreddit} ÂàÜÊûêÂ§±Ë¥•: {exc}")
                return []
            except Exception as exc:
                print(f"‚ùå Reddit r/{subreddit} ÂàÜÊûêÂ§±Ë¥•: {exc}")
                return []

        if data is None:
            return []

        posts = data.get('data', {}).get('children', [])
        keywords: List[Dict[str, Any]] = []
        for post in posts:
            post_data = post.get('data', {})
            title = post_data.get('title', '')
            selftext = post_data.get('selftext', '')
            score = post_data.get('score', 0)
            num_comments = post_data.get('num_comments', 0)

            extracted = self._extract_keywords_from_text(f"{title} {selftext}")
            for keyword in extracted:
                keywords.append({
                    'keyword': keyword,
                    'source': f'reddit_r_{subreddit}',
                    'title': title,
                    'score': score,
                    'comments': num_comments,
                    'url': f"https://reddit.com{post_data.get('permalink', '')}",
                    'platform': 'reddit'
                })

        if keywords:
            self._save_cached_payload('reddit', cache_key, keywords)
        return keywords

    def discover_reddit_keywords(self, subreddit: str, limit: int = 100) -> List[Dict[str, Any]]:
        return self._run_async(
            self._run_single_platform(
                lambda session, semaphore: self._discover_reddit_keywords_async(
                    session, semaphore, subreddit, limit
                )
            )
        )

    async def _discover_hackernews_keywords_async(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        query: str = 'AI',
        days: int = 30
    ) -> List[Dict[str, Any]]:
        cache_key = f"{query.lower()}_{int(days)}"
        cached = self._load_cached_payload('hackernews', cache_key)
        if cached is not None:
            print(f"‚ö° Hacker News (Êü•ËØ¢: {query}) ‰ΩøÁî®ÁºìÂ≠òÁªìÊûú")
            return cached

        print(f"üîç Ê≠£Âú®ÂàÜÊûê Hacker News (Êü•ËØ¢: {query})...")
        end_time = int(time.time())
        start_time = end_time - (int(days) * 24 * 3600)
        url = "https://hn.algolia.com/api/v1/search"
        params = {
            'query': query,
            'tags': 'story',
            'numericFilters': f'created_at_i>{start_time},created_at_i<{end_time}',
            'hitsPerPage': 100
        }
        try:
            data = await self._fetch_json(session, semaphore, 'GET', url, params=params)
        except Exception as exc:
            print(f"‚ùå Hacker News ÂàÜÊûêÂ§±Ë¥•: {exc}")
            return []

        hits = data.get('hits', [])
        keywords: List[Dict[str, Any]] = []
        for hit in hits:
            title = hit.get('title', '')
            post_url = hit.get('url', '')
            points = hit.get('points', 0)
            num_comments = hit.get('num_comments', 0)
            extracted = self._extract_keywords_from_text(title)
            for keyword in extracted:
                keywords.append({
                    'keyword': keyword,
                    'source': 'hackernews',
                    'title': title,
                    'score': points,
                    'comments': num_comments,
                    'url': post_url or f"https://news.ycombinator.com/item?id={hit.get('objectID')}",
                    'platform': 'hackernews'
                })

        if keywords:
            self._save_cached_payload('hackernews', cache_key, keywords)
        return keywords

    def discover_hackernews_keywords(self, query: str = 'AI', days: int = 30) -> List[Dict[str, Any]]:
        return self._run_async(
            self._run_single_platform(
                lambda session, semaphore: self._discover_hackernews_keywords_async(
                    session, semaphore, query, days
                )
            )
        )

    async def _discover_youtube_keywords_async(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        search_term: str
    ) -> List[Dict[str, Any]]:
        cache_key = search_term.lower()
        cached = self._load_cached_payload('youtube', cache_key)
        if cached is not None:
            print(f"‚ö° YouTube ÊêúÁ¥¢Âª∫ËÆÆ (Êü•ËØ¢: {search_term}) ‰ΩøÁî®ÁºìÂ≠òÁªìÊûú")
            return cached

        print(f"üîç Ê≠£Âú®ÂàÜÊûê YouTube ÊêúÁ¥¢Âª∫ËÆÆ (Êü•ËØ¢: {search_term})...")
        url = "https://suggestqueries.google.com/complete/search"
        params = {'client': 'youtube', 'ds': 'yt', 'q': search_term}
        try:
            content = await self._fetch_text(session, semaphore, 'GET', url, params=params)
        except Exception as exc:
            print(f"‚ùå YouTube ÂàÜÊûêÂ§±Ë¥•: {exc}")
            return []

        keywords: List[Dict[str, Any]] = []
        if content.startswith('window.google.ac.h('):
            json_str = content[19:-1]
            try:
                data = json.loads(json_str)
                suggestions = data[1] if len(data) > 1 else []
                for suggestion in suggestions:
                    if isinstance(suggestion, list) and suggestion:
                        keyword = suggestion[0]
                        keywords.append({
                            'keyword': keyword,
                            'source': 'youtube_suggestions',
                            'title': f'YouTubeÊêúÁ¥¢Âª∫ËÆÆ: {keyword}',
                            'score': 0,
                            'comments': 0,
                            'url': f'https://www.youtube.com/results?search_query={quote(keyword)}',
                            'platform': 'youtube'
                        })
            except Exception as exc:
                print(f"‚ö†Ô∏è YouTube Âª∫ËÆÆËß£ÊûêÂ§±Ë¥•: {exc}")

        if keywords:
            self._save_cached_payload('youtube', cache_key, keywords)
        return keywords

    def discover_youtube_keywords(self, search_term: str) -> List[Dict[str, Any]]:
        return self._run_async(
            self._run_single_platform(
                lambda session, semaphore: self._discover_youtube_keywords_async(
                    session, semaphore, search_term
                )
            )
        )

    async def _discover_google_suggestions_async(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        search_term: str
    ) -> List[Dict[str, Any]]:
        cache_key = search_term.lower()
        cached = self._load_cached_payload('google', cache_key)
        if cached is not None:
            print(f"‚ö° Google ÊêúÁ¥¢Âª∫ËÆÆ (Êü•ËØ¢: {search_term}) ‰ΩøÁî®ÁºìÂ≠òÁªìÊûú")
            return cached

        print(f"üîç Ê≠£Âú®ÂàÜÊûê Google ÊêúÁ¥¢Âª∫ËÆÆ (Êü•ËØ¢: {search_term})...")
        url = "https://suggestqueries.google.com/complete/search"
        params = {'client': 'firefox', 'q': search_term}
        try:
            raw_text = await self._fetch_text(session, semaphore, 'GET', url, params=params)
        except Exception as exc:
            print(f"‚ùå GoogleÊêúÁ¥¢Âª∫ËÆÆÂàÜÊûêÂ§±Ë¥•: {exc}")
            return []

        payload_text = raw_text.strip()
        if payload_text.startswith(")]}'"):
            payload_text = payload_text[4:]

        try:
            data = json.loads(payload_text)
        except Exception as exc:
            print(f"‚ùå GoogleÊêúÁ¥¢Âª∫ËÆÆËß£ÊûêÂ§±Ë¥•: {exc}")
            return []

        suggestions = data[1] if isinstance(data, list) and len(data) > 1 else []
        keywords: List[Dict[str, Any]] = []
        for suggestion in suggestions:
            keywords.append({
                'keyword': suggestion,
                'source': 'google_suggestions',
                'title': f'GoogleÊêúÁ¥¢Âª∫ËÆÆ: {suggestion}',
                'score': 0,
                'comments': 0,
                'url': f'https://www.google.com/search?q={quote(suggestion)}',
                'platform': 'google'
            })

        if keywords:
            self._save_cached_payload('google', cache_key, keywords)
        return keywords

    def discover_google_suggestions(self, search_term: str) -> List[Dict[str, Any]]:
        return self._run_async(
            self._run_single_platform(
                lambda session, semaphore: self._discover_google_suggestions_async(
                    session, semaphore, search_term
                )
            )
        )

    async def _discover_producthunt_keywords_async(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        search_term: str = 'AI',
        days: int = 30
    ) -> List[Dict[str, Any]]:
        if not self.platforms.get('producthunt', {}).get('enabled'):
            return []

        cache_key = f"{search_term.lower()}_{int(days)}"
        cached = self._load_cached_payload('producthunt', cache_key)
        if cached is not None:
            print(f"‚ö° ProductHunt (Êü•ËØ¢: {search_term}) ‰ΩøÁî®ÁºìÂ≠òÁªìÊûú")
            return cached

        print(f"üîç Ê≠£Âú®ÂàÜÊûê ProductHunt (Êü•ËØ¢: {search_term})...")
        query = """
        query($search: String!, $first: Int!) {
          posts(search: $search, first: $first, order: VOTES) {
            edges {
              node {
                id
                name
                tagline
                description
                votesCount
                commentsCount
                url
                topics {
                  edges {
                    node {
                      name
                    }
                  }
                }
              }
            }
          }
        }
        """
        variables = {
            "search": search_term,
            "first": 50
        }
        try:
            data = await self._fetch_json(
                session,
                semaphore,
                'POST',
                self.platforms['producthunt']['base_url'],
                json_payload={'query': query, 'variables': variables},
                headers=self.ph_headers
            )
        except Exception as exc:
            print(f"‚ùå ProductHunt ÂàÜÊûêÂ§±Ë¥•: {exc}")
            return []

        posts = ((data or {}).get('data') or {}).get('posts', {}).get('edges', [])
        keywords: List[Dict[str, Any]] = []
        for edge in posts:
            node = edge.get('node', {})
            name = node.get('name', '')
            tagline = node.get('tagline', '')
            description = node.get('description', '')
            votes = node.get('votesCount', 0)
            comments = node.get('commentsCount', 0)
            url = node.get('url', '')
            topics = [t.get('node', {}).get('name', '') for t in node.get('topics', {}).get('edges', [])]
            combined_text = " ".join(filter(None, [name, tagline, description] + topics))
            extracted = self._extract_keywords_from_text(combined_text)
            for keyword in extracted:
                keywords.append({
                    'keyword': keyword,
                    'source': 'producthunt',
                    'title': name,
                    'score': votes,
                    'comments': comments,
                    'url': url,
                    'platform': 'producthunt'
                })

        if keywords:
            self._save_cached_payload('producthunt', cache_key, keywords)
        return keywords

    def discover_producthunt_keywords(self, search_term: str = 'AI', days: int = 30) -> List[Dict[str, Any]]:
        return self._run_async(
            self._run_single_platform(
                lambda session, semaphore: self._discover_producthunt_keywords_async(
                    session, semaphore, search_term, days
                )
            )
        )

    async def _async_discover_all_platforms(self, search_terms: List[str]) -> pd.DataFrame:
        telemetry_stage = telemetry_manager.start_stage(
            "discovery.multi_platform",
            metadata={
                "input_terms": len(search_terms),
                "platforms": [p for p, cfg in self.platforms.items() if cfg.get('enabled', True)],
            }
        )
        try:
            print("üöÄ ÂºÄÂßãÂ§öÂπ≥Âè∞ÂÖ≥ÈîÆËØçÂèëÁé∞...")
            sanitized_terms, _ = self._filter_brand_terms(
                list(search_terms or []),
                'ËæìÂÖ•ÁßçÂ≠êÈò∂ÊÆµ',
                log=True,
                metrics_prefix='discovery.seeds.runtime.input'
            )
            prepared_terms = self.prepare_search_terms(sanitized_terms, log_input_filter=False)
            prepared_terms, _ = self._filter_brand_terms(
                prepared_terms,
                'Â§öÂπ≥Âè∞ÊâßË°åÈò∂ÊÆµ',
                log=True,
                metrics_prefix='discovery.seeds.runtime.final'
            )

            if not prepared_terms:
                print("‚ö†Ô∏è Áº∫Â∞ëÊúâÊïàÁöÑÊêúÁ¥¢ËØçÔºåÊó†Ê≥ïÊâßË°åÂ§öÂπ≥Âè∞ÂèëÁé∞")
                telemetry_manager.end_stage(
                    telemetry_stage,
                    extra={'result_keywords': 0, 'prepared_terms': 0, 'tasks': 0}
                )
                return pd.DataFrame(columns=['keyword', 'platform'])

            semaphore = asyncio.Semaphore(self.max_concurrency)
            tasks: List[Awaitable[List[Dict[str, Any]]]] = []

            async with aiohttp.ClientSession(headers=self._build_headers()) as session:
                for subreddit in self.ai_subreddits[:5]:
                    tasks.append(self._discover_reddit_keywords_async(session, semaphore, subreddit, 50))

                for term in prepared_terms:
                    tasks.append(self._discover_hackernews_keywords_async(session, semaphore, term))
                    tasks.append(self._discover_youtube_keywords_async(session, semaphore, term))
                    tasks.append(self._discover_google_suggestions_async(session, semaphore, term))
                    if self.platforms.get('producthunt', {}).get('enabled'):
                        tasks.append(self._discover_producthunt_keywords_async(session, semaphore, term))

                results = await asyncio.gather(*tasks, return_exceptions=True)

            all_keywords: List[Dict[str, Any]] = []
            for result in results:
                if isinstance(result, Exception):
                    print(f"‚ùå Âπ≥Âè∞ÈááÈõÜ‰ªªÂä°ÂºÇÂ∏∏: {result}")
                    telemetry_manager.log_event(
                        'discovery.error',
                        'platform_task_failed',
                        {'error': str(result)},
                    )
                    continue
                if result:
                    all_keywords.extend(result)

            telemetry_manager.increment_counter('discovery.multi_platform_runs')
            telemetry_manager.set_gauge('discovery.multi_platform.last_result_count', len(all_keywords))
            telemetry_manager.end_stage(
                telemetry_stage,
                extra={
                    'result_keywords': len(all_keywords),
                    'prepared_terms': len(prepared_terms),
                    'tasks': len(tasks),
                }
            )

            return self._post_process_keywords(all_keywords)
        except Exception as exc:
            telemetry_manager.end_stage(telemetry_stage, status='failed', error=str(exc))
            raise

    def discover_all_platforms(self, search_terms: List[str]) -> pd.DataFrame:
        return self._run_async(self._async_discover_all_platforms(search_terms))

    def _post_process_keywords(self, records: List[Dict[str, Any]]) -> pd.DataFrame:
        df = pd.DataFrame(records)
        if 'keyword' not in df.columns:
            print("‚ö†Ô∏è ÁªìÊûúÁº∫Â∞ëÂÖ≥ÈîÆËØçÂ≠óÊÆµ")
            return df

        if df.empty:
            return df

        try:
            df['keyword'] = df['keyword'].astype(str)
            df['normalized_keyword'] = df['keyword'].apply(standardize_term)
            df = df[df['normalized_keyword'].apply(lambda term: is_valid_term(term, self._cleaning_config))]
            df = df.drop_duplicates(subset='normalized_keyword')
            df['keyword'] = df['normalized_keyword']
            df = df.drop(columns=['normalized_keyword'])
        except Exception as exc:
            print(f"‚ö†Ô∏è ÂÖ≥ÈîÆËØçÊ∏ÖÊ¥óÂ§±Ë¥•: {exc}")
            df['keyword'] = df['keyword'].astype(str)

        if df.empty:
            print("‚ö†Ô∏è Ê∏ÖÊ¥óÂêéÊó†ÊúâÊïàÂÖ≥ÈîÆËØç")
            return df

        if 'platform' not in df.columns:
            df['platform'] = 'unknown'
        else:
            df['platform'] = df['platform'].fillna('unknown')

        if self.brand_filter_config.get('enabled', True):
            before_brand = len(df)
            df = df[~df['keyword'].apply(self._is_brand_heavy)].reset_index(drop=True)
            if len(df) < before_brand:
                print(f"‚ö†Ô∏è ÁßªÈô§‰∫Ü {before_brand - len(df)} ‰∏™ÂìÅÁâåÊ≥õËØç")
        elif not self._brand_filter_notice_shown:
            print("‚ÑπÔ∏è ÂìÅÁâåËØçËøáÊª§Â∑≤Á¶ÅÁî®Ôºå‰øùÁïôÂìÅÁâåÁõ∏ÂÖ≥ÂÖ≥ÈîÆËØç")
            self._brand_filter_notice_shown = True

        if df.empty:
            return df

        if self.generic_filter_config.get('enabled', True):
            before_generic = len(df)
            df = df[~df['keyword'].apply(self._is_underspecified_keyword)].reset_index(drop=True)
            if len(df) < before_generic:
                print(f"‚ö†Ô∏è ÁßªÈô§‰∫Ü {before_generic - len(df)} ‰∏™Ê≥õÂåñÂ§¥ÈÉ®ËØç")
        elif not self._generic_filter_notice_shown:
            print("‚ÑπÔ∏è Ê≥õËØçËøáÊª§Â∑≤Á¶ÅÁî®ÔºåÂ∞Ü‰øùÁïôÊõ¥ÂπøÊ≥õÁöÑÁÉ≠Èó®ËØç")
            self._generic_filter_notice_shown = True

        df = self._limit_brand_keywords(df)
        if df.empty:
            print("‚ö†Ô∏è ËøáÊª§ÂêéÊó†ÊúâÊïàÂÖ≥ÈîÆËØç")
            return df

        try:
            from datasketch import MinHash, MinHashLSH
            from sklearn.cluster import AgglomerativeClustering
            import numpy as np

            texts = df['keyword'].tolist()

            def shingles(s: str, k: int = 3):
                s = s.replace(' ', '_')
                return {s[i:i + k] for i in range(max(len(s) - k + 1, 1))}

            mhashes = []
            for text_value in texts:
                mh = MinHash(num_perm=64)
                for sh in shingles(text_value):
                    mh.update(sh.encode('utf-8'))
                mhashes.append(mh)

            lsh = MinHashLSH(threshold=self.lsh_similarity_threshold, num_perm=64)
            for idx, mh in enumerate(mhashes):
                lsh.insert(str(idx), mh)

            keep_idx = []
            seen = set()
            for idx, mh in enumerate(mhashes):
                if idx in seen:
                    continue
                near = lsh.query(mh)
                group = sorted(int(x) for x in near)
                for member in group:
                    seen.add(member)
                keep_idx.append(group[0])

            df = df.iloc[keep_idx].reset_index(drop=True)

            embeddings = None
            if self.embedding_enabled and len(df) >= self.embedding_min_keywords:
                try:
                    model = self._get_embedding_model()
                    embeddings = model.encode(df['keyword'].tolist(), normalize_embeddings=True)
                except Exception as exc:
                    print(f"Embedding model load failed: {exc}")
                    self.embedding_enabled = False
            else:
                if self.embedding_enabled and len(df) < self.embedding_min_keywords:
                    print(f"Skipping embedding stage: {len(df)} keywords, threshold {self.embedding_min_keywords}")

            if embeddings is not None and len(df) >= self.embedding_min_keywords:
                try:
                    clustering = AgglomerativeClustering(
                        n_clusters=None,
                        distance_threshold=self.cluster_distance_threshold,
                        metric='cosine',
                        linkage='average'
                    )
                except TypeError:
                    clustering = AgglomerativeClustering(
                        n_clusters=None,
                        distance_threshold=self.cluster_distance_threshold,
                        affinity='cosine',
                        linkage='average'
                    )
                labels = clustering.fit_predict(embeddings)
                df['cluster_id'] = labels

                representatives = []
                for cluster_id in sorted(set(labels)):
                    idxs = np.where(labels == cluster_id)[0]
                    subset = embeddings[idxs]
                    center = subset.mean(axis=0, keepdims=True)
                    sims = (subset @ center.T).ravel()
                    representative = idxs[int(np.argmax(sims))]
                    representatives.append(representative)
                df = df.iloc[sorted(set(representatives))].reset_index(drop=True)
            else:
                if 'cluster_id' not in df.columns:
                    df['cluster_id'] = 0
        except Exception as exc:
            print(f"Keyword clustering failed: {exc}")
            if 'cluster_id' not in df.columns:
                df['cluster_id'] = 0
            pass

        if 'score' not in df.columns:
            df['score'] = 0
        df['long_tail_score'] = df['keyword'].apply(self._calculate_long_tail_score)
        df['weighted_score'] = df['score'] * df['long_tail_score']
        df = df.sort_values('weighted_score', ascending=False)
        df['discovered_at'] = datetime.now().isoformat()
        print(f"‚úÖ ÂèëÁé∞ {len(df)} ‰∏™ÂÖ≥ÈîÆËØç")

        return df

    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØçÔºàËÆ∫Âùõ/Êñ∞ÈóªÂêçËØçÁü≠ËØ≠ÊäΩÂèñ + Ê®°ÂºèËØçÔºâ"""
        keywords = []
        text = text.lower()
        try:
            import nltk
            from nltk import pos_tag, word_tokenize, RegexpParser
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt', quiet=True)
            try:
                nltk.data.find('taggers/averaged_perceptron_tagger')
            except LookupError:
                nltk.download('averaged_perceptron_tagger', quiet=True)
            tokens = word_tokenize(text)
            tagged = pos_tag(tokens)
            grammar = r"NP: {<JJ>*<NN|NNS|NNP|NNPS>+}"
            cp = RegexpParser(grammar)
            tree = cp.parse(tagged)
            nps = []
            for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):
                phrase = ' '.join(w for w, t in subtree.leaves())
                nps.append(phrase)
            for np in nps:
                np_clean = re.sub(r'[^a-z0-9\s-]', ' ', np).strip()
                if 3 <= len(np_clean) <= 40 and 1 <= len(np_clean.split()) <= 3:
                    keywords.append(np_clean)
        except Exception:
            pass
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                keyword = re.sub(r'[^a-z0-9\s-]', ' ', match).strip()
                if 3 <= len(keyword) <= 40 and 1 <= len(keyword.split()) <= 3:
                    keywords.append(keyword)
        base = [
            'ai tool', 'ai generator', 'ai writer', 'ai assistant', 'ai chatbot',
            'machine learning', 'deep learning', 'neural network', 'gpt',
            'artificial intelligence', 'automation', 'nlp', 'computer vision'
        ]
        for term in base:
            if term in text and 3 <= len(term) <= 40:
                keywords.append(term)
        return list(set(keywords))
    
    def _is_long_tail_keyword(self, keyword: str) -> bool:
        """
        Âà§Êñ≠ÊòØÂê¶‰∏∫ÈïøÂ∞æÂÖ≥ÈîÆËØç
        
        Args:
            keyword: ÂÖ≥ÈîÆËØç
            
        Returns:
            ÊòØÂê¶‰∏∫ÈïøÂ∞æÂÖ≥ÈîÆËØç
        """
        words = keyword.split()
        # ÈïøÂ∞æËØçÊ†áÂáÜÔºö3‰∏™‰ª•‰∏äËØçÊ±á‰∏îÊÄªÈïøÂ∫¶15‰∏™Â≠óÁ¨¶‰ª•‰∏ä
        return len(words) >= 3 and len(keyword) >= 15
    
    def _calculate_long_tail_score(self, keyword: str) -> float:
        """
        ËÆ°ÁÆóÈïøÂ∞æËØçËØÑÂàÜÂä†ÊùÉ
        
        Args:
            keyword: ÂÖ≥ÈîÆËØç
            
        Returns:
            ËØÑÂàÜÂÄçÊï∞
        """
        word_count = len(keyword.split())
        keyword_lower = keyword.lower()
        
        base_score = 1.0
        
        # Âü∫‰∫éËØçÊï∞ÁöÑËØÑÂàÜÂä†ÊùÉ
        if word_count >= 5:
            base_score *= 3.0  # 5+ËØçÊ±áËé∑ÂæóÊúÄÈ´òÂä†ÊùÉ
        elif word_count >= 4:
            base_score *= 2.5  # 4ËØçÊ±áËé∑ÂæóÈ´òÂä†ÊùÉ
        elif word_count >= 3:
            base_score *= 2.0  # 3ËØçÊ±áËé∑Âæó‰∏≠Á≠âÂä†ÊùÉ
        
        # Âü∫‰∫éÊÑèÂõæÊòéÁ°ÆÊÄßÁöÑÂä†ÊùÉ
        high_intent_phrases = ['how to', 'step by step', 'tutorial', 'guide', 'without', 'for beginners']
        if any(phrase in keyword_lower for phrase in high_intent_phrases):
            base_score *= 1.5
        
        # Âü∫‰∫éÁ´û‰∫âÂ∫¶ÁöÑË∞ÉÊï¥
        high_competition_words = ['best', 'top', 'review', 'vs', 'comparison']
        if any(comp in keyword_lower for comp in high_competition_words):
            base_score *= 0.6  # È´òÁ´û‰∫âËØçÈôç‰ΩéËØÑÂàÜ

        return base_score

    def _is_brand_heavy(self, keyword: str) -> bool:
        """ËøáÊª§ÂìÅÁâåËØçÂèäÂÖ∂Â∏∏ËßÅÈôÑÂ±ûËØç"""
        if not keyword or not self.brand_filter_config.get('enabled', True):
            return False

        keyword_lower = keyword.lower()
        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        if not tokens:
            return False

        brand_present = any(phrase in keyword_lower for phrase in self.brand_phrases)
        if not brand_present:
            brand_present = any(token in self.brand_tokens for token in tokens)
        if not brand_present:
            return False

        non_brand_tokens = [t for t in tokens if t not in self.brand_tokens]
        if not non_brand_tokens:
            return True

        min_non_brand = max(int(self.brand_filter_config.get('min_non_brand_tokens', 1) or 0), 0)
        if min_non_brand and len(non_brand_tokens) < min_non_brand:
            return True

        if self.brand_filter_config.get('strict_brand_modifiers', False):
            if all(token in self.brand_modifier_tokens for token in non_brand_tokens):
                return True
            if len(non_brand_tokens) == 1 and non_brand_tokens[0] in self.brand_modifier_tokens:
                return True

        return False

    def _is_underspecified_keyword(self, keyword: str) -> bool:
        """ËøáÊª§Áº∫‰πèÈôêÂÆöËØçÁöÑÊ≥õËØç"""
        if not keyword:
            return True

        keyword_lower = keyword.lower()
        if keyword_lower in self.generic_head_terms:
            return True

        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        if not tokens:
            return True

        if len(tokens) == 1:
            token = tokens[0]
            return token in self.generic_head_terms or len(token) <= 3

        if len(tokens) == 2:
            joined = " ".join(tokens)
            if joined in self.generic_head_terms:
                return True
            if tokens[0] in self.generic_head_terms and tokens[1] in self.generic_head_terms:
                return True
            if tokens[0] in self.generic_lead_tokens and tokens[1] in self.generic_tail_tokens:
                return True
            if tokens[1] in self.generic_lead_tokens and tokens[0] in self.generic_tail_tokens:
                return True
            if not any(token in self.long_tail_tokens for token in tokens):
                return False

        if any(keyword_lower.startswith(prefix) for prefix in self.question_prefixes):
            return False

        if len(tokens) >= 3:
            informative_tokens = [t for t in tokens if t in self.long_tail_tokens]
            return len(informative_tokens) == 0

        return False

    def _identify_brand(self, keyword: str) -> Optional[str]:
        """ËØÜÂà´ÂÖ≥ÈîÆËØçÊâÄÂ±ûÂìÅÁâå"""
        if not keyword:
            return None

        keyword_lower = keyword.lower()
        for phrase in self.brand_phrases:
            if phrase in keyword_lower:
                return phrase

        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        for token in tokens:
            if token in self.brand_tokens:
                return token
        return None

    def _limit_brand_keywords(self, df: pd.DataFrame) -> pd.DataFrame:
        """ÈôêÂà∂Âçï‰∏ÄÂìÅÁâåÁöÑÂÖ≥ÈîÆËØçÊï∞Èáè"""
        if df.empty or 'keyword' not in df.columns or self.max_brand_variations <= 0:
            return df

        keep_indices = []
        brand_counts: Dict[str, int] = {}

        for idx, keyword in df['keyword'].items():
            brand = self._identify_brand(str(keyword))
            if not brand:
                keep_indices.append(idx)
                continue

            count = brand_counts.get(brand, 0)
            if count < self.max_brand_variations:
                keep_indices.append(idx)
                brand_counts[brand] = count + 1

        return df.loc[keep_indices].reset_index(drop=True)
    
    def analyze_keyword_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ÂàÜÊûêÂÖ≥ÈîÆËØçË∂ãÂäø"""
        if df.empty:
            return {}
        
        platform_distribution = df['platform'].value_counts().to_dict() if 'platform' in df.columns else {}

        score_df = df.copy()
        if 'score' not in score_df.columns:
            score_df['score'] = 0.0
        if 'platform' not in score_df.columns:
            score_df['platform'] = 'unknown'

        top_keywords = score_df.nlargest(10, 'score')[['keyword', 'score', 'platform']].to_dict('records')

        analysis = {
            'total_keywords': len(df),
            'platform_distribution': platform_distribution,
            'top_keywords_by_score': top_keywords,
            'keyword_length_stats': {
                'avg_length': df['keyword'].str.len().mean(),
                'min_length': df['keyword'].str.len().min(),
                'max_length': df['keyword'].str.len().max()
            },
            'common_terms': self._get_common_terms(df['keyword'].tolist())
        }
        
        return analysis
    
    def _get_common_terms(self, keywords: List[str]) -> Dict[str, int]:
        """Ëé∑ÂèñÂ∏∏ËßÅËØçÊ±á"""
        all_words = []
        for keyword in keywords:
            words = keyword.lower().split()
            all_words.extend([word for word in words if len(word) > 2])
        
        return dict(Counter(all_words).most_common(20))
    
    def save_results(self, df: pd.DataFrame, analysis: Dict, output_dir: str = 'data/multi_platform_keywords'):
        """‰øùÂ≠òÁªìÊûú"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ‰øùÂ≠òÂÖ≥ÈîÆËØçÊï∞ÊçÆ
        csv_path = os.path.join(output_dir, f'multi_platform_keywords_{timestamp}.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ‰øùÂ≠òÂàÜÊûêÊä•ÂëäÔºàÂ§ÑÁêÜnumpyÁ±ªÂûãÂ∫èÂàóÂåñÈóÆÈ¢òÔºâ
        json_path = os.path.join(output_dir, f'keyword_analysis_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            # ËΩ¨Êç¢numpyÁ±ªÂûã‰∏∫PythonÂéüÁîüÁ±ªÂûã
            def convert_numpy_types(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                else:
                    return obj
            
            serializable_analysis = convert_numpy_types(analysis)
            json.dump(serializable_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ÁªìÊûúÂ∑≤‰øùÂ≠ò:")
        print(f"   ÂÖ≥ÈîÆËØçÊï∞ÊçÆ: {csv_path}")
        print(f"   ÂàÜÊûêÊä•Âëä: {json_path}")
        
        return csv_path, json_path


def run_discovery(input_keywords=None, limit=10, output_dir=None, verbose=True, seed_profile: Optional[str] = None, min_terms: Optional[int] = None):
    """
    ËøêË°åÂ§öÂπ≥Âè∞ÂÖ≥ÈîÆËØçÂèëÁé∞
    
    ÂèÇÊï∞:
        input_keywords: ËæìÂÖ•ÂÖ≥ÈîÆËØçÂàóË°®ÔºåÂ¶ÇÊûú‰∏∫NoneÂàô‰ΩøÁî®ÈªòËÆ§ÂÖ≥ÈîÆËØç
        limit: ÈôêÂà∂‰ΩøÁî®ÁöÑÂÖ≥ÈîÆËØçÊï∞Èáè
        output_dir: ËæìÂá∫ÁõÆÂΩïÔºåÂ¶ÇÊûú‰∏∫NoneÂàô‰ΩøÁî®ÈªòËÆ§ÁõÆÂΩï
        verbose: ÊòØÂê¶ÊâìÂç∞ËØ¶ÁªÜ‰ø°ÊÅØ
    
    ËøîÂõû:
        tuple: (ÂÖ≥ÈîÆËØçDataFrame, ÂàÜÊûêÁªìÊûúÂ≠óÂÖ∏, ËæìÂá∫CSVË∑ØÂæÑ, ËæìÂá∫JSONË∑ØÂæÑ)
    """
    # ÂàùÂßãÂåñÂèëÁé∞Â∑•ÂÖ∑
    discoverer = MultiPlatformKeywordDiscovery()
    
    raw_terms = input_keywords if input_keywords else []
    search_terms = discoverer.prepare_search_terms(
        seeds=raw_terms,
        profile=seed_profile,
        limit=limit,
        min_terms=min_terms
    )
    if verbose:
        if input_keywords:
            print(f"‚úÖ ‰ΩøÁî® {len(raw_terms)} ‰∏™ËæìÂÖ•ÂÖ≥ÈîÆËØçÔºåÂêàÂπ∂ÂêéÊêúÁ¥¢ {len(search_terms)} ‰∏™ÁßçÂ≠êËØç")
        else:
            profile_label = seed_profile or discoverer.default_seed_profile or 'default'
            print(f"‚ÑπÔ∏è ‰ªéÈÖçÁΩÆ '{profile_label}' ËΩΩÂÖ• {len(search_terms)} ‰∏™ÈªòËÆ§ÂÖ≥ÈîÆËØç")

    if verbose:
        print("üîç Â§öÂπ≥Âè∞ÂÖ≥ÈîÆËØçÂèëÁé∞Â∑•ÂÖ∑")
        print(f"üìä ÊêúÁ¥¢ËØçÊ±á: {', '.join(search_terms)}")
        print("-" * 50)
    
    # ÂèëÁé∞ÂÖ≥ÈîÆËØç
    df = discoverer.discover_all_platforms(search_terms)
    
    if not df.empty:
        # ÂàÜÊûêË∂ãÂäø
        analysis = discoverer.analyze_keyword_trends(df)
        
        if verbose:
            # ÊòæÁ§∫ÁªìÊûúÊëòË¶Å
            print("\nüìà ÂèëÁé∞ÁªìÊûúÊëòË¶Å:")
            print(f"ÊÄªÂÖ≥ÈîÆËØçÊï∞: {analysis['total_keywords']}")
            print(f"Âπ≥Âè∞ÂàÜÂ∏É: {analysis['platform_distribution']}")
            print(f"Âπ≥ÂùáÂÖ≥ÈîÆËØçÈïøÂ∫¶: {analysis['keyword_length_stats']['avg_length']:.1f} Â≠óÁ¨¶")
            
            print("\nüèÜ ÁÉ≠Èó®ÂÖ≥ÈîÆËØç (ÊåâËØÑÂàÜÊéíÂ∫è):")
            for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                print(f"  {i}. {kw['keyword']} (ËØÑÂàÜ: {kw['score']}, Êù•Ê∫ê: {kw['platform']})")
            
            print("\nüî§ Â∏∏ËßÅËØçÊ±á:")
            for word, count in list(analysis['common_terms'].items())[:10]:
                print(f"  {word}: {count}Ê¨°")
        
        # ‰øùÂ≠òÁªìÊûú
        csv_path, json_path = discoverer.save_results(df, analysis, output_dir=output_dir)
        
        return df, analysis, csv_path, json_path
    else:
        if verbose:
            print("‚ùå Êú™ÂèëÁé∞‰ªª‰ΩïÂÖ≥ÈîÆËØçÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•ÊàñË∞ÉÊï¥ÊêúÁ¥¢ÂèÇÊï∞")
        return pd.DataFrame(), {}, None, None


def main():
    """ÂëΩ‰ª§Ë°åÂÖ•Âè£ÂáΩÊï∞"""
    import argparse
    
    # ÂëΩ‰ª§Ë°åÂèÇÊï∞Ëß£Êûê
    parser = argparse.ArgumentParser(description="Â§öÂπ≥Âè∞ÂÖ≥ÈîÆËØçÂèëÁé∞Â∑•ÂÖ∑")
    parser.add_argument("--input", "-i", help="ËæìÂÖ•ÂÖ≥ÈîÆËØçÊñá‰ª∂Ë∑ØÂæÑ (CSVÊ†ºÂºèÔºåÂåÖÂê´keywordÂàó)")
    parser.add_argument("--keywords", "-k", help="Áõ¥Êé•ÊåáÂÆöÂÖ≥ÈîÆËØçÔºåÁî®ÈÄóÂè∑ÂàÜÈöî")
    parser.add_argument("--use-root-words", "-r", action="store_true", help="‰ΩøÁî®ËØçÊ†πË∂ãÂäøÊï∞ÊçÆ")
    parser.add_argument("--limit", "-l", type=int, default=10, help="ÊØè‰∏™Êù•Ê∫ê‰ΩøÁî®ÁöÑÂÖ≥ÈîÆËØçÊï∞ÈáèÈôêÂà∂")
    parser.add_argument("--output-dir", "-o", help="ËæìÂá∫ÁõÆÂΩï")
    parser.add_argument("--seed-profile", help="ÊåáÂÆöÈÖçÁΩÆ‰∏≠ÁöÑÁßçÂ≠êÂÖ≥ÈîÆËØçÊ°£Ê°à")
    parser.add_argument("--min-terms", type=int, help="Á°Æ‰øùÊúÄÂ∞ë‰ΩøÁî®ÁöÑÁßçÂ≠êÂÖ≥ÈîÆËØçÊï∞Èáè")
    args = parser.parse_args()
    
    # Ëé∑ÂèñÂàùÂßãÂÖ≥ÈîÆËØç
    search_terms = []
    
    if args.input:
        # ‰ªéCSVÊñá‰ª∂ËØªÂèñÂÖ≥ÈîÆËØç
        try:
            import pandas as pd
            df_input = pd.read_csv(args.input)
            if 'keyword' in df_input.columns:
                search_terms = df_input['keyword'].tolist()
                print(f"‚úÖ ‰ªéÊñá‰ª∂ {args.input} ËØªÂèñ‰∫Ü {len(search_terms)} ‰∏™ÂÖ≥ÈîÆËØç")
            else:
                print("‚ùå ËæìÂÖ•Êñá‰ª∂ÂøÖÈ°ªÂåÖÂê´'keyword'Âàó")
                return
        except Exception as e:
            print(f"‚ùå ËØªÂèñËæìÂÖ•Êñá‰ª∂Â§±Ë¥•: {e}")
            return
    
    elif args.keywords:
        # Áõ¥Êé•‰ΩøÁî®ÊåáÂÆöÁöÑÂÖ≥ÈîÆËØç
        search_terms = [k.strip() for k in args.keywords.split(',')]
        print(f"‚úÖ ‰ΩøÁî®ÊåáÂÆöÁöÑ {len(search_terms)} ‰∏™ÂÖ≥ÈîÆËØç")
    
    elif args.use_root_words:
        # ‰ΩøÁî®ËØçÊ†πÁõ∏ÂÖ≥ÂÖ≥ÈîÆËØç
        try:
            # Â∞ùËØï‰ªéËØçÊ†πË∂ãÂäøÊï∞ÊçÆÁõÆÂΩïËØªÂèñ
            root_words_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'data', 'root_word_trends')
            if os.path.exists(root_words_dir):
                # Êü•ÊâæÊúÄÊñ∞ÁöÑËØçÊ†πË∂ãÂäøÊñá‰ª∂
                files = [f for f in os.listdir(root_words_dir) if f.endswith('.csv')]
                if files:
                    # Êåâ‰øÆÊîπÊó∂Èó¥ÊéíÂ∫èÔºåËé∑ÂèñÊúÄÊñ∞Êñá‰ª∂
                    latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(root_words_dir, f)))
                    file_path = os.path.join(root_words_dir, latest_file)
                    
                    # ËØªÂèñËØçÊ†πË∂ãÂäøÊï∞ÊçÆ
                    import pandas as pd
                    df_roots = pd.read_csv(file_path)
                    if 'keyword' in df_roots.columns:
                        # Ëé∑ÂèñÂÖ≥ÈîÆËØç
                        search_terms = df_roots['keyword'].head(args.limit).tolist()
                        print(f"‚úÖ ‰ªéËØçÊ†πË∂ãÂäøÊñá‰ª∂ {latest_file} ËØªÂèñ‰∫Ü {len(search_terms)} ‰∏™ÂÖ≥ÈîÆËØç")
                    else:
                        raise ValueError("ËØçÊ†πË∂ãÂäøÊñá‰ª∂Áº∫Â∞ë'keyword'Âàó")
                else:
                    raise FileNotFoundError("Êú™ÊâæÂà∞ËØçÊ†πË∂ãÂäøCSVÊñá‰ª∂")
            else:
                raise FileNotFoundError(f"ËØçÊ†πË∂ãÂäøÁõÆÂΩï‰∏çÂ≠òÂú®: {root_words_dir}")
                
        except Exception as e:
            print(f"‚ùå ËØªÂèñËØçÊ†πË∂ãÂäøÊï∞ÊçÆÂ§±Ë¥•: {e}")
            search_terms = []
            print("‚ö†Ô∏è Â∞Ü‰ΩøÁî®ÈÖçÁΩÆ‰∏≠ÁöÑÈªòËÆ§ÁßçÂ≠êÂÖ≥ÈîÆËØç")
    
    # ËøêË°åÂèëÁé∞ËøáÁ®ã
    run_discovery(
        input_keywords=search_terms,
        limit=args.limit,
        output_dir=args.output_dir,
        verbose=True,
        seed_profile=args.seed_profile,
        min_terms=args.min_terms
    )


if __name__ == "__main__":
    main()
    
# Á§∫‰æãÁî®Ê≥ï:
# 1. ÂëΩ‰ª§Ë°å‰ΩøÁî®:
#    - ‰ΩøÁî®ÈªòËÆ§ÂÖ≥ÈîÆËØç: python multi_platform_keyword_discovery.py
#    - ÊåáÂÆöÂÖ≥ÈîÆËØç: python multi_platform_keyword_discovery.py --keywords "AI tools,machine learning,data science"
#    - ‰ªéÊñá‰ª∂ËØªÂèñ: python multi_platform_keyword_discovery.py --input path/to/keywords.csv
#    - ‰ΩøÁî®ËØçÊ†π: python multi_platform_keyword_discovery.py --use-root-words --limit 20
#
# 2. ‰Ωú‰∏∫Ê®°ÂùóÂØºÂÖ•:
#    from demand_mining.tools.multi_platform_keyword_discovery import run_discovery
#    
#    # ‰ΩøÁî®‰∏ªÊµÅÁ®ã‰∏≠Ëé∑ÂèñÁöÑÂÖ≥ÈîÆËØç
#    keywords = ["ai writing", "machine learning", "data science"]
#    df, analysis, csv_path, json_path = run_discovery(
#        input_keywords=keywords,
#        limit=10,
#        output_dir="output/multi_platform_keywords",
#        verbose=True
#    )
#    
#    # ‰ΩøÁî®ÁªìÊûúËøõË°åÂêéÁª≠Â§ÑÁêÜ
#    top_keywords = df.head(20)
