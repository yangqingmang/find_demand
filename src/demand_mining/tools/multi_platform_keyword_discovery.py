#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·
æ•´åˆRedditã€Hacker Newsã€Product Huntç­‰å¹³å°çš„å…³é”®è¯å‘ç°
"""

import requests
import json
import time
import random
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from urllib.parse import quote
import pandas as pd
from collections import Counter
import sys
import os

# æ·»åŠ configç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'config'))
from config.config_manager import get_config
from src.pipeline.cleaning.cleaner import standardize_term, is_valid_term, CleaningConfig

class MultiPlatformKeywordDiscovery:
    """å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

        self.request_timeout = 10
        self.max_retries = 3
        self.retry_backoff = (1, 3)
        
        # åŠ è½½é…ç½®
        self.config = get_config()
        
        # å¹³å°é…ç½®
        self.platforms = {
            'reddit': {
                'base_url': 'https://www.reddit.com',
                'api_url': 'https://www.reddit.com/r/{}/search.json',
                'enabled': True
            },
            'hackernews': {
                'base_url': 'https://hacker-news.firebaseio.com/v0',
                'search_url': 'https://hn.algolia.com/api/v1/search',
                'enabled': True
            },
            'producthunt': {
                'base_url': 'https://api.producthunt.com/v2/api/graphql',
                'enabled': bool(self.config.PRODUCTHUNT_API_TOKEN)
            }
        }
        
        # è®¾ç½®ProductHuntè®¤è¯å¤´
        if self.config.PRODUCTHUNT_API_TOKEN:
            self.ph_headers = {
                'Authorization': f'Bearer {self.config.PRODUCTHUNT_API_TOKEN}',
                'Content-Type': 'application/json',
                'Accept': 'application/json'
            }

        # è¯·æ±‚æ§åˆ¶å‚æ•°
        self._retry_enabled = True

        # AIç›¸å…³subredditåˆ—è¡¨
        self.ai_subreddits = [
            'artificial', 'MachineLearning', 'deeplearning', 'ChatGPT',
            'OpenAI', 'artificial_intelligence', 'singularity', 'Futurology',
            'programming', 'webdev', 'entrepreneur', 'SaaS', 'startups'
        ]
        
        # å…³é”®è¯æå–æ¨¡å¼ï¼ˆç”¨äºè®ºå›/æ–°é—»åŸå§‹æ–‡æœ¬ï¼‰
        self.keyword_patterns = [
            r'\b(?:ai|artificial intelligence|machine learning|deep learning|neural network)\b',
            r'\b(?:tool|software|app|platform|service|solution)\b'
        ]

        self.brand_phrases = {
            'chatgpt', 'openai', 'gpt', 'gpt-4', 'gpt4', 'gpt5', 'claude',
            'midjourney', 'deepseek', 'hix bypass', 'undetectable ai', 'turnitin',
            'copilot'
        }
        self.brand_tokens = {token for phrase in self.brand_phrases for token in phrase.split()}
        self.brand_tokens.update({'chatgpt', 'openai', 'gpt', 'claude', 'midjourney', 'deepseek', 'hix', 'bypass', 'turnitin', 'copilot'})
        self.brand_modifier_tokens = {
            'login', 'logins', 'signup', 'sign', 'account', 'app', 'apps',
            'download', 'downloads', 'premium', 'price', 'prices', 'pricing',
            'cost', 'costs', 'free', 'trial', 'promo', 'discount', 'code',
            'codes', 'coupon', 'review', 'reviews', 'vs', 'versus', 'alternative',
            'alternatives', 'compare', 'comparison', 'checker', 'detector',
            'essay', 'humanizer', 'turnitin', 'unlimited', 'pro', 'plus', 'plan',
            'plans', 'tier', 'tiers', 'lifetime', 'prompt', 'prompts', 'price',
            'pricing'
        }
        self.generic_head_terms = {
            'service', 'services', 'software', 'platform', 'platforms', 'solution',
            'solutions', 'application', 'applications', 'tool', 'tools',
            'machine learning', 'artificial intelligence', 'automation', 'ai',
            'technology', 'technologies', 'gpt'
        }
        self.generic_lead_tokens = {'ai', 'machine', 'software', 'platform', 'service', 'tool', 'technology', 'data'}
        self.generic_tail_tokens = {
            'tool', 'tools', 'software', 'platform', 'platforms', 'service',
            'services', 'application', 'applications', 'app', 'apps', 'solution',
            'solutions', 'system', 'systems', 'suite'
        }
        self.long_tail_tokens = {
            'workflow', 'workflows', 'strategy', 'strategies', 'ideas', 'guide',
            'guides', 'tutorial', 'tutorials', 'template', 'templates', 'checklist',
            'automation', 'process', 'processes', 'plan', 'plans', 'blueprint',
            'blueprints', 'examples', 'case', 'cases', 'study', 'studies', 'use',
            'uses', 'stack', 'stacks', 'integration', 'integrations', 'niche',
            'niches', 'system', 'systems', 'playbook', 'playbooks', 'framework',
            'frameworks', 'marketing', 'seo', 'content', 'workflow', 'roadmap',
            'roadmaps', 'setup', 'automation', 'builders', 'for', 'beginners',
            'advanced', 'agency', 'agencies', 'students', 'writers', 'designers',
            'developers', 'founders', 'startups', 'teams', 'checklists'
        }
        self.question_prefixes = (
            'how to', 'how do', 'how can', 'what is', 'what are', 'why', 'should i',
            'can i', 'is there', 'best way', 'ways to'
        )
        self.max_brand_variations = 5
        self._cleaning_config = CleaningConfig()

    def _request_with_retry(self, method: str, url: str, **kwargs) -> requests.Response:
        """å¯¹å¤–éƒ¨è¯·æ±‚å¢åŠ è¶…æ—¶ä¸é‡è¯•æœºåˆ¶"""
        kwargs.setdefault('timeout', self.request_timeout)
        attempt = 0
        while True:
            attempt += 1
            try:
                request_func = getattr(self.session, method)
                response = request_func(url, **kwargs)
                response.raise_for_status()
                return response
            except requests.RequestException as exc:
                if not self._retry_enabled or attempt >= self.max_retries:
                    raise
                sleep_for = random.uniform(*self.retry_backoff)
                time.sleep(sleep_for)

    def discover_reddit_keywords(self, subreddit: str, limit: int = 100) -> List[Dict]:
        """ä»Redditå‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Reddit r/{subreddit}...")
        
        keywords = []
        
        try:
            # è·å–çƒ­é—¨å¸–å­
            url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
            response = self._request_with_retry('get', url)

            data = response.json()
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                selftext = post_data.get('selftext', '')
                score = post_data.get('score', 0)
                num_comments = post_data.get('num_comments', 0)
                
                # æå–å…³é”®è¯
                extracted_keywords = self._extract_keywords_from_text(title + ' ' + selftext)
                
                for keyword in extracted_keywords:
                    keywords.append({
                        'keyword': keyword,
                        'source': f'reddit_r_{subreddit}',
                        'title': title,
                        'score': score,
                        'comments': num_comments,
                        'url': f"https://reddit.com{post_data.get('permalink', '')}",
                        'platform': 'reddit'
                    })
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"âŒ Reddit r/{subreddit} åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_hackernews_keywords(self, query: str = 'AI', days: int = 30) -> List[Dict]:
        """ä»Hacker Newså‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Hacker News (æŸ¥è¯¢: {query})...")
        
        keywords = []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = int(time.time())
            start_time = end_time - (days * 24 * 3600)
            
            # æœç´¢ç›¸å…³å¸–å­
            url = "https://hn.algolia.com/api/v1/search"
            params = {
                'query': query,
                'tags': 'story',
                'numericFilters': f'created_at_i>{start_time},created_at_i<{end_time}',
                'hitsPerPage': 100
            }
            
            response = self._request_with_retry('get', url, params=params)

            data = response.json()
            hits = data.get('hits', [])
            
            for hit in hits:
                title = hit.get('title', '')
                url = hit.get('url', '')
                points = hit.get('points', 0)
                num_comments = hit.get('num_comments', 0)
                
                # æå–å…³é”®è¯
                extracted_keywords = self._extract_keywords_from_text(title)
                
                for keyword in extracted_keywords:
                    keywords.append({
                        'keyword': keyword,
                        'source': 'hackernews',
                        'title': title,
                        'score': points,
                        'comments': num_comments,
                        'url': url or f"https://news.ycombinator.com/item?id={hit.get('objectID')}",
                        'platform': 'hackernews'
                    })
        
        except Exception as e:
            print(f"âŒ Hacker News åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_youtube_keywords(self, search_term: str) -> List[Dict]:
        """ä»YouTubeæœç´¢å»ºè®®å‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ YouTube æœç´¢å»ºè®® (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        try:
            # YouTubeæœç´¢å»ºè®®API (éå®˜æ–¹)
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'youtube',
                'ds': 'yt',
                'q': search_term
            }
            
            response = self._request_with_retry('get', url, params=params)

            # è§£æå“åº” (JSONPæ ¼å¼)
            content = response.text
            if content.startswith('window.google.ac.h('):
                json_str = content[19:-1]  # ç§»é™¤JSONPåŒ…è£…
                data = json.loads(json_str)
                
                suggestions = data[1] if len(data) > 1 else []
                
                for suggestion in suggestions:
                    if isinstance(suggestion, list) and len(suggestion) > 0:
                        keyword = suggestion[0]
                        keywords.append({
                            'keyword': keyword,
                            'source': 'youtube_suggestions',
                            'title': f'YouTubeæœç´¢å»ºè®®: {keyword}',
                            'score': 0,
                            'comments': 0,
                            'url': f'https://www.youtube.com/results?search_query={quote(keyword)}',
                            'platform': 'youtube'
                        })
        
        except Exception as e:
            print(f"âŒ YouTube åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_google_suggestions(self, search_term: str) -> List[Dict]:
        """ä»Googleæœç´¢å»ºè®®å‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Google æœç´¢å»ºè®® (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        try:
            # Googleæœç´¢å»ºè®®API
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'firefox',
                'q': search_term
            }
            
            response = self._request_with_retry('get', url, params=params)

            data = response.json()
            suggestions = data[1] if len(data) > 1 else []
            
            for suggestion in suggestions:
                keywords.append({
                    'keyword': suggestion,
                    'source': 'google_suggestions',
                    'title': f'Googleæœç´¢å»ºè®®: {suggestion}',
                    'score': 0,
                    'comments': 0,
                    'url': f'https://www.google.com/search?q={quote(suggestion)}',
                    'platform': 'google'
                })
        
        except Exception as e:
            print(f"âŒ Googleæœç´¢å»ºè®®åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_producthunt_keywords(self, search_term: str = "AI", days: int = 30) -> List[Dict]:
        """ä»ProductHuntå‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ ProductHunt (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        if not self.config.PRODUCTHUNT_API_TOKEN:
            print("âš ï¸ ProductHunt API Tokenæœªé…ç½®ï¼Œè·³è¿‡ProductHuntåˆ†æ")
            return keywords
        
        try:
            # ProductHunt GraphQLæŸ¥è¯¢
            query = """
            query($search: String!, $first: Int!) {
              posts(search: $search, first: $first, order: VOTES) {
                edges {
                  node {
                    id
                    name
                    tagline
                    description
                    votesCount
                    commentsCount
                    url
                    topics {
                      edges {
                        node {
                          name
                        }
                      }
                    }
                  }
                }
              }
            }
            """
            
            variables = {
                "search": search_term,
                "first": 50
            }
            
            response = self._request_with_retry(
                'post',
                self.platforms['producthunt']['base_url'],
                headers=self.ph_headers,
                json={
                    'query': query,
                    'variables': variables
                }
            )

            data = response.json()
            
            if 'data' in data and 'posts' in data['data']:
                posts = data['data']['posts']['edges']
                
                for post in posts:
                    node = post['node']
                    name = node.get('name', '')
                    tagline = node.get('tagline', '')
                    description = node.get('description', '')
                    votes = node.get('votesCount', 0)
                    comments = node.get('commentsCount', 0)
                    url = node.get('url', '')
                    
                    # ä»äº§å“åç§°ã€æ ‡è¯­å’Œæè¿°ä¸­æå–å…³é”®è¯
                    text_content = f"{name} {tagline} {description}"
                    extracted_keywords = self._extract_keywords_from_text(text_content)
                    
                    # æ·»åŠ ä¸»é¢˜æ ‡ç­¾ä½œä¸ºå…³é”®è¯
                    topics = node.get('topics', {}).get('edges', [])
                    for topic in topics:
                        topic_name = topic.get('node', {}).get('name', '')
                        if topic_name:
                            extracted_keywords.append(topic_name.lower())
                    
                    for keyword in extracted_keywords:
                        keywords.append({
                            'keyword': keyword,
                            'source': 'producthunt',
                            'title': name,
                            'score': votes,
                            'comments': comments,
                            'url': url,
                            'platform': 'producthunt'
                        })
        
        except Exception as e:
            print(f"âŒ ProductHunt åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆè®ºå›/æ–°é—»åè¯çŸ­è¯­æŠ½å– + æ¨¡å¼è¯ï¼‰"""
        keywords = []
        text = text.lower()
        try:
            import nltk
            from nltk import pos_tag, word_tokenize, RegexpParser
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt', quiet=True)
            try:
                nltk.data.find('taggers/averaged_perceptron_tagger')
            except LookupError:
                nltk.download('averaged_perceptron_tagger', quiet=True)
            tokens = word_tokenize(text)
            tagged = pos_tag(tokens)
            grammar = r"NP: {<JJ>*<NN|NNS|NNP|NNPS>+}"
            cp = RegexpParser(grammar)
            tree = cp.parse(tagged)
            nps = []
            for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):
                phrase = ' '.join(w for w, t in subtree.leaves())
                nps.append(phrase)
            for np in nps:
                np_clean = re.sub(r'[^a-z0-9\s-]', ' ', np).strip()
                if 3 <= len(np_clean) <= 40 and 1 <= len(np_clean.split()) <= 3:
                    keywords.append(np_clean)
        except Exception:
            pass
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                keyword = re.sub(r'[^a-z0-9\s-]', ' ', match).strip()
                if 3 <= len(keyword) <= 40 and 1 <= len(keyword.split()) <= 3:
                    keywords.append(keyword)
        base = [
            'ai tool', 'ai generator', 'ai writer', 'ai assistant', 'ai chatbot',
            'machine learning', 'deep learning', 'neural network', 'gpt',
            'artificial intelligence', 'automation', 'nlp', 'computer vision'
        ]
        for term in base:
            if term in text and 3 <= len(term) <= 40:
                keywords.append(term)
        return list(set(keywords))
    
    def _is_long_tail_keyword(self, keyword: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé•¿å°¾å…³é”®è¯
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            æ˜¯å¦ä¸ºé•¿å°¾å…³é”®è¯
        """
        words = keyword.split()
        # é•¿å°¾è¯æ ‡å‡†ï¼š3ä¸ªä»¥ä¸Šè¯æ±‡ä¸”æ€»é•¿åº¦15ä¸ªå­—ç¬¦ä»¥ä¸Š
        return len(words) >= 3 and len(keyword) >= 15
    
    def _calculate_long_tail_score(self, keyword: str) -> float:
        """
        è®¡ç®—é•¿å°¾è¯è¯„åˆ†åŠ æƒ
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            è¯„åˆ†å€æ•°
        """
        word_count = len(keyword.split())
        keyword_lower = keyword.lower()
        
        base_score = 1.0
        
        # åŸºäºè¯æ•°çš„è¯„åˆ†åŠ æƒ
        if word_count >= 5:
            base_score *= 3.0  # 5+è¯æ±‡è·å¾—æœ€é«˜åŠ æƒ
        elif word_count >= 4:
            base_score *= 2.5  # 4è¯æ±‡è·å¾—é«˜åŠ æƒ
        elif word_count >= 3:
            base_score *= 2.0  # 3è¯æ±‡è·å¾—ä¸­ç­‰åŠ æƒ
        
        # åŸºäºæ„å›¾æ˜ç¡®æ€§çš„åŠ æƒ
        high_intent_phrases = ['how to', 'step by step', 'tutorial', 'guide', 'without', 'for beginners']
        if any(phrase in keyword_lower for phrase in high_intent_phrases):
            base_score *= 1.5
        
        # åŸºäºç«äº‰åº¦çš„è°ƒæ•´
        high_competition_words = ['best', 'top', 'review', 'vs', 'comparison']
        if any(comp in keyword_lower for comp in high_competition_words):
            base_score *= 0.6  # é«˜ç«äº‰è¯é™ä½è¯„åˆ†

        return base_score

    def _is_brand_heavy(self, keyword: str) -> bool:
        """è¿‡æ»¤å“ç‰Œè¯åŠå…¶å¸¸è§é™„å±è¯"""
        if not keyword:
            return False

        keyword_lower = keyword.lower()
        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        if not tokens:
            return False

        brand_present = any(phrase in keyword_lower for phrase in self.brand_phrases)
        if not brand_present:
            brand_present = any(token in self.brand_tokens for token in tokens)
        if not brand_present:
            return False

        non_brand_tokens = [t for t in tokens if t not in self.brand_tokens]
        if not non_brand_tokens:
            return True

        if all(token in self.brand_modifier_tokens for token in non_brand_tokens):
            return True

        if len(non_brand_tokens) == 1 and non_brand_tokens[0] in self.brand_modifier_tokens:
            return True

        return False

    def _is_underspecified_keyword(self, keyword: str) -> bool:
        """è¿‡æ»¤ç¼ºä¹é™å®šè¯çš„æ³›è¯"""
        if not keyword:
            return True

        keyword_lower = keyword.lower()
        if keyword_lower in self.generic_head_terms:
            return True

        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        if not tokens:
            return True

        if len(tokens) == 1:
            token = tokens[0]
            return token in self.generic_head_terms or len(token) <= 3

        if len(tokens) == 2:
            joined = " ".join(tokens)
            if joined in self.generic_head_terms:
                return True
            if tokens[0] in self.generic_head_terms and tokens[1] in self.generic_head_terms:
                return True
            if tokens[0] in self.generic_lead_tokens and tokens[1] in self.generic_tail_tokens:
                return True
            if tokens[1] in self.generic_lead_tokens and tokens[0] in self.generic_tail_tokens:
                return True
            if not any(token in self.long_tail_tokens for token in tokens):
                return False

        if any(keyword_lower.startswith(prefix) for prefix in self.question_prefixes):
            return False

        if len(tokens) >= 3:
            informative_tokens = [t for t in tokens if t in self.long_tail_tokens]
            return len(informative_tokens) == 0

        return False

    def _identify_brand(self, keyword: str) -> Optional[str]:
        """è¯†åˆ«å…³é”®è¯æ‰€å±å“ç‰Œ"""
        if not keyword:
            return None

        keyword_lower = keyword.lower()
        for phrase in self.brand_phrases:
            if phrase in keyword_lower:
                return phrase

        tokens = [t for t in re.split(r"[^a-z0-9]+", keyword_lower) if t]
        for token in tokens:
            if token in self.brand_tokens:
                return token
        return None

    def _limit_brand_keywords(self, df: pd.DataFrame) -> pd.DataFrame:
        """é™åˆ¶å•ä¸€å“ç‰Œçš„å…³é”®è¯æ•°é‡"""
        if df.empty or 'keyword' not in df.columns:
            return df

        keep_indices = []
        brand_counts: Dict[str, int] = {}

        for idx, keyword in df['keyword'].items():
            brand = self._identify_brand(str(keyword))
            if not brand:
                keep_indices.append(idx)
                continue

            count = brand_counts.get(brand, 0)
            if count < self.max_brand_variations:
                keep_indices.append(idx)
                brand_counts[brand] = count + 1

        return df.loc[keep_indices].reset_index(drop=True)
    
    def discover_all_platforms(self, search_terms: List[str]) -> pd.DataFrame:
        """ä»æ‰€æœ‰å¹³å°å‘ç°å…³é”®è¯"""
        print("ğŸš€ å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
        
        all_keywords = []
        
        # Redditåˆ†æ
        for subreddit in self.ai_subreddits[:5]:  # é™åˆ¶æ•°é‡é¿å…è¯·æ±‚è¿‡å¤š
            reddit_keywords = self.discover_reddit_keywords(subreddit, limit=50)
            all_keywords.extend(reddit_keywords)
        
        # Hacker Newsåˆ†æ
        for term in search_terms:
            hn_keywords = self.discover_hackernews_keywords(term)
            all_keywords.extend(hn_keywords)
        
        # YouTubeæœç´¢å»ºè®®
        for term in search_terms:
            youtube_keywords = self.discover_youtube_keywords(term)
            all_keywords.extend(youtube_keywords)
        
        # Googleæœç´¢å»ºè®®
        # Googleæœç´¢å»ºè®®
        for term in search_terms:
            google_keywords = self.discover_google_suggestions(term)
            all_keywords.extend(google_keywords)
        
        # ProductHuntåˆ†æ
        if self.platforms['producthunt']['enabled']:
            for term in search_terms:
                ph_keywords = self.discover_producthunt_keywords(term)
                all_keywords.extend(ph_keywords)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_keywords)
        
        if 'keyword' not in df.columns:
            print("âš ï¸ ç»“æœç¼ºå°‘å…³é”®è¯å­—æ®µ")
            return df

        if not df.empty:
            try:
                df['keyword'] = df['keyword'].astype(str)
                df['normalized_keyword'] = df['keyword'].apply(standardize_term)
                df = df[df['normalized_keyword'].apply(lambda term: is_valid_term(term, self._cleaning_config))]
                df = df.drop_duplicates(subset='normalized_keyword')
                df['keyword'] = df['normalized_keyword']
                df = df.drop(columns=['normalized_keyword'])
            except Exception as exc:
                print(f"âš ï¸ å…³é”®è¯æ¸…æ´—å¤±è´¥: {exc}")
                df['keyword'] = df['keyword'].astype(str)

            if df.empty:
                print("âš ï¸ æ¸…æ´—åæ— æœ‰æ•ˆå…³é”®è¯")
            else:
                if 'platform' not in df.columns:
                    df['platform'] = 'unknown'
                else:
                    df['platform'] = df['platform'].fillna('unknown')

                before_brand = len(df)
                df = df[~df['keyword'].apply(self._is_brand_heavy)].reset_index(drop=True)
                if len(df) < before_brand:
                    print(f"âš ï¸ ç§»é™¤äº† {before_brand - len(df)} ä¸ªå“ç‰Œæ³›è¯")

                before_generic = len(df)
                df = df[~df['keyword'].apply(self._is_underspecified_keyword)].reset_index(drop=True)
                if len(df) < before_generic:
                    print(f"âš ï¸ ç§»é™¤äº† {before_generic - len(df)} ä¸ªæ³›åŒ–å¤´éƒ¨è¯")

                df = self._limit_brand_keywords(df)

                if df.empty:
                    print("âš ï¸ è¿‡æ»¤åæ— æœ‰æ•ˆå…³é”®è¯")
                else:
                    try:
                        from datasketch import MinHash, MinHashLSH
                        from sklearn.cluster import AgglomerativeClustering
                        from sentence_transformers import SentenceTransformer
                        import numpy as np

                        texts = df['keyword'].tolist()

                        def shingles(s, k=3):
                            s = s.replace(' ', '_')
                            return {s[i:i+k] for i in range(max(len(s) - k + 1, 1))}

                        mhashes = []
                        for t in texts:
                            mh = MinHash(num_perm=64)
                            for sh in shingles(t):
                                mh.update(sh.encode('utf-8'))
                            mhashes.append(mh)
                        lsh = MinHashLSH(threshold=0.85, num_perm=64)
                        for idx, mh in enumerate(mhashes):
                            lsh.insert(str(idx), mh)
                        keep_idx = []
                        seen = set()
                        for i, mh in enumerate(mhashes):
                            if i in seen:
                                continue
                            near = lsh.query(mh)
                            grp = sorted(int(x) for x in near)
                            for j in grp:
                                seen.add(j)
                            keep_idx.append(grp[0])
                        df = df.iloc[keep_idx].reset_index(drop=True)

                        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
                        emb = model.encode(df['keyword'].tolist(), normalize_embeddings=True)
                        if len(df) >= 5:
                            clustering = AgglomerativeClustering(
                                n_clusters=None,
                                distance_threshold=0.18,
                                affinity='cosine',
                                linkage='average'
                            )
                            labels = clustering.fit_predict(emb)
                            df['cluster_id'] = labels
                            reps = []
                            for c in sorted(set(labels)):
                                idxs = np.where(labels == c)[0]
                                sub = emb[idxs]
                                center = sub.mean(axis=0, keepdims=True)
                                sims = (sub @ center.T).ravel()
                                rep = idxs[int(np.argmax(sims))]
                                reps.append(rep)
                            df = df.iloc[sorted(set(reps))].reset_index(drop=True)
                        else:
                            df['cluster_id'] = 0
                    except Exception:
                        pass

                    df['score'] = 0 if 'score' not in df.columns else df['score']
                    df['long_tail_score'] = df['keyword'].apply(self._calculate_long_tail_score)
                    df['weighted_score'] = df['score'] * df['long_tail_score']
                    df = df.sort_values('weighted_score', ascending=False)
                    df['discovered_at'] = datetime.now().isoformat()
                    print(f"âœ… å‘ç° {len(df)} ä¸ªå…³é”®è¯")
        else:
            print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯")

        return df
    
    def analyze_keyword_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        if df.empty:
            return {}
        
        platform_distribution = df['platform'].value_counts().to_dict() if 'platform' in df.columns else {}

        score_df = df.copy()
        if 'score' not in score_df.columns:
            score_df['score'] = 0.0
        if 'platform' not in score_df.columns:
            score_df['platform'] = 'unknown'

        top_keywords = score_df.nlargest(10, 'score')[['keyword', 'score', 'platform']].to_dict('records')

        analysis = {
            'total_keywords': len(df),
            'platform_distribution': platform_distribution,
            'top_keywords_by_score': top_keywords,
            'keyword_length_stats': {
                'avg_length': df['keyword'].str.len().mean(),
                'min_length': df['keyword'].str.len().min(),
                'max_length': df['keyword'].str.len().max()
            },
            'common_terms': self._get_common_terms(df['keyword'].tolist())
        }
        
        return analysis
    
    def _get_common_terms(self, keywords: List[str]) -> Dict[str, int]:
        """è·å–å¸¸è§è¯æ±‡"""
        all_words = []
        for keyword in keywords:
            words = keyword.lower().split()
            all_words.extend([word for word in words if len(word) > 2])
        
        return dict(Counter(all_words).most_common(20))
    
    def save_results(self, df: pd.DataFrame, analysis: Dict, output_dir: str = 'data/multi_platform_keywords'):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜å…³é”®è¯æ•°æ®
        csv_path = os.path.join(output_dir, f'multi_platform_keywords_{timestamp}.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜åˆ†ææŠ¥å‘Šï¼ˆå¤„ç†numpyç±»å‹åºåˆ—åŒ–é—®é¢˜ï¼‰
        json_path = os.path.join(output_dir, f'keyword_analysis_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                else:
                    return obj
            
            serializable_analysis = convert_numpy_types(analysis)
            json.dump(serializable_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜:")
        print(f"   å…³é”®è¯æ•°æ®: {csv_path}")
        print(f"   åˆ†ææŠ¥å‘Š: {json_path}")
        
        return csv_path, json_path


def run_discovery(input_keywords=None, limit=10, output_dir=None, verbose=True):
    """
    è¿è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°
    
    å‚æ•°:
        input_keywords: è¾“å…¥å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯
        limit: é™åˆ¶ä½¿ç”¨çš„å…³é”®è¯æ•°é‡
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
        tuple: (å…³é”®è¯DataFrame, åˆ†æç»“æœå­—å…¸, è¾“å‡ºCSVè·¯å¾„, è¾“å‡ºJSONè·¯å¾„)
    """
    # åˆå§‹åŒ–å‘ç°å·¥å…·
    discoverer = MultiPlatformKeywordDiscovery()
    
    # è·å–åˆå§‹å…³é”®è¯
    if input_keywords is None or len(input_keywords) == 0:
        # ä½¿ç”¨é»˜è®¤å…³é”®è¯
        search_terms = [
            'AI tool', 'AI generator', 'AI writer', 'AI assistant',
            'machine learning', 'chatbot', 'automation'
        ]
        if verbose:
            print(f"â„¹ï¸ ä½¿ç”¨é»˜è®¤å…³é”®è¯: {', '.join(search_terms)}")
    else:
        search_terms = input_keywords
        if verbose:
            print(f"âœ… ä½¿ç”¨æä¾›çš„ {len(search_terms)} ä¸ªå…³é”®è¯")
    
    # é™åˆ¶å…³é”®è¯æ•°é‡
    if limit and len(search_terms) > limit:
        search_terms = search_terms[:limit]
        if verbose:
            print(f"â„¹ï¸ é™åˆ¶ä½¿ç”¨å‰ {limit} ä¸ªå…³é”®è¯")
    
    if verbose:
        print("ğŸ” å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·")
        print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
        print("-" * 50)
    
    # å‘ç°å…³é”®è¯
    df = discoverer.discover_all_platforms(search_terms)
    
    if not df.empty:
        # åˆ†æè¶‹åŠ¿
        analysis = discoverer.analyze_keyword_trends(df)
        
        if verbose:
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print("\nğŸ“ˆ å‘ç°ç»“æœæ‘˜è¦:")
            print(f"æ€»å…³é”®è¯æ•°: {analysis['total_keywords']}")
            print(f"å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
            print(f"å¹³å‡å…³é”®è¯é•¿åº¦: {analysis['keyword_length_stats']['avg_length']:.1f} å­—ç¬¦")
            
            print("\nğŸ† çƒ­é—¨å…³é”®è¯ (æŒ‰è¯„åˆ†æ’åº):")
            for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            
            print("\nğŸ”¤ å¸¸è§è¯æ±‡:")
            for word, count in list(analysis['common_terms'].items())[:10]:
                print(f"  {word}: {count}æ¬¡")
        
        # ä¿å­˜ç»“æœ
        csv_path, json_path = discoverer.save_results(df, analysis, output_dir=output_dir)
        
        return df, analysis, csv_path, json_path
    else:
        if verbose:
            print("âŒ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
        return pd.DataFrame(), {}, None, None


def main():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·")
    parser.add_argument("--input", "-i", help="è¾“å…¥å…³é”®è¯æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼ï¼ŒåŒ…å«keywordåˆ—)")
    parser.add_argument("--keywords", "-k", help="ç›´æ¥æŒ‡å®šå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”")
    parser.add_argument("--use-root-words", "-r", action="store_true", help="ä½¿ç”¨è¯æ ¹è¶‹åŠ¿æ•°æ®")
    parser.add_argument("--limit", "-l", type=int, default=10, help="æ¯ä¸ªæ¥æºä½¿ç”¨çš„å…³é”®è¯æ•°é‡é™åˆ¶")
    parser.add_argument("--output-dir", "-o", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    # è·å–åˆå§‹å…³é”®è¯
    search_terms = []
    
    if args.input:
        # ä»CSVæ–‡ä»¶è¯»å–å…³é”®è¯
        try:
            import pandas as pd
            df_input = pd.read_csv(args.input)
            if 'keyword' in df_input.columns:
                search_terms = df_input['keyword'].tolist()
                print(f"âœ… ä»æ–‡ä»¶ {args.input} è¯»å–äº† {len(search_terms)} ä¸ªå…³é”®è¯")
            else:
                print("âŒ è¾“å…¥æ–‡ä»¶å¿…é¡»åŒ…å«'keyword'åˆ—")
                return
        except Exception as e:
            print(f"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return
    
    elif args.keywords:
        # ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„å…³é”®è¯
        search_terms = [k.strip() for k in args.keywords.split(',')]
        print(f"âœ… ä½¿ç”¨æŒ‡å®šçš„ {len(search_terms)} ä¸ªå…³é”®è¯")
    
    elif args.use_root_words:
        # ä½¿ç”¨è¯æ ¹ç›¸å…³å…³é”®è¯
        try:
            # å°è¯•ä»è¯æ ¹è¶‹åŠ¿æ•°æ®ç›®å½•è¯»å–
            root_words_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'data', 'root_word_trends')
            if os.path.exists(root_words_dir):
                # æŸ¥æ‰¾æœ€æ–°çš„è¯æ ¹è¶‹åŠ¿æ–‡ä»¶
                files = [f for f in os.listdir(root_words_dir) if f.endswith('.csv')]
                if files:
                    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°æ–‡ä»¶
                    latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(root_words_dir, f)))
                    file_path = os.path.join(root_words_dir, latest_file)
                    
                    # è¯»å–è¯æ ¹è¶‹åŠ¿æ•°æ®
                    import pandas as pd
                    df_roots = pd.read_csv(file_path)
                    if 'keyword' in df_roots.columns:
                        # è·å–å…³é”®è¯
                        search_terms = df_roots['keyword'].head(args.limit).tolist()
                        print(f"âœ… ä»è¯æ ¹è¶‹åŠ¿æ–‡ä»¶ {latest_file} è¯»å–äº† {len(search_terms)} ä¸ªå…³é”®è¯")
                    else:
                        raise ValueError("è¯æ ¹è¶‹åŠ¿æ–‡ä»¶ç¼ºå°‘'keyword'åˆ—")
                else:
                    raise FileNotFoundError("æœªæ‰¾åˆ°è¯æ ¹è¶‹åŠ¿CSVæ–‡ä»¶")
            else:
                raise FileNotFoundError(f"è¯æ ¹è¶‹åŠ¿ç›®å½•ä¸å­˜åœ¨: {root_words_dir}")
                
        except Exception as e:
            print(f"âŒ è¯»å–è¯æ ¹è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å…³é”®è¯ä½œä¸ºå¤‡é€‰
            search_terms = [
                'AI tool', 'AI generator', 'AI writer', 'AI assistant',
                'machine learning', 'chatbot', 'automation'
            ]
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤å…³é”®è¯: {', '.join(search_terms)}")
    
    # è¿è¡Œå‘ç°è¿‡ç¨‹
    run_discovery(
        input_keywords=search_terms,
        limit=args.limit,
        output_dir=args.output_dir,
        verbose=True
    )


if __name__ == "__main__":
    main()
    
# ç¤ºä¾‹ç”¨æ³•:
# 1. å‘½ä»¤è¡Œä½¿ç”¨:
#    - ä½¿ç”¨é»˜è®¤å…³é”®è¯: python multi_platform_keyword_discovery.py
#    - æŒ‡å®šå…³é”®è¯: python multi_platform_keyword_discovery.py --keywords "AI tools,machine learning,data science"
#    - ä»æ–‡ä»¶è¯»å–: python multi_platform_keyword_discovery.py --input path/to/keywords.csv
#    - ä½¿ç”¨è¯æ ¹: python multi_platform_keyword_discovery.py --use-root-words --limit 20
#
# 2. ä½œä¸ºæ¨¡å—å¯¼å…¥:
#    from demand_mining.tools.multi_platform_keyword_discovery import run_discovery
#    
#    # ä½¿ç”¨ä¸»æµç¨‹ä¸­è·å–çš„å…³é”®è¯
#    keywords = ["ai writing", "machine learning", "data science"]
#    df, analysis, csv_path, json_path = run_discovery(
#        input_keywords=keywords,
#        limit=10,
#        output_dir="output/multi_platform_keywords",
#        verbose=True
#    )
#    
#    # ä½¿ç”¨ç»“æœè¿›è¡Œåç»­å¤„ç†
#    top_keywords = df.head(20)
