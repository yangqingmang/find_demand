#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·
æ•´åˆRedditã€Hacker Newsã€Product Huntç­‰å¹³å°çš„å…³é”®è¯å‘ç°
"""

import requests
import json
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from urllib.parse import quote
import pandas as pd
from collections import Counter

class MultiPlatformKeywordDiscovery:
    """å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        # å¹³å°é…ç½®
        self.platforms = {
            'reddit': {
                'base_url': 'https://www.reddit.com',
                'api_url': 'https://www.reddit.com/r/{}/search.json',
                'enabled': True
            },
            'hackernews': {
                'base_url': 'https://hacker-news.firebaseio.com/v0',
                'search_url': 'https://hn.algolia.com/api/v1/search',
                'enabled': True
            },
            'producthunt': {
                'base_url': 'https://www.producthunt.com',
                'enabled': False  # éœ€è¦APIå¯†é’¥
            }
        }
        
        # AIç›¸å…³subredditåˆ—è¡¨
        self.ai_subreddits = [
            'artificial', 'MachineLearning', 'deeplearning', 'ChatGPT',
            'OpenAI', 'artificial_intelligence', 'singularity', 'Futurology',
            'programming', 'webdev', 'entrepreneur', 'SaaS', 'startups'
        ]
        
        # å…³é”®è¯æå–æ¨¡å¼
        self.keyword_patterns = [
            r'\b(?:how to|what is|best|top|vs|compare|review|tutorial|guide)\b[^.!?]*',
            r'\b(?:AI|artificial intelligence|machine learning|deep learning|neural network)\b[^.!?]*',
            r'\b(?:tool|software|app|platform|service|solution)\b[^.!?]*',
            r'\b(?:free|open source|alternative|competitor)\b[^.!?]*'
        ]
    
    def discover_reddit_keywords(self, subreddit: str, limit: int = 100) -> List[Dict]:
        """ä»Redditå‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Reddit r/{subreddit}...")
        
        keywords = []
        
        try:
            # è·å–çƒ­é—¨å¸–å­
            url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
            response = self.session.get(url)
            response.raise_for_status()
            
            data = response.json()
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                selftext = post_data.get('selftext', '')
                score = post_data.get('score', 0)
                num_comments = post_data.get('num_comments', 0)
                
                # æå–å…³é”®è¯
                extracted_keywords = self._extract_keywords_from_text(title + ' ' + selftext)
                
                for keyword in extracted_keywords:
                    keywords.append({
                        'keyword': keyword,
                        'source': f'reddit_r_{subreddit}',
                        'title': title,
                        'score': score,
                        'comments': num_comments,
                        'url': f"https://reddit.com{post_data.get('permalink', '')}",
                        'platform': 'reddit'
                    })
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"âŒ Reddit r/{subreddit} åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_hackernews_keywords(self, query: str = 'AI', days: int = 30) -> List[Dict]:
        """ä»Hacker Newså‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Hacker News (æŸ¥è¯¢: {query})...")
        
        keywords = []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = int(time.time())
            start_time = end_time - (days * 24 * 3600)
            
            # æœç´¢ç›¸å…³å¸–å­
            url = "https://hn.algolia.com/api/v1/search"
            params = {
                'query': query,
                'tags': 'story',
                'numericFilters': f'created_at_i>{start_time},created_at_i<{end_time}',
                'hitsPerPage': 100
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            hits = data.get('hits', [])
            
            for hit in hits:
                title = hit.get('title', '')
                url = hit.get('url', '')
                points = hit.get('points', 0)
                num_comments = hit.get('num_comments', 0)
                
                # æå–å…³é”®è¯
                extracted_keywords = self._extract_keywords_from_text(title)
                
                for keyword in extracted_keywords:
                    keywords.append({
                        'keyword': keyword,
                        'source': 'hackernews',
                        'title': title,
                        'score': points,
                        'comments': num_comments,
                        'url': url or f"https://news.ycombinator.com/item?id={hit.get('objectID')}",
                        'platform': 'hackernews'
                    })
        
        except Exception as e:
            print(f"âŒ Hacker News åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_youtube_keywords(self, search_term: str) -> List[Dict]:
        """ä»YouTubeæœç´¢å»ºè®®å‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ YouTube æœç´¢å»ºè®® (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        try:
            # YouTubeæœç´¢å»ºè®®API (éå®˜æ–¹)
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'youtube',
                'ds': 'yt',
                'q': search_term
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            # è§£æå“åº” (JSONPæ ¼å¼)
            content = response.text
            if content.startswith('window.google.ac.h('):
                json_str = content[19:-1]  # ç§»é™¤JSONPåŒ…è£…
                data = json.loads(json_str)
                
                suggestions = data[1] if len(data) > 1 else []
                
                for suggestion in suggestions:
                    if isinstance(suggestion, list) and len(suggestion) > 0:
                        keyword = suggestion[0]
                        keywords.append({
                            'keyword': keyword,
                            'source': 'youtube_suggestions',
                            'title': f'YouTubeæœç´¢å»ºè®®: {keyword}',
                            'score': 0,
                            'comments': 0,
                            'url': f'https://www.youtube.com/results?search_query={quote(keyword)}',
                            'platform': 'youtube'
                        })
        
        except Exception as e:
            print(f"âŒ YouTube åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_google_suggestions(self, search_term: str) -> List[Dict]:
        """ä»Googleæœç´¢å»ºè®®å‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Google æœç´¢å»ºè®® (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        try:
            # Googleæœç´¢å»ºè®®API
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'firefox',
                'q': search_term
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            suggestions = data[1] if len(data) > 1 else []
            
            for suggestion in suggestions:
                keywords.append({
                    'keyword': suggestion,
                    'source': 'google_suggestions',
                    'title': f'Googleæœç´¢å»ºè®®: {suggestion}',
                    'score': 0,
                    'comments': 0,
                    'url': f'https://www.google.com/search?q={quote(suggestion)}',
                    'platform': 'google'
                })
        
        except Exception as e:
            print(f"âŒ Googleæœç´¢å»ºè®®åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []
        text = text.lower()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # æ¸…ç†å’Œæ ‡å‡†åŒ–å…³é”®è¯
                keyword = re.sub(r'[^\w\s-]', '', match).strip()
                if len(keyword) > 5 and len(keyword) < 100:
                    keywords.append(keyword)
        
        # æå–å¸¸è§çš„AIå·¥å…·ç›¸å…³è¯æ±‡
        ai_terms = [
            'ai tool', 'ai generator', 'ai writer', 'ai assistant', 'ai chatbot',
            'machine learning', 'deep learning', 'neural network', 'gpt',
            'artificial intelligence', 'automation', 'nlp', 'computer vision'
        ]
        
        for term in ai_terms:
            if term in text:
                keywords.append(term)
        
        return list(set(keywords))  # å»é‡
    
    def discover_all_platforms(self, search_terms: List[str]) -> pd.DataFrame:
        """ä»æ‰€æœ‰å¹³å°å‘ç°å…³é”®è¯"""
        print("ğŸš€ å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
        
        all_keywords = []
        
        # Redditåˆ†æ
        for subreddit in self.ai_subreddits[:5]:  # é™åˆ¶æ•°é‡é¿å…è¯·æ±‚è¿‡å¤š
            reddit_keywords = self.discover_reddit_keywords(subreddit, limit=50)
            all_keywords.extend(reddit_keywords)
        
        # Hacker Newsåˆ†æ
        for term in search_terms:
            hn_keywords = self.discover_hackernews_keywords(term)
            all_keywords.extend(hn_keywords)
        
        # YouTubeæœç´¢å»ºè®®
        for term in search_terms:
            youtube_keywords = self.discover_youtube_keywords(term)
            all_keywords.extend(youtube_keywords)
        
        # Googleæœç´¢å»ºè®®
        for term in search_terms:
            google_keywords = self.discover_google_suggestions(term)
            all_keywords.extend(google_keywords)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_keywords)
        
        if not df.empty:
            # å»é‡å’Œæ’åº
            df = df.drop_duplicates(subset=['keyword'])
            df = df.sort_values('score', ascending=False)
            
            # æ·»åŠ å‘ç°æ—¶é—´
            df['discovered_at'] = datetime.now().isoformat()
            
            print(f"âœ… å‘ç° {len(df)} ä¸ªå…³é”®è¯")
        else:
            print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯")
        
        return df
    
    def analyze_keyword_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        if df.empty:
            return {}
        
        analysis = {
            'total_keywords': len(df),
            'platform_distribution': df['platform'].value_counts().to_dict(),
            'top_keywords_by_score': df.nlargest(10, 'score')[['keyword', 'score', 'platform']].to_dict('records'),
            'keyword_length_stats': {
                'avg_length': df['keyword'].str.len().mean(),
                'min_length': df['keyword'].str.len().min(),
                'max_length': df['keyword'].str.len().max()
            },
            'common_terms': self._get_common_terms(df['keyword'].tolist())
        }
        
        return analysis
    
    def _get_common_terms(self, keywords: List[str]) -> Dict[str, int]:
        """è·å–å¸¸è§è¯æ±‡"""
        all_words = []
        for keyword in keywords:
            words = keyword.lower().split()
            all_words.extend([word for word in words if len(word) > 2])
        
        return dict(Counter(all_words).most_common(20))
    
    def save_results(self, df: pd.DataFrame, analysis: Dict, output_dir: str = 'data/multi_platform_keywords'):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜å…³é”®è¯æ•°æ®
        csv_path = os.path.join(output_dir, f'multi_platform_keywords_{timestamp}.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜åˆ†ææŠ¥å‘Šï¼ˆå¤„ç†numpyç±»å‹åºåˆ—åŒ–é—®é¢˜ï¼‰
        json_path = os.path.join(output_dir, f'keyword_analysis_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                else:
                    return obj
            
            serializable_analysis = convert_numpy_types(analysis)
            json.dump(serializable_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜:")
        print(f"   å…³é”®è¯æ•°æ®: {csv_path}")
        print(f"   åˆ†ææŠ¥å‘Š: {json_path}")
        
        return csv_path, json_path


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–å‘ç°å·¥å…·
    discoverer = MultiPlatformKeywordDiscovery()
    
    # æœç´¢è¯æ±‡
    search_terms = [
        'AI tool', 'AI generator', 'AI writer', 'AI assistant',
        'machine learning', 'chatbot', 'automation'
    ]
    
    print("ğŸ” å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·")
    print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
    print("-" * 50)
    
    # å‘ç°å…³é”®è¯
    df = discoverer.discover_all_platforms(search_terms)
    
    if not df.empty:
        # åˆ†æè¶‹åŠ¿
        analysis = discoverer.analyze_keyword_trends(df)
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print("\nğŸ“ˆ å‘ç°ç»“æœæ‘˜è¦:")
        print(f"æ€»å…³é”®è¯æ•°: {analysis['total_keywords']}")
        print(f"å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
        print(f"å¹³å‡å…³é”®è¯é•¿åº¦: {analysis['keyword_length_stats']['avg_length']:.1f} å­—ç¬¦")
        
        print("\nğŸ† çƒ­é—¨å…³é”®è¯ (æŒ‰è¯„åˆ†æ’åº):")
        for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
            print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
        
        print("\nğŸ”¤ å¸¸è§è¯æ±‡:")
        for word, count in list(analysis['common_terms'].items())[:10]:
            print(f"  {word}: {count}æ¬¡")
        
        # ä¿å­˜ç»“æœ
        discoverer.save_results(df, analysis)
    
    else:
        print("âŒ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")


if __name__ == "__main__":
    main()