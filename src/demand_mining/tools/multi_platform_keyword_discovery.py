#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·
æ•´åˆRedditã€Hacker Newsã€Product Huntç­‰å¹³å°çš„å…³é”®è¯å‘ç°
"""

import requests
import json
import time
import re
from datetime import datetime
from typing import Dict, List, Any
from urllib.parse import quote
import pandas as pd
from collections import Counter
import sys
import os

# æ·»åŠ configç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'config'))
from config.config_manager import get_config

class MultiPlatformKeywordDiscovery:
    """å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        # åŠ è½½é…ç½®
        self.config = get_config()
        
        # å¹³å°é…ç½®
        self.platforms = {
            'reddit': {
                'base_url': 'https://www.reddit.com',
                'api_url': 'https://www.reddit.com/r/{}/search.json',
                'enabled': True
            },
            'hackernews': {
                'base_url': 'https://hacker-news.firebaseio.com/v0',
                'search_url': 'https://hn.algolia.com/api/v1/search',
                'enabled': True
            },
            'producthunt': {
                'base_url': 'https://api.producthunt.com/v2/api/graphql',
                'enabled': bool(self.config.PRODUCTHUNT_API_TOKEN)
            }
        }
        
        # è®¾ç½®ProductHuntè®¤è¯å¤´
        if self.config.PRODUCTHUNT_API_TOKEN:
            self.ph_headers = {
                'Authorization': f'Bearer {self.config.PRODUCTHUNT_API_TOKEN}',
                'Content-Type': 'application/json',
                'Accept': 'application/json'
            }
        
        # AIç›¸å…³subredditåˆ—è¡¨
        self.ai_subreddits = [
            'artificial', 'MachineLearning', 'deeplearning', 'ChatGPT',
            'OpenAI', 'artificial_intelligence', 'singularity', 'Futurology',
            'programming', 'webdev', 'entrepreneur', 'SaaS', 'startups'
        ]
        
        # å…³é”®è¯æå–æ¨¡å¼
        self.keyword_patterns = [
            r'\b(?:how to|what is|best|top|vs|compare|review|tutorial|guide)\b[^.!?]*',
            r'\b(?:AI|artificial intelligence|machine learning|deep learning|neural network)\b[^.!?]*',
            r'\b(?:tool|software|app|platform|service|solution)\b[^.!?]*',
            r'\b(?:free|open source|alternative|competitor)\b[^.!?]*'
        ]
    
    def discover_reddit_keywords(self, subreddit: str, limit: int = 100) -> List[Dict]:
        """ä»Redditå‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Reddit r/{subreddit}...")
        
        keywords = []
        
        try:
            # è·å–çƒ­é—¨å¸–å­
            url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
            response = self.session.get(url)
            response.raise_for_status()
            
            data = response.json()
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                selftext = post_data.get('selftext', '')
                score = post_data.get('score', 0)
                num_comments = post_data.get('num_comments', 0)
                
                # æå–å…³é”®è¯
                extracted_keywords = self._extract_keywords_from_text(title + ' ' + selftext)
                
                for keyword in extracted_keywords:
                    keywords.append({
                        'keyword': keyword,
                        'source': f'reddit_r_{subreddit}',
                        'title': title,
                        'score': score,
                        'comments': num_comments,
                        'url': f"https://reddit.com{post_data.get('permalink', '')}",
                        'platform': 'reddit'
                    })
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"âŒ Reddit r/{subreddit} åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_hackernews_keywords(self, query: str = 'AI', days: int = 30) -> List[Dict]:
        """ä»Hacker Newså‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Hacker News (æŸ¥è¯¢: {query})...")
        
        keywords = []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = int(time.time())
            start_time = end_time - (days * 24 * 3600)
            
            # æœç´¢ç›¸å…³å¸–å­
            url = "https://hn.algolia.com/api/v1/search"
            params = {
                'query': query,
                'tags': 'story',
                'numericFilters': f'created_at_i>{start_time},created_at_i<{end_time}',
                'hitsPerPage': 100
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            hits = data.get('hits', [])
            
            for hit in hits:
                title = hit.get('title', '')
                url = hit.get('url', '')
                points = hit.get('points', 0)
                num_comments = hit.get('num_comments', 0)
                
                # æå–å…³é”®è¯
                extracted_keywords = self._extract_keywords_from_text(title)
                
                for keyword in extracted_keywords:
                    keywords.append({
                        'keyword': keyword,
                        'source': 'hackernews',
                        'title': title,
                        'score': points,
                        'comments': num_comments,
                        'url': url or f"https://news.ycombinator.com/item?id={hit.get('objectID')}",
                        'platform': 'hackernews'
                    })
        
        except Exception as e:
            print(f"âŒ Hacker News åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_youtube_keywords(self, search_term: str) -> List[Dict]:
        """ä»YouTubeæœç´¢å»ºè®®å‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ YouTube æœç´¢å»ºè®® (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        try:
            # YouTubeæœç´¢å»ºè®®API (éå®˜æ–¹)
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'youtube',
                'ds': 'yt',
                'q': search_term
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            # è§£æå“åº” (JSONPæ ¼å¼)
            content = response.text
            if content.startswith('window.google.ac.h('):
                json_str = content[19:-1]  # ç§»é™¤JSONPåŒ…è£…
                data = json.loads(json_str)
                
                suggestions = data[1] if len(data) > 1 else []
                
                for suggestion in suggestions:
                    if isinstance(suggestion, list) and len(suggestion) > 0:
                        keyword = suggestion[0]
                        keywords.append({
                            'keyword': keyword,
                            'source': 'youtube_suggestions',
                            'title': f'YouTubeæœç´¢å»ºè®®: {keyword}',
                            'score': 0,
                            'comments': 0,
                            'url': f'https://www.youtube.com/results?search_query={quote(keyword)}',
                            'platform': 'youtube'
                        })
        
        except Exception as e:
            print(f"âŒ YouTube åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_google_suggestions(self, search_term: str) -> List[Dict]:
        """ä»Googleæœç´¢å»ºè®®å‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ Google æœç´¢å»ºè®® (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        try:
            # Googleæœç´¢å»ºè®®API
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'firefox',
                'q': search_term
            }
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            suggestions = data[1] if len(data) > 1 else []
            
            for suggestion in suggestions:
                keywords.append({
                    'keyword': suggestion,
                    'source': 'google_suggestions',
                    'title': f'Googleæœç´¢å»ºè®®: {suggestion}',
                    'score': 0,
                    'comments': 0,
                    'url': f'https://www.google.com/search?q={quote(suggestion)}',
                    'platform': 'google'
                })
        
        except Exception as e:
            print(f"âŒ Googleæœç´¢å»ºè®®åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def discover_producthunt_keywords(self, search_term: str = "AI", days: int = 30) -> List[Dict]:
        """ä»ProductHuntå‘ç°å…³é”®è¯"""
        print(f"ğŸ” æ­£åœ¨åˆ†æ ProductHunt (æŸ¥è¯¢: {search_term})...")
        
        keywords = []
        
        if not self.config.PRODUCTHUNT_API_TOKEN:
            print("âš ï¸ ProductHunt API Tokenæœªé…ç½®ï¼Œè·³è¿‡ProductHuntåˆ†æ")
            return keywords
        
        try:
            # ProductHunt GraphQLæŸ¥è¯¢
            query = """
            query($search: String!, $first: Int!) {
              posts(search: $search, first: $first, order: VOTES) {
                edges {
                  node {
                    id
                    name
                    tagline
                    description
                    votesCount
                    commentsCount
                    url
                    topics {
                      edges {
                        node {
                          name
                        }
                      }
                    }
                  }
                }
              }
            }
            """
            
            variables = {
                "search": search_term,
                "first": 50
            }
            
            response = self.session.post(
                self.platforms['producthunt']['base_url'],
                headers=self.ph_headers,
                json={
                    'query': query,
                    'variables': variables
                }
            )
            response.raise_for_status()
            
            data = response.json()
            
            if 'data' in data and 'posts' in data['data']:
                posts = data['data']['posts']['edges']
                
                for post in posts:
                    node = post['node']
                    name = node.get('name', '')
                    tagline = node.get('tagline', '')
                    description = node.get('description', '')
                    votes = node.get('votesCount', 0)
                    comments = node.get('commentsCount', 0)
                    url = node.get('url', '')
                    
                    # ä»äº§å“åç§°ã€æ ‡è¯­å’Œæè¿°ä¸­æå–å…³é”®è¯
                    text_content = f"{name} {tagline} {description}"
                    extracted_keywords = self._extract_keywords_from_text(text_content)
                    
                    # æ·»åŠ ä¸»é¢˜æ ‡ç­¾ä½œä¸ºå…³é”®è¯
                    topics = node.get('topics', {}).get('edges', [])
                    for topic in topics:
                        topic_name = topic.get('node', {}).get('name', '')
                        if topic_name:
                            extracted_keywords.append(topic_name.lower())
                    
                    for keyword in extracted_keywords:
                        keywords.append({
                            'keyword': keyword,
                            'source': 'producthunt',
                            'title': name,
                            'score': votes,
                            'comments': comments,
                            'url': url,
                            'platform': 'producthunt'
                        })
        
        except Exception as e:
            print(f"âŒ ProductHunt åˆ†æå¤±è´¥: {e}")
        
        return keywords
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []
        text = text.lower()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®è¯
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # æ¸…ç†å’Œæ ‡å‡†åŒ–å…³é”®è¯
                keyword = re.sub(r'[^\w\s-]', '', match).strip()
                # ä¼˜å…ˆé•¿å°¾å…³é”®è¯ï¼š3ä¸ªä»¥ä¸Šè¯æ±‡ä¸”15ä¸ªå­—ç¬¦ä»¥ä¸Š
                if self._is_long_tail_keyword(keyword):
                    keywords.append(keyword)
                elif len(keyword) > 10 and len(keyword) < 100 and len(keyword.split()) >= 2:
                    # æ¬¡ä¼˜é€‰æ‹©ï¼šä¸­ç­‰é•¿åº¦çš„å…³é”®è¯
                    keywords.append(keyword)
        
        # æå–å¸¸è§çš„AIå·¥å…·ç›¸å…³è¯æ±‡
        ai_terms = [
            'ai tool', 'ai generator', 'ai writer', 'ai assistant', 'ai chatbot',
            'machine learning', 'deep learning', 'neural network', 'gpt',
            'artificial intelligence', 'automation', 'nlp', 'computer vision'
        ]
        
        for term in ai_terms:
            if term in text:
                keywords.append(term)
        
        return list(set(keywords))  # å»é‡
    
    def _is_long_tail_keyword(self, keyword: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé•¿å°¾å…³é”®è¯
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            æ˜¯å¦ä¸ºé•¿å°¾å…³é”®è¯
        """
        words = keyword.split()
        # é•¿å°¾è¯æ ‡å‡†ï¼š3ä¸ªä»¥ä¸Šè¯æ±‡ä¸”æ€»é•¿åº¦15ä¸ªå­—ç¬¦ä»¥ä¸Š
        return len(words) >= 3 and len(keyword) >= 15
    
    def _calculate_long_tail_score(self, keyword: str) -> float:
        """
        è®¡ç®—é•¿å°¾è¯è¯„åˆ†åŠ æƒ
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            è¯„åˆ†å€æ•°
        """
        word_count = len(keyword.split())
        keyword_lower = keyword.lower()
        
        base_score = 1.0
        
        # åŸºäºè¯æ•°çš„è¯„åˆ†åŠ æƒ
        if word_count >= 5:
            base_score *= 3.0  # 5+è¯æ±‡è·å¾—æœ€é«˜åŠ æƒ
        elif word_count >= 4:
            base_score *= 2.5  # 4è¯æ±‡è·å¾—é«˜åŠ æƒ
        elif word_count >= 3:
            base_score *= 2.0  # 3è¯æ±‡è·å¾—ä¸­ç­‰åŠ æƒ
        
        # åŸºäºæ„å›¾æ˜ç¡®æ€§çš„åŠ æƒ
        high_intent_phrases = ['how to', 'step by step', 'tutorial', 'guide', 'without', 'for beginners']
        if any(phrase in keyword_lower for phrase in high_intent_phrases):
            base_score *= 1.5
        
        # åŸºäºç«äº‰åº¦çš„è°ƒæ•´
        high_competition_words = ['best', 'top', 'review', 'vs', 'comparison']
        if any(comp in keyword_lower for comp in high_competition_words):
            base_score *= 0.6  # é«˜ç«äº‰è¯é™ä½è¯„åˆ†
        
        return base_score
    
    def discover_all_platforms(self, search_terms: List[str]) -> pd.DataFrame:
        """ä»æ‰€æœ‰å¹³å°å‘ç°å…³é”®è¯"""
        print("ğŸš€ å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
        
        all_keywords = []
        
        # Redditåˆ†æ
        for subreddit in self.ai_subreddits[:5]:  # é™åˆ¶æ•°é‡é¿å…è¯·æ±‚è¿‡å¤š
            reddit_keywords = self.discover_reddit_keywords(subreddit, limit=50)
            all_keywords.extend(reddit_keywords)
        
        # Hacker Newsåˆ†æ
        for term in search_terms:
            hn_keywords = self.discover_hackernews_keywords(term)
            all_keywords.extend(hn_keywords)
        
        # YouTubeæœç´¢å»ºè®®
        for term in search_terms:
            youtube_keywords = self.discover_youtube_keywords(term)
            all_keywords.extend(youtube_keywords)
        
        # Googleæœç´¢å»ºè®®
        # Googleæœç´¢å»ºè®®
        for term in search_terms:
            google_keywords = self.discover_google_suggestions(term)
            all_keywords.extend(google_keywords)
        
        # ProductHuntåˆ†æ
        if self.platforms['producthunt']['enabled']:
            for term in search_terms:
                ph_keywords = self.discover_producthunt_keywords(term)
                all_keywords.extend(ph_keywords)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_keywords)
        
        if not df.empty:
            # å»é‡
            df = df.drop_duplicates(subset=['keyword'])
            
            # æ·»åŠ é•¿å°¾è¯è¯„åˆ†åŠ æƒ
            df['long_tail_score'] = df['keyword'].apply(self._calculate_long_tail_score)
            df['weighted_score'] = df['score'] * df['long_tail_score']
            
            # æŒ‰åŠ æƒè¯„åˆ†æ’åºï¼ˆé•¿å°¾è¯ä¼˜å…ˆï¼‰
            df = df.sort_values('weighted_score', ascending=False)
            
            # æ·»åŠ å‘ç°æ—¶é—´
            df['discovered_at'] = datetime.now().isoformat()
            
            print(f"âœ… å‘ç° {len(df)} ä¸ªå…³é”®è¯")
        else:
            print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯")
        
        return df
    
    def analyze_keyword_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        if df.empty:
            return {}
        
        analysis = {
            'total_keywords': len(df),
            'platform_distribution': df['platform'].value_counts().to_dict(),
            'top_keywords_by_score': df.nlargest(10, 'score')[['keyword', 'score', 'platform']].to_dict('records'),
            'keyword_length_stats': {
                'avg_length': df['keyword'].str.len().mean(),
                'min_length': df['keyword'].str.len().min(),
                'max_length': df['keyword'].str.len().max()
            },
            'common_terms': self._get_common_terms(df['keyword'].tolist())
        }
        
        return analysis
    
    def _get_common_terms(self, keywords: List[str]) -> Dict[str, int]:
        """è·å–å¸¸è§è¯æ±‡"""
        all_words = []
        for keyword in keywords:
            words = keyword.lower().split()
            all_words.extend([word for word in words if len(word) > 2])
        
        return dict(Counter(all_words).most_common(20))
    
    def save_results(self, df: pd.DataFrame, analysis: Dict, output_dir: str = 'data/multi_platform_keywords'):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜å…³é”®è¯æ•°æ®
        csv_path = os.path.join(output_dir, f'multi_platform_keywords_{timestamp}.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜åˆ†ææŠ¥å‘Šï¼ˆå¤„ç†numpyç±»å‹åºåˆ—åŒ–é—®é¢˜ï¼‰
        json_path = os.path.join(output_dir, f'keyword_analysis_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                else:
                    return obj
            
            serializable_analysis = convert_numpy_types(analysis)
            json.dump(serializable_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜:")
        print(f"   å…³é”®è¯æ•°æ®: {csv_path}")
        print(f"   åˆ†ææŠ¥å‘Š: {json_path}")
        
        return csv_path, json_path


def run_discovery(input_keywords=None, limit=10, output_dir=None, verbose=True):
    """
    è¿è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°
    
    å‚æ•°:
        input_keywords: è¾“å…¥å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯
        limit: é™åˆ¶ä½¿ç”¨çš„å…³é”®è¯æ•°é‡
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    è¿”å›:
        tuple: (å…³é”®è¯DataFrame, åˆ†æç»“æœå­—å…¸, è¾“å‡ºCSVè·¯å¾„, è¾“å‡ºJSONè·¯å¾„)
    """
    # åˆå§‹åŒ–å‘ç°å·¥å…·
    discoverer = MultiPlatformKeywordDiscovery()
    
    # è·å–åˆå§‹å…³é”®è¯
    if input_keywords is None or len(input_keywords) == 0:
        # ä½¿ç”¨é»˜è®¤å…³é”®è¯
        search_terms = [
            'AI tool', 'AI generator', 'AI writer', 'AI assistant',
            'machine learning', 'chatbot', 'automation'
        ]
        if verbose:
            print(f"â„¹ï¸ ä½¿ç”¨é»˜è®¤å…³é”®è¯: {', '.join(search_terms)}")
    else:
        search_terms = input_keywords
        if verbose:
            print(f"âœ… ä½¿ç”¨æä¾›çš„ {len(search_terms)} ä¸ªå…³é”®è¯")
    
    # é™åˆ¶å…³é”®è¯æ•°é‡
    if limit and len(search_terms) > limit:
        search_terms = search_terms[:limit]
        if verbose:
            print(f"â„¹ï¸ é™åˆ¶ä½¿ç”¨å‰ {limit} ä¸ªå…³é”®è¯")
    
    if verbose:
        print("ğŸ” å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·")
        print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
        print("-" * 50)
    
    # å‘ç°å…³é”®è¯
    df = discoverer.discover_all_platforms(search_terms)
    
    if not df.empty:
        # åˆ†æè¶‹åŠ¿
        analysis = discoverer.analyze_keyword_trends(df)
        
        if verbose:
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print("\nğŸ“ˆ å‘ç°ç»“æœæ‘˜è¦:")
            print(f"æ€»å…³é”®è¯æ•°: {analysis['total_keywords']}")
            print(f"å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
            print(f"å¹³å‡å…³é”®è¯é•¿åº¦: {analysis['keyword_length_stats']['avg_length']:.1f} å­—ç¬¦")
            
            print("\nğŸ† çƒ­é—¨å…³é”®è¯ (æŒ‰è¯„åˆ†æ’åº):")
            for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            
            print("\nğŸ”¤ å¸¸è§è¯æ±‡:")
            for word, count in list(analysis['common_terms'].items())[:10]:
                print(f"  {word}: {count}æ¬¡")
        
        # ä¿å­˜ç»“æœ
        csv_path, json_path = discoverer.save_results(df, analysis, output_dir=output_dir)
        
        return df, analysis, csv_path, json_path
    else:
        if verbose:
            print("âŒ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
        return pd.DataFrame(), {}, None, None


def main():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="å¤šå¹³å°å…³é”®è¯å‘ç°å·¥å…·")
    parser.add_argument("--input", "-i", help="è¾“å…¥å…³é”®è¯æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼ï¼ŒåŒ…å«keywordåˆ—)")
    parser.add_argument("--keywords", "-k", help="ç›´æ¥æŒ‡å®šå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”")
    parser.add_argument("--use-root-words", "-r", action="store_true", help="ä½¿ç”¨è¯æ ¹è¶‹åŠ¿æ•°æ®")
    parser.add_argument("--limit", "-l", type=int, default=10, help="æ¯ä¸ªæ¥æºä½¿ç”¨çš„å…³é”®è¯æ•°é‡é™åˆ¶")
    parser.add_argument("--output-dir", "-o", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    # è·å–åˆå§‹å…³é”®è¯
    search_terms = []
    
    if args.input:
        # ä»CSVæ–‡ä»¶è¯»å–å…³é”®è¯
        try:
            import pandas as pd
            df_input = pd.read_csv(args.input)
            if 'keyword' in df_input.columns:
                search_terms = df_input['keyword'].tolist()
                print(f"âœ… ä»æ–‡ä»¶ {args.input} è¯»å–äº† {len(search_terms)} ä¸ªå…³é”®è¯")
            else:
                print("âŒ è¾“å…¥æ–‡ä»¶å¿…é¡»åŒ…å«'keyword'åˆ—")
                return
        except Exception as e:
            print(f"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return
    
    elif args.keywords:
        # ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„å…³é”®è¯
        search_terms = [k.strip() for k in args.keywords.split(',')]
        print(f"âœ… ä½¿ç”¨æŒ‡å®šçš„ {len(search_terms)} ä¸ªå…³é”®è¯")
    
    elif args.use_root_words:
        # ä½¿ç”¨è¯æ ¹ç›¸å…³å…³é”®è¯
        try:
            # å°è¯•ä»è¯æ ¹è¶‹åŠ¿æ•°æ®ç›®å½•è¯»å–
            root_words_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'data', 'root_word_trends')
            if os.path.exists(root_words_dir):
                # æŸ¥æ‰¾æœ€æ–°çš„è¯æ ¹è¶‹åŠ¿æ–‡ä»¶
                files = [f for f in os.listdir(root_words_dir) if f.endswith('.csv')]
                if files:
                    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°æ–‡ä»¶
                    latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(root_words_dir, f)))
                    file_path = os.path.join(root_words_dir, latest_file)
                    
                    # è¯»å–è¯æ ¹è¶‹åŠ¿æ•°æ®
                    import pandas as pd
                    df_roots = pd.read_csv(file_path)
                    if 'keyword' in df_roots.columns:
                        # è·å–å…³é”®è¯
                        search_terms = df_roots['keyword'].head(args.limit).tolist()
                        print(f"âœ… ä»è¯æ ¹è¶‹åŠ¿æ–‡ä»¶ {latest_file} è¯»å–äº† {len(search_terms)} ä¸ªå…³é”®è¯")
                    else:
                        raise ValueError("è¯æ ¹è¶‹åŠ¿æ–‡ä»¶ç¼ºå°‘'keyword'åˆ—")
                else:
                    raise FileNotFoundError("æœªæ‰¾åˆ°è¯æ ¹è¶‹åŠ¿CSVæ–‡ä»¶")
            else:
                raise FileNotFoundError(f"è¯æ ¹è¶‹åŠ¿ç›®å½•ä¸å­˜åœ¨: {root_words_dir}")
                
        except Exception as e:
            print(f"âŒ è¯»å–è¯æ ¹è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å…³é”®è¯ä½œä¸ºå¤‡é€‰
            search_terms = [
                'AI tool', 'AI generator', 'AI writer', 'AI assistant',
                'machine learning', 'chatbot', 'automation'
            ]
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤å…³é”®è¯: {', '.join(search_terms)}")
    
    # è¿è¡Œå‘ç°è¿‡ç¨‹
    run_discovery(
        input_keywords=search_terms,
        limit=args.limit,
        output_dir=args.output_dir,
        verbose=True
    )


if __name__ == "__main__":
    main()
    
# ç¤ºä¾‹ç”¨æ³•:
# 1. å‘½ä»¤è¡Œä½¿ç”¨:
#    - ä½¿ç”¨é»˜è®¤å…³é”®è¯: python multi_platform_keyword_discovery.py
#    - æŒ‡å®šå…³é”®è¯: python multi_platform_keyword_discovery.py --keywords "AI tools,machine learning,data science"
#    - ä»æ–‡ä»¶è¯»å–: python multi_platform_keyword_discovery.py --input path/to/keywords.csv
#    - ä½¿ç”¨è¯æ ¹: python multi_platform_keyword_discovery.py --use-root-words --limit 20
#
# 2. ä½œä¸ºæ¨¡å—å¯¼å…¥:
#    from demand_mining.tools.multi_platform_keyword_discovery import run_discovery
#    
#    # ä½¿ç”¨ä¸»æµç¨‹ä¸­è·å–çš„å…³é”®è¯
#    keywords = ["ai writing", "machine learning", "data science"]
#    df, analysis, csv_path, json_path = run_discovery(
#        input_keywords=keywords,
#        limit=10,
#        output_dir="output/multi_platform_keywords",
#        verbose=True
#    )
#    
#    # ä½¿ç”¨ç»“æœè¿›è¡Œåç»­å¤„ç†
#    top_keywords = df.head(20)
