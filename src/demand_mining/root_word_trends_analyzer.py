"""
è¯æ ¹è¶‹åŠ¿åˆ†æå™¨
ç”¨äºåˆ†æ51ä¸ªè¯æ ¹å¯¹åº”å…³é”®è¯çš„Google Trendsè¶‹åŠ¿æ•°æ®
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import time

from ..utils.logger import setup_logger
from ..utils.file_utils import ensure_directory_exists

class RootWordTrendsAnalyzer:
    """è¯æ ¹è¶‹åŠ¿åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "data/root_word_trends"):
        self.output_dir = output_dir
        ensure_directory_exists(self.output_dir)
        self.logger = setup_logger(__name__)
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—è¿½è¸ªå®ä¾‹åˆ›å»º
        import threading
        thread_id = threading.current_thread().ident
        self.logger.info(f"ğŸ”§ RootWordTrendsAnalyzerå®ä¾‹åˆ›å»º - çº¿ç¨‹ID: {thread_id}")
        
        # ä½¿ç”¨ç»å¯¹å¯¼å…¥è·¯å¾„ç¡®ä¿å•ä¾‹ä¸€è‡´æ€§
        from src.collectors.trends_singleton import get_trends_collector
        self.logger.info(f"ğŸ“ è°ƒç”¨get_trends_collector() - çº¿ç¨‹ID: {thread_id}")
        self.trends_collector = get_trends_collector()
        self.logger.info(f"âœ… trends_collectorè·å–å®Œæˆ - çº¿ç¨‹ID: {thread_id}")
        
        # åˆå§‹åŒ–æ–°è¯æ£€æµ‹å™¨ - ä½¿ç”¨å•ä¾‹æ¨¡å¼
        try:
            from .analyzers.new_word_detector_singleton import get_new_word_detector
            self.new_word_detector = get_new_word_detector()
            self.new_word_detection_available = True
            self.logger.info("æ–°è¯æ£€æµ‹å™¨å•ä¾‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.new_word_detector = None
            self.new_word_detection_available = False
            self.logger.warning(f"æ–°è¯æ£€æµ‹å™¨å•ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # 51ä¸ªè¯æ ¹åˆ—è¡¨
        self.root_words = [
            "AI", "artificial intelligence", "machine learning", "deep learning", "neural network",
            "chatbot", "GPT", "OpenAI", "ChatGPT", "language model",
            "computer vision", "natural language processing", "NLP", "automation", "robot",
            "algorithm", "data science", "big data", "analytics", "prediction",
            "classification", "clustering", "regression", "supervised learning", "unsupervised learning",
            "reinforcement learning", "transfer learning", "generative AI", "LLM", "transformer",
            "BERT", "attention mechanism", "encoder", "decoder", "embedding",
            "tokenization", "fine-tuning", "prompt engineering", "few-shot learning", "zero-shot learning",
            "multimodal", "text-to-image", "image generation", "voice synthesis", "speech recognition",
            "recommendation system", "personalization", "content generation", "code generation", "AI assistant"
        ]
    
    def analyze_single_root_word(self, root_word: str, timeframe: str = None) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªè¯æ ¹çš„è¶‹åŠ¿
        
        å‚æ•°:
            root_word: è¦åˆ†æçš„è¯æ ¹
            timeframe: æ—¶é—´èŒƒå›´ï¼Œé»˜è®¤ä½¿ç”¨ç»Ÿä¸€é…ç½®ä¸­çš„å€¼
            
        è¿”å›:
            åŒ…å«è¶‹åŠ¿æ•°æ®çš„å­—å…¸
        """
        if timeframe is None:
            from src.utils.constants import GOOGLE_TRENDS_CONFIG
            timeframe = GOOGLE_TRENDS_CONFIG['default_timeframe'].replace('today ', '')
        try:
            self.logger.info(f"æ­£åœ¨åˆ†æè¯æ ¹: {root_word}")
            
            # è·å–è¶‹åŠ¿æ•°æ® - ä¼ é€’å•ä¸ªå­—ç¬¦ä¸²è€Œä¸æ˜¯åˆ—è¡¨
            trend_data = self.trends_collector.get_keyword_trends(root_word, timeframe=timeframe)
            
            if not trend_data:
                self.logger.warning(f"æœªè·å–åˆ° {root_word} çš„è¶‹åŠ¿æ•°æ®")
                return {"root_word": root_word, "status": "no_data", "data": None}
            
            # å¤„ç†è¶‹åŠ¿æ•°æ®
            processed_data = self._process_trend_data(root_word, trend_data)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶ - å¢åŠ é—´éš”æ—¶é—´é¿å…429é”™è¯¯
            time.sleep(3)
            
            return {
                "root_word": root_word,
                "status": "success",
                "data": processed_data,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"åˆ†æè¯æ ¹ {root_word} æ—¶å‡ºé”™: {str(e)}")
            return {"root_word": root_word, "status": "error", "error": str(e)}
    
    def _process_trend_data(self, root_word: str, trend_data: Dict) -> Dict[str, Any]:
        """å¤„ç†è¶‹åŠ¿æ•°æ®"""
        processed = {
            "keyword": root_word,
            "trend_points": [],
            "average_interest": 0,
            "peak_interest": 0,
            "trend_direction": "stable",
            "related_queries": []
        }
        
        try:
            # å¤„ç†è¶‹åŠ¿æ•°æ® - ä»TrendsCollectorè¿”å›çš„æ•°æ®ç»“æ„
            if isinstance(trend_data, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–ç›¸å…³æŸ¥è¯¢æ•°æ®
                if "related_queries" in trend_data and trend_data["related_queries"]:
                    queries = trend_data["related_queries"]
                    if isinstance(queries, list):
                        # å¤„ç†æŸ¥è¯¢åˆ—è¡¨
                        for query_item in queries[:10]:  # é™åˆ¶å‰10ä¸ª
                            if isinstance(query_item, dict):
                                query_text = query_item.get("query", "")
                                query_value = query_item.get("value", 0)
                                
                                # ä¸ºå…³è”æƒ³è¯æ·»åŠ æ–°è¯æ£€æµ‹
                                new_word_info = self._detect_new_word_for_query(query_text)
                                
                                processed["related_queries"].append({
                                    "query": query_text,
                                    "value": query_value,
                                    "type": "related",
                                    "new_word_detection": new_word_info
                                })
                
                # è®¡ç®—å¹³å‡å…´è¶£åº¦
                if "avg_volume" in trend_data:
                    processed["average_interest"] = float(trend_data["avg_volume"])
                    processed["peak_interest"] = int(processed["average_interest"] * 1.5)  # ä¼°ç®—å³°å€¼
                
                # æ ¹æ®æ•°æ®é‡åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
                total_queries = trend_data.get("total_queries", 0)
                if total_queries > 50:
                    processed["trend_direction"] = "rising"
                elif total_queries > 20:
                    processed["trend_direction"] = "stable"
                else:
                    processed["trend_direction"] = "declining"
            
            elif isinstance(trend_data, pd.DataFrame) and not trend_data.empty:
                # å¦‚æœæ˜¯DataFrameæ ¼å¼ï¼Œç›´æ¥å¤„ç†
                try:
                    # å®‰å…¨åœ°å¤„ç†DataFrameæ•°æ®
                    for idx, row in trend_data.iterrows():
                        try:
                            query_value = row.get("query", "") if hasattr(row, 'get') else str(row.iloc[0]) if len(row) > 0 else ""
                            value_data = row.get("value", 0) if hasattr(row, 'get') else (row.iloc[1] if len(row) > 1 else 0)
                            
                            # ä¸ºå…³è”æƒ³è¯æ·»åŠ æ–°è¯æ£€æµ‹
                            new_word_info = self._detect_new_word_for_query(str(query_value))
                            
                            processed["related_queries"].append({
                                "query": str(query_value),
                                "value": int(value_data) if pd.notna(value_data) else 0,
                                "type": "related",
                                "new_word_detection": new_word_info
                            })
                        except Exception as row_error:
                            self.logger.warning(f"å¤„ç†è¡Œæ•°æ®æ—¶å‡ºé”™: {row_error}")
                            continue
                    
                    # è®¡ç®—ç»Ÿè®¡æ•°æ®
                    if processed["related_queries"]:
                        values = [item["value"] for item in processed["related_queries"]]
                        processed["average_interest"] = sum(values) / len(values) if values else 0
                        processed["peak_interest"] = max(values) if values else 0
                        
                        # ç®€å•çš„è¶‹åŠ¿åˆ¤æ–­
                        if processed["average_interest"] > 30:
                            processed["trend_direction"] = "rising"
                        elif processed["average_interest"] > 10:
                            processed["trend_direction"] = "stable"
                        else:
                            processed["trend_direction"] = "declining"
                
                except Exception as df_error:
                    self.logger.error(f"å¤„ç†DataFrameæ—¶å‡ºé”™: {df_error}")
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®ï¼Œæä¾›é»˜è®¤å€¼
            if not processed["related_queries"]:
                processed["average_interest"] = 10  # é»˜è®¤å…´è¶£åº¦
                processed["peak_interest"] = 15
                processed["trend_direction"] = "stable"
        
        except Exception as e:
            self.logger.error(f"å¤„ç†è¶‹åŠ¿æ•°æ®æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤æ•°æ®ç»“æ„
            processed = {
                "keyword": root_word,
                "trend_points": [],
                "average_interest": 5,
                "peak_interest": 10,
                "trend_direction": "stable",
                "related_queries": []
            }
        
        return processed
    
    def _detect_new_word_for_query(self, query: str) -> Dict[str, Any]:
        """
        ä¸ºå•ä¸ªå…³è”æƒ³è¯è¿›è¡Œæ–°è¯æ£€æµ‹
        
        å‚æ•°:
            query: å…³è”æƒ³è¯
            
        è¿”å›:
            æ–°è¯æ£€æµ‹ç»“æœ
        """
        if not self.new_word_detection_available or not query.strip():
            return {
                "is_new_word": False,
                "new_word_score": 0.0,
                "new_word_grade": "D",
                "confidence_level": "low",
                "growth_rate_7d": 0.0,
                "historical_pattern": "unknown",
                "detection_reasons": "æ–°è¯æ£€æµ‹ä¸å¯ç”¨"
            }
        
        try:
            # åˆ›å»ºä¸´æ—¶DataFrameè¿›è¡Œæ–°è¯æ£€æµ‹
            temp_df = pd.DataFrame({'query': [query]})
            
            # ä½¿ç”¨æ–°è¯æ£€æµ‹å™¨åˆ†æ
            result_df = self.new_word_detector.detect_new_words(temp_df, 'query')
            
            if len(result_df) > 0:
                row = result_df.iloc[0]
                return {
                    "is_new_word": bool(row.get('is_new_word', False)),
                    "new_word_score": float(row.get('new_word_score', 0.0)),
                    "new_word_grade": str(row.get('new_word_grade', 'D')),
                    "confidence_level": str(row.get('confidence_level', 'low')),
                    "growth_rate_7d": float(row.get('growth_rate_7d', 0.0)),
                    "historical_pattern": str(row.get('historical_pattern', 'unknown')),
                    "detection_reasons": str(row.get('detection_reasons', ''))
                }
            else:
                return self._get_default_new_word_result()
                
        except Exception as e:
            self.logger.warning(f"å…³è”æƒ³è¯ '{query}' æ–°è¯æ£€æµ‹å¤±è´¥: {e}")
            return self._get_default_new_word_result()
    
    def _get_default_new_word_result(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ–°è¯æ£€æµ‹ç»“æœ"""
        return {
            "is_new_word": False,
            "new_word_score": 0.0,
            "new_word_grade": "D",
            "confidence_level": "low",
            "growth_rate_7d": 0.0,
            "historical_pattern": "unknown",
            "detection_reasons": "æ£€æµ‹å¤±è´¥"
        }
    
    
    def _generate_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "successful_analyses": 0,
            "failed_analyses": 0,
            "top_trending_words": [],
            "declining_words": [],
            "stable_words": [],
            "average_interest_distribution": {}
        }
        
        successful_results = []
        
        for result in results:
            if result["status"] == "success" and result["data"]:
                summary["successful_analyses"] += 1
                successful_results.append(result)
            else:
                summary["failed_analyses"] += 1
        
        # æŒ‰è¶‹åŠ¿æ–¹å‘åˆ†ç±»
        for result in successful_results:
            data = result["data"]
            root_word = result["root_word"]
            
            if data["trend_direction"] == "rising":
                summary["top_trending_words"].append({
                    "word": root_word,
                    "average_interest": data["average_interest"],
                    "peak_interest": data["peak_interest"]
                })
            elif data["trend_direction"] == "declining":
                summary["declining_words"].append({
                    "word": root_word,
                    "average_interest": data["average_interest"]
                })
            else:
                summary["stable_words"].append({
                    "word": root_word,
                    "average_interest": data["average_interest"]
                })
        
        # æ’åº
        summary["top_trending_words"].sort(key=lambda x: x["average_interest"], reverse=True)
        summary["declining_words"].sort(key=lambda x: x["average_interest"], reverse=True)
        summary["stable_words"].sort(key=lambda x: x["average_interest"], reverse=True)
        
        return summary
    
    def _save_results(self, results: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_path = os.path.join(self.output_dir, f"root_word_trends_full_{timestamp}.json")
        with open(full_results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦
        summary_path = os.path.join(self.output_dir, f"root_word_trends_summary_{timestamp}.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(results["summary"], f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆCSVæŠ¥å‘Š
        self._generate_csv_report(results, timestamp)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def _generate_csv_report(self, results: Dict[str, Any], timestamp: str):
        """ç”ŸæˆCSVæ ¼å¼çš„æŠ¥å‘Š"""
        csv_data = []
        
        for result in results["results"]:
            if result["status"] == "success" and result["data"]:
                data = result["data"]
                
                # ç»Ÿè®¡æ–°è¯ä¿¡æ¯
                new_words_count = 0
                high_confidence_new_words = 0
                total_new_word_score = 0
                
                for query in data["related_queries"]:
                    if "new_word_detection" in query:
                        nwd = query["new_word_detection"]
                        if nwd.get("is_new_word", False):
                            new_words_count += 1
                            total_new_word_score += nwd.get("new_word_score", 0)
                            if nwd.get("confidence_level") == "high":
                                high_confidence_new_words += 1
                
                avg_new_word_score = (total_new_word_score / new_words_count) if new_words_count > 0 else 0
                
                csv_data.append({
                    "è¯æ ¹": result["root_word"],
                    "çŠ¶æ€": "æˆåŠŸ",
                    "å¹³å‡å…´è¶£åº¦": round(data["average_interest"], 2),
                    "å³°å€¼å…´è¶£åº¦": data["peak_interest"],
                    "è¶‹åŠ¿æ–¹å‘": data["trend_direction"],
                    "ç›¸å…³æŸ¥è¯¢æ•°é‡": len(data["related_queries"]),
                    "æ–°è¯æ•°é‡": new_words_count,
                    "é«˜ç½®ä¿¡åº¦æ–°è¯": high_confidence_new_words,
                    "å¹³å‡æ–°è¯åˆ†æ•°": round(avg_new_word_score, 1)
                })
            else:
                csv_data.append({
                    "è¯æ ¹": result["root_word"],
                    "çŠ¶æ€": "å¤±è´¥",
                    "å¹³å‡å…´è¶£åº¦": 0,
                    "å³°å€¼å…´è¶£åº¦": 0,
                    "è¶‹åŠ¿æ–¹å‘": "æœªçŸ¥",
                    "ç›¸å…³æŸ¥è¯¢æ•°é‡": 0,
                    "æ–°è¯æ•°é‡": 0,
                    "é«˜ç½®ä¿¡åº¦æ–°è¯": 0,
                    "å¹³å‡æ–°è¯åˆ†æ•°": 0
                })
        
        df = pd.DataFrame(csv_data)
        csv_path = os.path.join(self.output_dir, f"root_word_trends_report_{timestamp}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        self.logger.info(f"CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {csv_path}")
    
    def get_trending_keywords_for_root(self, root_word: str, limit: int = 20) -> List[str]:
        """
        è·å–ç‰¹å®šè¯æ ¹çš„çƒ­é—¨ç›¸å…³å…³é”®è¯
        
        Args:
            root_word: è¯æ ¹
            limit: è¿”å›å…³é”®è¯æ•°é‡é™åˆ¶
            
        Returns:
            ç›¸å…³å…³é”®è¯åˆ—è¡¨
        """
        try:
            # è·å–ç›¸å…³æŸ¥è¯¢
            trend_data = self.trends_collector.get_keyword_trends([root_word])
            
            keywords = []
            if "related_queries" in trend_data:
                related_data = trend_data["related_queries"]
                if isinstance(related_data, dict):
                    for query_type, queries in related_data.items():
                        if isinstance(queries, pd.DataFrame) and not queries.empty:
                            for _, row in queries.head(limit).iterrows():
                                keywords.append(row["query"])
            
            return keywords[:limit]
            
        except Exception as e:
            self.logger.error(f"è·å– {root_word} ç›¸å…³å…³é”®è¯æ—¶å‡ºé”™: {str(e)}")
            return []

