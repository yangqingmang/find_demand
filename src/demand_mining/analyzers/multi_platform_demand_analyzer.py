#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°éœ€æ±‚åˆ†æå™¨
é€šè¿‡æœç´¢Redditã€GitHubç­‰å¹³å°æ¥éªŒè¯å…³é”®è¯éœ€æ±‚å’Œå‘ç°ç—›ç‚¹
"""

import asyncio
import json
import os
import csv
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
import requests
from urllib.parse import quote
import time
import random

logger = logging.getLogger(__name__)

class MultiPlatformDemandAnalyzer:
    """å¤šå¹³å°éœ€æ±‚åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "output/reports"):
        """
        åˆå§‹åŒ–å¤šå¹³å°éœ€æ±‚åˆ†æå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.platforms = {
            'reddit': self._search_reddit,
            'github': self._search_github,
            'stackoverflow': self._search_stackoverflow
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”¨æˆ·ä»£ç†
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        logger.info("âœ… å¤šå¹³å°éœ€æ±‚åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def analyze_high_opportunity_keywords(
        self, 
        keywords_data: List[Dict], 
        min_opportunity_score: float = 70.0,
        max_keywords: int = 5
    ) -> Dict[str, Any]:
        """
        åˆ†æé«˜æœºä¼šå…³é”®è¯çš„å¤šå¹³å°éœ€æ±‚
        
        Args:
            keywords_data: å…³é”®è¯æ•°æ®åˆ—è¡¨
            min_opportunity_score: æœ€å°æœºä¼šåˆ†æ•°é˜ˆå€¼
            max_keywords: æœ€å¤§åˆ†æå…³é”®è¯æ•°é‡
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” å¼€å§‹å¤šå¹³å°éœ€æ±‚åˆ†æï¼Œé˜ˆå€¼: {min_opportunity_score}")
        
        # ç­›é€‰é«˜æœºä¼šå…³é”®è¯
        high_opportunity_keywords = []
        for kw_data in keywords_data:
            score = kw_data.get('opportunity_score', 0)
            if score >= min_opportunity_score:
                high_opportunity_keywords.append(kw_data)
        
        # é™åˆ¶åˆ†ææ•°é‡
        high_opportunity_keywords = high_opportunity_keywords[:max_keywords]
        
        if not high_opportunity_keywords:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„é«˜æœºä¼šå…³é”®è¯")
            return {
                'analyzed_keywords': 0,
                'platform_results': {},
                'summary': {
                    'total_discussions': 0,
                    'pain_points': [],
                    'opportunities': []
                }
            }
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(high_opportunity_keywords)} ä¸ªé«˜æœºä¼šå…³é”®è¯")
        
        # åˆ†ææ¯ä¸ªå…³é”®è¯
        results = {
            'analyzed_keywords': len(high_opportunity_keywords),
            'platform_results': {},
            'summary': {
                'total_discussions': 0,
                'pain_points': [],
                'opportunities': []
            }
        }
        
        for kw_data in high_opportunity_keywords:
            keyword = kw_data.get('keyword', '')
            logger.info(f"ğŸ” åˆ†æå…³é”®è¯: {keyword}")
            
            keyword_results = await self._analyze_keyword_across_platforms(keyword)
            results['platform_results'][keyword] = keyword_results
            
            # æ›´æ–°æ€»ç»“
            for platform_data in keyword_results.values():
                results['summary']['total_discussions'] += platform_data.get('total_results', 0)
                results['summary']['pain_points'].extend(platform_data.get('pain_points', []))
                results['summary']['opportunities'].extend(platform_data.get('opportunities', []))
        
        # å»é‡å’Œæ’åº
        results['summary']['pain_points'] = list(set(results['summary']['pain_points']))[:10]
        results['summary']['opportunities'] = list(set(results['summary']['opportunities']))[:10]
        
        logger.info(f"âœ… å¤šå¹³å°éœ€æ±‚åˆ†æå®Œæˆï¼Œå…±åˆ†æ {results['analyzed_keywords']} ä¸ªå…³é”®è¯")
        
        return results
    
    async def _analyze_keyword_across_platforms(self, keyword: str) -> Dict[str, Any]:
        """
        è·¨å¹³å°åˆ†æå•ä¸ªå…³é”®è¯
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            å¹³å°åˆ†æç»“æœ
        """
        platform_results = {}
        
        for platform_name, search_func in self.platforms.items():
            try:
                logger.info(f"  ğŸ” æœç´¢ {platform_name}...")
                result = await search_func(keyword)
                platform_results[platform_name] = result
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                await asyncio.sleep(random.uniform(1, 3))
                
            except Exception as e:
                logger.error(f"âŒ {platform_name} æœç´¢å¤±è´¥: {e}")
                platform_results[platform_name] = {
                    'total_results': 0,
                    'discussions': [],
                    'pain_points': [],
                    'opportunities': [],
                    'error': str(e)
                }
        
        return platform_results
    
    async def _search_reddit(self, keyword: str) -> Dict[str, Any]:
        """
        æœç´¢Redditè®¨è®º
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            Redditæœç´¢ç»“æœ
        """
        try:
            # ä½¿ç”¨Redditæœç´¢APIï¼ˆæ— éœ€è®¤è¯ï¼‰
            search_url = f"https://www.reddit.com/search.json"
            params = {
                'q': keyword,
                'sort': 'relevance',
                'limit': 10,
                't': 'month'  # æœ€è¿‘ä¸€ä¸ªæœˆ
            }
            
            response = requests.get(search_url, params=params, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                posts = data.get('data', {}).get('children', [])
                
                discussions = []
                pain_points = []
                opportunities = []
                
                for post in posts[:5]:  # åªåˆ†æå‰5ä¸ªç»“æœ
                    post_data = post.get('data', {})
                    title = post_data.get('title', '')
                    selftext = post_data.get('selftext', '')
                    score = post_data.get('score', 0)
                    num_comments = post_data.get('num_comments', 0)
                    
                    discussion = {
                        'title': title,
                        'content': selftext[:200] + '...' if len(selftext) > 200 else selftext,
                        'score': score,
                        'comments': num_comments,
                        'url': f"https://reddit.com{post_data.get('permalink', '')}"
                    }
                    discussions.append(discussion)
                    
                    # ç®€å•çš„ç—›ç‚¹å’Œæœºä¼šè¯†åˆ«
                    text = (title + ' ' + selftext).lower()
                    
                    # ç—›ç‚¹å…³é”®è¯
                    pain_keywords = ['problem', 'issue', 'bug', 'error', 'difficult', 'hard', 'frustrating', 'slow', 'expensive']
                    for pain_word in pain_keywords:
                        if pain_word in text and len(pain_points) < 5:
                            pain_points.append(f"ç”¨æˆ·åæ˜ {keyword}å­˜åœ¨{pain_word}ç›¸å…³é—®é¢˜")
                    
                    # æœºä¼šå…³é”®è¯
                    opportunity_keywords = ['need', 'want', 'wish', 'hope', 'better', 'improve', 'feature', 'alternative']
                    for opp_word in opportunity_keywords:
                        if opp_word in text and len(opportunities) < 5:
                            opportunities.append(f"ç”¨æˆ·å¸Œæœ›{keyword}èƒ½å¤Ÿ{opp_word}")
                
                return {
                    'total_results': len(posts),
                    'discussions': discussions,
                    'pain_points': pain_points,
                    'opportunities': opportunities
                }
            else:
                logger.warning(f"Reddit APIè¿”å›çŠ¶æ€ç : {response.status_code}")
                return self._empty_result()
                
        except Exception as e:
            logger.error(f"Redditæœç´¢å¼‚å¸¸: {e}")
            return self._empty_result()
    
    async def _search_github(self, keyword: str) -> Dict[str, Any]:
        """
        æœç´¢GitHub Issues
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            GitHubæœç´¢ç»“æœ
        """
        try:
            # ä½¿ç”¨GitHubæœç´¢APIï¼ˆæ— éœ€è®¤è¯ï¼Œä½†æœ‰é™åˆ¶ï¼‰
            search_url = "https://api.github.com/search/issues"
            params = {
                'q': f"{keyword} is:issue",
                'sort': 'updated',
                'per_page': 5
            }
            
            response = requests.get(search_url, params=params, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                issues = data.get('items', [])
                
                discussions = []
                pain_points = []
                opportunities = []
                
                for issue in issues:
                    title = issue.get('title', '')
                    body = issue.get('body', '') or ''
                    state = issue.get('state', '')
                    comments = issue.get('comments', 0)
                    
                    discussion = {
                        'title': title,
                        'content': body[:200] + '...' if len(body) > 200 else body,
                        'state': state,
                        'comments': comments,
                        'url': issue.get('html_url', '')
                    }
                    discussions.append(discussion)
                    
                    # åˆ†æç—›ç‚¹å’Œæœºä¼š
                    text = (title + ' ' + body).lower()
                    
                    if 'bug' in text or 'error' in text:
                        pain_points.append(f"GitHubä¸Šæœ‰å…³äº{keyword}çš„bugæŠ¥å‘Š")
                    
                    if 'feature' in text or 'enhancement' in text:
                        opportunities.append(f"ç”¨æˆ·è¯·æ±‚{keyword}çš„æ–°åŠŸèƒ½")
                
                return {
                    'total_results': data.get('total_count', 0),
                    'discussions': discussions,
                    'pain_points': pain_points,
                    'opportunities': opportunities
                }
            else:
                logger.warning(f"GitHub APIè¿”å›çŠ¶æ€ç : {response.status_code}")
                return self._empty_result()
                
        except Exception as e:
            logger.error(f"GitHubæœç´¢å¼‚å¸¸: {e}")
            return self._empty_result()
    
    async def _search_stackoverflow(self, keyword: str) -> Dict[str, Any]:
        """
        æœç´¢Stack Overflowé—®é¢˜
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            Stack Overflowæœç´¢ç»“æœ
        """
        try:
            # ä½¿ç”¨Stack Overflow API
            search_url = "https://api.stackexchange.com/2.3/search"
            params = {
                'order': 'desc',
                'sort': 'relevance',
                'intitle': keyword,
                'site': 'stackoverflow',
                'pagesize': 5
            }
            
            response = requests.get(search_url, params=params, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                questions = data.get('items', [])
                
                discussions = []
                pain_points = []
                opportunities = []
                
                for question in questions:
                    title = question.get('title', '')
                    tags = question.get('tags', [])
                    score = question.get('score', 0)
                    answer_count = question.get('answer_count', 0)
                    
                    discussion = {
                        'title': title,
                        'tags': tags,
                        'score': score,
                        'answers': answer_count,
                        'url': question.get('link', '')
                    }
                    discussions.append(discussion)
                    
                    # åˆ†ææŠ€æœ¯ç—›ç‚¹
                    if answer_count == 0:
                        pain_points.append(f"{keyword}ç›¸å…³é—®é¢˜ç¼ºå°‘è§£å†³æ–¹æ¡ˆ")
                    
                    if score > 5:
                        opportunities.append(f"{keyword}æ˜¯çƒ­é—¨æŠ€æœ¯è¯é¢˜")
                
                return {
                    'total_results': data.get('total', 0),
                    'discussions': discussions,
                    'pain_points': pain_points,
                    'opportunities': opportunities
                }
            else:
                logger.warning(f"Stack Overflow APIè¿”å›çŠ¶æ€ç : {response.status_code}")
                return self._empty_result()
                
        except Exception as e:
            logger.error(f"Stack Overflowæœç´¢å¼‚å¸¸: {e}")
            return self._empty_result()
    
    def _empty_result(self) -> Dict[str, Any]:
        """è¿”å›ç©ºç»“æœ"""
        return {
            'total_results': 0,
            'discussions': [],
            'pain_points': [],
            'opportunities': []
        }
    
    def save_results(self, results: Dict[str, Any]) -> str:
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            results: åˆ†æç»“æœ
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæ ¼å¼
        json_file = os.path.join(self.output_dir, f"demand_validation_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼çš„æ€»ç»“
        csv_file = os.path.join(self.output_dir, f"demand_validation_summary_{timestamp}.csv")
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥æ ‡é¢˜
            writer.writerow(['ç±»å‹', 'å†…å®¹', 'å…³é”®è¯', 'å¹³å°'])
            
            # å†™å…¥ç—›ç‚¹
            for pain_point in results['summary']['pain_points']:
                writer.writerow(['ç—›ç‚¹', pain_point, '', ''])
            
            # å†™å…¥æœºä¼š
            for opportunity in results['summary']['opportunities']:
                writer.writerow(['æœºä¼š', opportunity, '', ''])
            
            # å†™å…¥è¯¦ç»†è®¨è®º
            for keyword, platform_data in results['platform_results'].items():
                for platform, data in platform_data.items():
                    for discussion in data.get('discussions', []):
                        writer.writerow([
                            'è®¨è®º',
                            discussion.get('title', ''),
                            keyword,
                            platform
                        ])
        
        logger.info(f"ğŸ“ éœ€æ±‚éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        logger.info(f"ğŸ“ éœ€æ±‚éªŒè¯æ€»ç»“å·²ä¿å­˜åˆ°: {csv_file}")
        
        return json_file

# å¼‚æ­¥è¿è¡Œç¤ºä¾‹
async def main():
    """æµ‹è¯•å‡½æ•°"""
    analyzer = MultiPlatformDemandAnalyzer()
    
    # æ¨¡æ‹Ÿå…³é”®è¯æ•°æ®
    test_keywords = [
        {
            'keyword': 'AI code generator',
            'opportunity_score': 85.5,
            'intent': {'intent_description': 'Commercial'}
        }
    ]
    
    results = await analyzer.analyze_high_opportunity_keywords(test_keywords)
    analyzer.save_results(results)

if __name__ == "__main__":
    asyncio.run(main())