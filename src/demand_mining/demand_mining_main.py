#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éœ€æ±‚æŒ–æ˜ä¸å…³é”®è¯åˆ†æä¸»ç¨‹åº
æ•´åˆä¼ ç»Ÿå…³é”®è¯æŒ–æ˜ä¸å‡ºæµ·AIå·¥å…·éœ€æ±‚å‘ç°ç­–ç•¥
åŸºäºè·¯æ¼«æ¼«åˆ†äº«çš„å…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•è¿›è¡Œç³»ç»Ÿæ€§éœ€æ±‚åˆ†æ
"""

import os
import sys
import argparse
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from .analyzers.intent_analyzer_v2 import IntentAnalyzerV2
from .analyzers.market_analyzer import MarketAnalyzer
from .analyzers.keyword_analyzer import KeywordAnalyzer

class DemandMiningManager:
    """
    æ•´åˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
    åŸºäºå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•ï¼š
    1. åŸºäºè¯æ ¹å…³é”®è¯æ‹“å±•
    2. åŸºäºSEOå¤§ç«™æµé‡åˆ†æ  
    3. æœç´¢å¼•æ“ä¸‹æ‹‰æ¨è
    4. å¾ªç¯æŒ–æ˜æ³•
    5. ä»˜è´¹å¹¿å‘Šå…³é”®è¯åˆ†æ
    6. æ”¶å…¥æ’è¡Œæ¦œåˆ†æ
    """
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–éœ€æ±‚æŒ–æ˜ç®¡ç†å™¨"""
        self.config = self._load_config(config_path)
        self.output_dir = "src/demand_mining/reports"
        self._ensure_output_dirs()
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.intent_analyzer = IntentAnalyzerV2()
        self.market_analyzer = MarketAnalyzer()
        self.keyword_analyzer = KeywordAnalyzer()
        
        # åˆå§‹åŒ–è¯æ ¹ç®¡ç†å™¨ï¼ˆæ•´åˆ51ä¸ªç½‘ç»œæ”¶é›†çš„è¯æ ¹ï¼‰
        self.root_manager = RootWordManager(config_path)
        
        # è·å–å½“å‰æ¿€æ´»çš„è¯æ ¹ï¼ˆæ”¯æŒæ‰‹åŠ¨æŒ‡å®šå’Œé»˜è®¤é…ç½®ï¼‰
        self.core_roots = self.root_manager.get_active_roots()
        self.ai_prefixes = self.root_manager.ai_prefixes
        
        # é«˜ä»·å€¼ç«å“ç½‘ç«™ï¼ˆç”¨äºæµé‡åˆ†æï¼‰
        self.competitor_sites = [
            'canva.com', 'midjourney.com', 'openai.com', 'jasper.ai',
            'copy.ai', 'writesonic.com', 'rytr.me', 'jarvis.ai'
        ]
        
        print("ğŸš€ æ•´åˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å·²åŠ è½½ {len(self.core_roots)} ä¸ªæ ¸å¿ƒè¯æ ¹")
        print(f"ğŸ¯ å·²é…ç½® {len(self.competitor_sites)} ä¸ªç«å“åˆ†æç›®æ ‡")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'min_search_volume': 100,
            'max_competition': 0.8,
            'min_confidence': 0.7,
            'output_formats': ['csv', 'json'],
            'data_sources': ['google_trends', 'keyword_planner'],
            'analysis_depth': 'standard'  # basic, standard, deep
        }
        
        if config_path and os.path.exists(config_path):
            import json
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def _ensure_output_dirs(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        dirs = [
            self.output_dir,
            os.path.join(self.output_dir, 'daily_reports'),
            os.path.join(self.output_dir, 'weekly_reports'),
            os.path.join(self.output_dir, 'monthly_reports'),
            os.path.join(self.output_dir, 'keyword_analysis'),
            os.path.join(self.output_dir, 'intent_analysis'),
            os.path.join(self.output_dir, 'market_analysis')
        ]
        
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
    
    def analyze_keywords(self, input_file: str, output_dir: str = None) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯æ–‡ä»¶"""
        print(f"ğŸ“Š å¼€å§‹åˆ†æå…³é”®è¯æ–‡ä»¶: {input_file}")
        
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        
        # è¯»å–æ•°æ®
        if input_file.endswith('.csv'):
            df = pd.read_csv(input_file)
        elif input_file.endswith('.json'):
            df = pd.read_json(input_file)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVæˆ–JSONæ–‡ä»¶")
        
        print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡å…³é”®è¯æ•°æ®")
        
        # åˆ†æç»“æœ
        results = {
            'total_keywords': len(df),
            'analysis_time': datetime.now().isoformat(),
            'keywords': [],
            'intent_summary': {},
            'market_insights': {},
            'recommendations': []
        }
        
        # å…³é”®è¯é€ä¸ªåˆ†æ
        for idx, row in df.iterrows():
            keyword = row.get('query', row.get('keyword', ''))
            if not keyword:
                continue
            
            print(f"ğŸ” åˆ†æå…³é”®è¯ ({idx+1}/{len(df)}): {keyword}")
            
            # æ„å›¾åˆ†æ
            intent_result = self._analyze_keyword_intent(keyword)
            
            # å¸‚åœºåˆ†æ
            market_result = self._analyze_keyword_market(keyword)
            
            # æ•´åˆç»“æœ
            keyword_result = {
                'keyword': keyword,
                'intent': intent_result,
                'market': market_result,
                'opportunity_score': self._calculate_opportunity_score(intent_result, market_result)
            }
            
            results['keywords'].append(keyword_result)
        
        # ç”Ÿæˆæ‘˜è¦
        results['intent_summary'] = self._generate_intent_summary(results['keywords'])
        results['market_insights'] = self._generate_market_insights(results['keywords'])
        results['recommendations'] = self._generate_recommendations(results['keywords'])
        
        # ä¿å­˜ç»“æœ
        output_path = self._save_analysis_results(results, output_dir)
        
        print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return results
    
    def _analyze_keyword_intent(self, keyword: str) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯æ„å›¾"""
        try:
            intent, confidence, secondary = self.intent_analyzer.detect_intent_from_keyword(keyword)
            return {
                'primary_intent': intent,
                'confidence': confidence,
                'secondary_intent': secondary,
                'intent_description': self.intent_analyzer.INTENT_DESCRIPTIONS.get(intent, '')
            }
        except Exception as e:
            print(f"âš ï¸ æ„å›¾åˆ†æå¤±è´¥ ({keyword}): {e}")
            return {
                'primary_intent': 'Unknown',
                'confidence': 0.0,
                'secondary_intent': None,
                'intent_description': 'åˆ†æå¤±è´¥'
            }
    
    def _analyze_keyword_market(self, keyword: str) -> Dict[str, Any]:
        """
        åˆ†æå…³é”®è¯å¸‚åœºæ•°æ®
        æ•´åˆå…­å¤§æŒ–æ˜æ–¹æ³•çš„å¸‚åœºéªŒè¯é€»è¾‘
        """
        try:
            # åŸºç¡€å¸‚åœºæ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…å¯æ¥å…¥çœŸå®APIï¼‰
            base_data = {
                'search_volume': 1000,
                'competition': 0.5,
                'cpc': 2.0,
                'trend': 'stable',
                'seasonality': 'low'
            }
            
            # æ–°å¢ï¼šAIç›¸å…³å…³é”®è¯åŠ åˆ†
            ai_bonus = self._calculate_ai_bonus(keyword)
            
            # æ–°å¢ï¼šè¯æ ¹åŒ¹é…åº¦è¯„åˆ†
            root_match_score = self._calculate_root_match_score(keyword)
            
            # æ–°å¢ï¼šæ–°å…´å…³é”®è¯æ£€æµ‹
            newness_score = self._detect_keyword_newness(keyword)
            
            # æ–°å¢ï¼šå•†ä¸šä»·å€¼è¯„ä¼°
            commercial_value = self._assess_commercial_value(keyword)
            
            # æ•´åˆè¯„åˆ†
            base_data.update({
                'ai_bonus': ai_bonus,
                'root_match_score': root_match_score,
                'newness_score': newness_score,
                'commercial_value': commercial_value,
                'opportunity_indicators': self._get_opportunity_indicators(keyword)
            })
            
            return base_data
            
        except Exception as e:
            print(f"âš ï¸ å¸‚åœºåˆ†æå¤±è´¥ ({keyword}): {e}")
            return {
                'search_volume': 0,
                'competition': 0.0,
                'cpc': 0.0,
                'trend': 'unknown',
                'seasonality': 'unknown',
                'ai_bonus': 0,
                'root_match_score': 0,
                'newness_score': 0,
                'commercial_value': 0,
                'opportunity_indicators': []
            }
    
    def _calculate_ai_bonus(self, keyword: str) -> float:
        """è®¡ç®—AIç›¸å…³å…³é”®è¯åŠ åˆ†"""
        keyword_lower = keyword.lower()
        ai_score = 0
        
        # AIå‰ç¼€åŒ¹é…
        for prefix in self.ai_prefixes:
            if prefix in keyword_lower:
                ai_score += 20
                break
        
        # AIå·¥å…·ç±»å‹åŒ¹é…
        ai_tool_types = ['generator', 'detector', 'writer', 'assistant', 'chatbot']
        for tool_type in ai_tool_types:
            if tool_type in keyword_lower:
                ai_score += 15
                break
        
        # æ–°å…´AIæ¦‚å¿µ
        emerging_concepts = ['gpt', 'llm', 'diffusion', 'transformer', 'neural']
        for concept in emerging_concepts:
            if concept in keyword_lower:
                ai_score += 10
                break
        
        return min(ai_score, 50)  # æœ€é«˜50åˆ†
    
    def _calculate_root_match_score(self, keyword: str) -> float:
        """è®¡ç®—è¯æ ¹åŒ¹é…åº¦è¯„åˆ†"""
        keyword_lower = keyword.lower()
        match_score = 0
        
        for root in self.core_roots:
            if root in keyword_lower:
                match_score += 30
                break
        
        return min(match_score, 30)  # æœ€é«˜30åˆ†
    
    def _detect_keyword_newness(self, keyword: str) -> float:
        """æ£€æµ‹å…³é”®è¯æ–°å…´ç¨‹åº¦"""
        # ç®€åŒ–çš„æ–°å…´å…³é”®è¯æ£€æµ‹é€»è¾‘
        keyword_lower = keyword.lower()
        newness_indicators = [
            'ai', 'gpt', 'chatbot', 'neural', 'machine learning',
            'deep learning', 'transformer', 'diffusion', 'stable diffusion'
        ]
        
        newness_score = 0
        for indicator in newness_indicators:
            if indicator in keyword_lower:
                newness_score += 15
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½ï¼ˆå¦‚2024, 2025ç­‰ï¼Œè¡¨ç¤ºæ–°å…´ï¼‰
        import re
        if re.search(r'202[4-9]', keyword):
            newness_score += 20
        
        return min(newness_score, 40)  # æœ€é«˜40åˆ†
    
    def _assess_commercial_value(self, keyword: str) -> float:
        """è¯„ä¼°å•†ä¸šä»·å€¼"""
        keyword_lower = keyword.lower()
        commercial_score = 0
        
        # é«˜ä»·å€¼å…³é”®è¯ç±»å‹
        high_value_types = [
            'generator', 'converter', 'editor', 'maker', 'creator',
            'optimizer', 'enhancer', 'analyzer', 'detector'
        ]
        
        for value_type in high_value_types:
            if value_type in keyword_lower:
                commercial_score += 25
                break
        
        # ä»˜è´¹æ„æ„¿å¼ºçš„é¢†åŸŸ
        high_payment_domains = [
            'business', 'marketing', 'seo', 'content', 'design',
            'video', 'image', 'writing', 'coding', 'academic'
        ]
        
        for domain in high_payment_domains:
            if domain in keyword_lower:
                commercial_score += 20
                break
        
        return min(commercial_score, 50)  # æœ€é«˜50åˆ†
    
    def _get_opportunity_indicators(self, keyword: str) -> List[str]:
        """è·å–æœºä¼šæŒ‡æ ‡"""
        indicators = []
        keyword_lower = keyword.lower()
        
        # AIç›¸å…³
        if any(prefix in keyword_lower for prefix in self.ai_prefixes):
            indicators.append("AIç›¸å…³éœ€æ±‚")
        
        # å·¥å…·ç±»
        if any(root in keyword_lower for root in self.core_roots):
            indicators.append("å·¥å…·ç±»éœ€æ±‚")
        
        # æ–°å…´æ¦‚å¿µ
        if any(concept in keyword_lower for concept in ['gpt', 'chatbot', 'neural']):
            indicators.append("æ–°å…´æŠ€æœ¯")
        
        # å‡ºæµ·å‹å¥½
        if not any(chinese_char in keyword for chinese_char in 'ä¸­æ–‡æ±‰å­—'):
            indicators.append("å‡ºæµ·å‹å¥½")
        
        return indicators
    
    def _calculate_opportunity_score(self, intent_result: Dict, market_result: Dict) -> float:
        """
        è®¡ç®—ç»¼åˆæœºä¼šåˆ†æ•°
        æ•´åˆå…­å¤§æŒ–æ˜æ–¹æ³•çš„è¯„åˆ†é€»è¾‘
        """
        try:
            # åŸºç¡€è¯„åˆ†æƒé‡
            weights = {
                'intent_confidence': 0.15,      # æ„å›¾ç½®ä¿¡åº¦
                'search_volume': 0.20,          # æœç´¢é‡
                'competition': 0.15,            # ç«äº‰åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                'ai_bonus': 0.20,              # AIç›¸å…³åŠ åˆ†
                'commercial_value': 0.15,       # å•†ä¸šä»·å€¼
                'newness': 0.10,               # æ–°å…´ç¨‹åº¦
                'root_match': 0.05             # è¯æ ¹åŒ¹é…
            }
            
            # å„é¡¹åˆ†æ•°è®¡ç®—
            intent_score = intent_result.get('confidence', 0) * 100
            volume_score = min(market_result.get('search_volume', 0) / 1000, 1) * 100
            competition_score = (1 - market_result.get('competition', 1)) * 100
            ai_bonus = market_result.get('ai_bonus', 0)
            commercial_value = market_result.get('commercial_value', 0)
            newness_score = market_result.get('newness_score', 0)
            root_match_score = market_result.get('root_match_score', 0)
            
            # åŠ æƒè®¡ç®—æ€»åˆ†
            total_score = (
                intent_score * weights['intent_confidence'] +
                volume_score * weights['search_volume'] +
                competition_score * weights['competition'] +
                ai_bonus * weights['ai_bonus'] +
                commercial_value * weights['commercial_value'] +
                newness_score * weights['newness'] +
                root_match_score * weights['root_match']
            )
            
            # ç‰¹æ®ŠåŠ åˆ†é¡¹
            bonus_points = 0
            
            # è¶‹åŠ¿åŠ åˆ†
            if market_result.get('trend') == 'rising':
                bonus_points += 5
            
            # æœºä¼šæŒ‡æ ‡åŠ åˆ†
            opportunity_indicators = market_result.get('opportunity_indicators', [])
            if len(opportunity_indicators) >= 3:
                bonus_points += 5
            
            final_score = total_score + bonus_points
            return round(min(final_score, 100), 2)
            
        except Exception as e:
            print(f"âš ï¸ æœºä¼šåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _generate_intent_summary(self, keywords: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ„å›¾æ‘˜è¦"""
        intent_counts = {}
        total_keywords = len(keywords)
        
        for kw in keywords:
            intent = kw['intent']['primary_intent']
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        intent_percentages = {
            intent: round(count / total_keywords * 100, 1)
            for intent, count in intent_counts.items()
        }
        
        return {
            'total_keywords': total_keywords,
            'intent_distribution': intent_counts,
            'intent_percentages': intent_percentages,
            'dominant_intent': max(intent_counts.items(), key=lambda x: x[1])[0] if intent_counts else 'Unknown'
        }
    
    def _generate_market_insights(self, keywords: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå¸‚åœºæ´å¯Ÿ"""
        high_opportunity = [kw for kw in keywords if kw['opportunity_score'] >= 70]
        medium_opportunity = [kw for kw in keywords if 40 <= kw['opportunity_score'] < 70]
        low_opportunity = [kw for kw in keywords if kw['opportunity_score'] < 40]
        
        return {
            'high_opportunity_count': len(high_opportunity),
            'medium_opportunity_count': len(medium_opportunity),
            'low_opportunity_count': len(low_opportunity),
            'top_opportunities': sorted(keywords, key=lambda x: x['opportunity_score'], reverse=True)[:10],
            'avg_opportunity_score': round(sum(kw['opportunity_score'] for kw in keywords) / len(keywords), 2) if keywords else 0
        }
    
    def _generate_recommendations(self, keywords: List[Dict]) -> List[str]:
        """
        ç”ŸæˆåŸºäºå…­å¤§æŒ–æ˜æ–¹æ³•çš„ç»¼åˆå»ºè®®
        """
        recommendations = []
        
        # åˆ†æå…³é”®è¯åˆ†å¸ƒ
        high_opportunity = [kw for kw in keywords if kw['opportunity_score'] >= 70]
        medium_opportunity = [kw for kw in keywords if 40 <= kw['opportunity_score'] < 70]
        ai_keywords = [kw for kw in keywords if kw['market'].get('ai_bonus', 0) > 0]
        new_keywords = [kw for kw in keywords if kw['market'].get('newness_score', 0) > 20]
        
        # é«˜æœºä¼šå…³é”®è¯å»ºè®®
        if high_opportunity:
            recommendations.append(f"ğŸ¯ å‘ç° {len(high_opportunity)} ä¸ªé«˜æœºä¼šå…³é”®è¯ï¼Œå»ºè®®ç«‹å³å¼€å‘MVPäº§å“æŠ¢å å…ˆæœº")
            top_3 = sorted(high_opportunity, key=lambda x: x['opportunity_score'], reverse=True)[:3]
            for i, kw in enumerate(top_3, 1):
                recommendations.append(f"   {i}. {kw['keyword']} (æœºä¼šåˆ†æ•°: {kw['opportunity_score']})")
        
        # AIç›¸å…³å»ºè®®
        if ai_keywords:
            recommendations.append(f"ğŸ¤– å‘ç° {len(ai_keywords)} ä¸ªAIç›¸å…³å…³é”®è¯ï¼Œç¬¦åˆå‡ºæµ·AIå·¥å…·æ–¹å‘")
            recommendations.append("   å»ºè®®é‡ç‚¹å…³æ³¨AI Generatorã€AI Detectorã€AI Assistantç­‰é«˜ä»·å€¼ç±»å‹")
        
        # æ–°å…´å…³é”®è¯å»ºè®®
        if new_keywords:
            recommendations.append(f"ğŸ†• å‘ç° {len(new_keywords)} ä¸ªæ–°å…´å…³é”®è¯ï¼Œç«äº‰ç›¸å¯¹è¾ƒå°")
            recommendations.append("   å»ºè®®å¿«é€ŸéªŒè¯éœ€æ±‚å¹¶å¼€å‘ï¼Œåˆ©ç”¨SEOä¼˜åŠ¿æŠ¢å æ’å")
        
        # åŸºäºå…­å¤§æ–¹æ³•çš„å…·ä½“å»ºè®®
        recommendations.extend([
            "ğŸ“Š æ–¹æ³•ä¸€å»ºè®®ï¼šåŸºäºæ ¸å¿ƒè¯æ ¹ç»§ç»­æ‹“å±•ï¼Œé‡ç‚¹å…³æ³¨Generatorã€Converterã€Analyzerç±»å…³é”®è¯",
            "ğŸ” æ–¹æ³•äºŒå»ºè®®ï¼šåˆ†æCanvaã€Midjourneyç­‰å¤§ç«™æµé‡ï¼Œå¯»æ‰¾å†…é¡µæ’åçš„æœºä¼šå…³é”®è¯",
            "ğŸ’¡ æ–¹æ³•ä¸‰å»ºè®®ï¼šåˆ©ç”¨Googleä¸‹æ‹‰æ¨èå‘ç°é•¿å°¾å…³é”®è¯ï¼Œé™ä½ç«äº‰éš¾åº¦",
            "ğŸ”„ æ–¹æ³•å››å»ºè®®ï¼šå»ºç«‹å¾ªç¯æŒ–æ˜æœºåˆ¶ï¼Œä»æœç´¢ç»“æœâ†’ç«å“åˆ†æâ†’æ–°å…³é”®è¯â†’å†æœç´¢",
            "ğŸ’° æ–¹æ³•äº”å»ºè®®ï¼šå…³æ³¨ä»˜è´¹å¹¿å‘ŠæŠ•æ”¾çš„å…³é”®è¯ï¼Œè¿™äº›é€šå¸¸æ˜¯é«˜ROIçš„ç›ˆåˆ©éœ€æ±‚",
            "ğŸ“ˆ æ–¹æ³•å…­å»ºè®®ï¼šå®šæœŸæŸ¥çœ‹Toolify.aiå’ŒIndieHackersæ’è¡Œæ¦œï¼Œè·Ÿè¸ªæˆåŠŸäº§å“çš„å…³é”®è¯ç­–ç•¥"
        ])
        
        # æµé‡è·å–å»ºè®®
        recommendations.extend([
            "ğŸš€ æµé‡è·å–ï¼šä¼˜å…ˆSEO+SEMç»„åˆï¼Œå…è´¹æµé‡+ä»˜è´¹æµé‡åŒç®¡é½ä¸‹",
            "ğŸŒ å¯¼èˆªç«™ï¼šæäº¤åˆ°300+ä¸ªAIå¯¼èˆªç«™ï¼Œé‡ç‚¹å…³æ³¨theresanaiforthat.comå’ŒToolify.ai",
            "ğŸ“± ç¤¾äº¤åª’ä½“ï¼šåˆ©ç”¨Redditã€ProductHuntã€Hacker Newsç­‰å¹³å°è¿›è¡Œè½¯æ€§æ¨å¹¿",
            "ğŸ¤ è”ç›Ÿè¥é”€ï¼šå»ºç«‹Affiliateæœºåˆ¶ï¼Œè®©ä¸­å°æµé‡æ¸ é“å¸®åŠ©æ¨å¹¿"
        ])
        
        # æ”¶å…¥éªŒè¯å»ºè®®
        if medium_opportunity or high_opportunity:
            recommendations.extend([
                "ğŸ’µ æ”¶å…¥éªŒè¯ï¼šé€šè¿‡Similarwebåˆ†æcheckout.stripe.comæµé‡ï¼ŒéªŒè¯éœ€æ±‚çš„çœŸå®ç›ˆåˆ©èƒ½åŠ›",
                "ğŸ“Š ç«å“åˆ†æï¼šå…³æ³¨ç«å“çš„å¹¿å‘ŠæŠ•æ”¾æƒ…å†µï¼ŒæŒç»­æŠ•æ”¾=æŒç»­ç›ˆåˆ©",
                "âš¡ å¿«é€Ÿè¡ŒåŠ¨ï¼šå‘ç°é«˜ä»·å€¼éœ€æ±‚å24-48å°æ—¶å†…å®ŒæˆMVPï¼ŒæŠ¢å å…ˆå‘ä¼˜åŠ¿"
            ])
        
        # æ„å›¾åˆ†å¸ƒå»ºè®®
        intent_summary = self._generate_intent_summary(keywords)
        dominant_intent = intent_summary['dominant_intent']
        if dominant_intent != 'Unknown':
            recommendations.append(f"ğŸ¯ ä¸»è¦æ„å›¾ä¸º {dominant_intent}ï¼Œå»ºè®®é’ˆå¯¹æ­¤æ„å›¾ä¼˜åŒ–äº§å“åŠŸèƒ½å’Œé¡µé¢å†…å®¹")
        
        return recommendations
    
    def _save_analysis_results(self, results: Dict[str, Any], output_dir: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not output_dir:
            output_dir = os.path.join(self.output_dir, 'keyword_analysis')
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = os.path.join(output_dir, f'keyword_analysis_{timestamp}.json')
        import json
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼ï¼ˆå…³é”®è¯è¯¦æƒ…ï¼‰
        csv_path = os.path.join(output_dir, f'keywords_detail_{timestamp}.csv')
        keywords_df = pd.DataFrame([
            {
                'keyword': kw['keyword'],
                'primary_intent': kw['intent']['primary_intent'],
                'confidence': kw['intent']['confidence'],
                'search_volume': kw['market']['search_volume'],
                'competition': kw['market']['competition'],
                'opportunity_score': kw['opportunity_score']
            }
            for kw in results['keywords']
        ])
        keywords_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        return json_path
    
    def generate_daily_report(self, date: str = None) -> str:
        """ç”Ÿæˆæ—¥æŠ¥"""
        if not date:
            date = datetime.now().strftime('%Y-%m-%d')
        
        print(f"ğŸ“… ç”Ÿæˆæ—¥æŠ¥: {date}")
        
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„æ—¥æŠ¥ç”Ÿæˆé€»è¾‘
        report_content = f"""# éœ€æ±‚æŒ–æ˜æ—¥æŠ¥ - {date}

## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ
- åˆ†æå…³é”®è¯æ•°é‡: å¾…ç»Ÿè®¡
- å‘ç°é«˜ä»·å€¼å…³é”®è¯: å¾…ç»Ÿè®¡
- æ–°å¢æ„å›¾ç±»åˆ«: å¾…ç»Ÿè®¡

## ğŸ” å…³é”®å‘ç°
- å¾…è¡¥å……å…·ä½“å‘ç°

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ
- å¾…è¡¥å……è¶‹åŠ¿æ•°æ®

## ğŸ’¡ è¡ŒåŠ¨å»ºè®®
- å¾…è¡¥å……å…·ä½“å»ºè®®

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        report_path = os.path.join(self.output_dir, 'daily_reports', f'daily_report_{date}.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æ—¥æŠ¥å·²ç”Ÿæˆ: {report_path}")
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='éœ€æ±‚æŒ–æ˜ä¸å…³é”®è¯åˆ†æå·¥å…·')
    parser.add_argument('--action', choices=['analyze', 'report', 'help'], 
                       default='help', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--input', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--date', help='æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    if args.action == 'help':
        print("""
ğŸ” éœ€æ±‚æŒ–æ˜ä¸å…³é”®è¯åˆ†æå·¥å…·

ä½¿ç”¨æ–¹æ³•:
  python demand_mining_main.py --action analyze --input data/keywords.csv
  python demand_mining_main.py --action report --date 2025-08-08
  python demand_mining_main.py --help

æ“ä½œè¯´æ˜:
  analyze  - åˆ†æå…³é”®è¯æ–‡ä»¶
  report   - ç”Ÿæˆåˆ†ææŠ¥å‘Š
  help     - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  # åˆ†æå…³é”®è¯
  python demand_mining_main.py --action analyze --input data/test_intent_keywords.csv
  
  # ç”Ÿæˆä»Šæ—¥æŠ¥å‘Š
  python demand_mining_main.py --action report
  
  # ç”ŸæˆæŒ‡å®šæ—¥æœŸæŠ¥å‘Š
  python demand_mining_main.py --action report --date 2025-08-08
        """)
        return
    
    try:
        # åˆå§‹åŒ–ç®¡ç†å™¨
        manager = DemandMiningManager(args.config)
        
        if args.action == 'analyze':
            if not args.input:
                print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶ (--input)")
                return
            
            results = manager.analyze_keywords(args.input, args.output)
            print(f"ğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {results['total_keywords']} ä¸ªå…³é”®è¯")
            
        elif args.action == 'report':
            report_path = manager.generate_daily_report(args.date)
            print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()