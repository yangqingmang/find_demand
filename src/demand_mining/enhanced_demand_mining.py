#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
æ•´åˆ51ä¸ªç½‘ç»œæ”¶é›†è¯æ ¹ï¼Œæ”¯æŒæ‰‹åŠ¨æŒ‡å®šæ¯è¯å’Œæ™ºèƒ½å…³é”®è¯å‘ç°
"""

import os
import sys
import argparse
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.demand_mining.demand_mining_main import DemandMiningManager
from src.demand_mining.core.root_word_manager import RootWordManager


class EnhancedDemandMiningManager(DemandMiningManager):
    """
    å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
    åŸºäº51ä¸ªç½‘ç»œæ”¶é›†è¯æ ¹ï¼Œæ”¯æŒæ‰‹åŠ¨æŒ‡å®šæ¯è¯å’Œæ™ºèƒ½å…³é”®è¯å‘ç°
    """
    
    def __init__(self, config_path: str = None, root_config_path: str = None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
        
        Args:
            config_path: ä¸»é…ç½®æ–‡ä»¶è·¯å¾„
            root_config_path: è¯æ ¹é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # å…ˆåˆå§‹åŒ–è¯æ ¹ç®¡ç†å™¨
        self.root_manager = RootWordManager(root_config_path)
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(config_path)
        
        # ä½¿ç”¨è¯æ ¹ç®¡ç†å™¨çš„æ•°æ®è¦†ç›–çˆ¶ç±»æ•°æ®
        self.core_roots = self.root_manager.get_active_roots()
        self.ai_prefixes = self.root_manager.ai_prefixes
        
        print("ğŸš€ å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å·²æ•´åˆ {len(self.core_roots)} ä¸ªç½‘ç»œæ”¶é›†è¯æ ¹")
        print(f"ğŸ”§ è¯æ ¹çŠ¶æ€: {'æ‰‹åŠ¨æŒ‡å®š' if self.root_manager.manual_roots else 'é»˜è®¤é…ç½®'}")
    
    def set_manual_roots(self, roots: List[str]) -> None:
        """
        æ‰‹åŠ¨æŒ‡å®šæ¯è¯
        
        Args:
            roots: æ‰‹åŠ¨æŒ‡å®šçš„è¯æ ¹åˆ—è¡¨
        """
        self.root_manager.set_manual_roots(roots)
        # æ›´æ–°å½“å‰ä½¿ç”¨çš„è¯æ ¹
        self.core_roots = self.root_manager.get_active_roots()
        print(f"âœ… å·²æ‰‹åŠ¨æŒ‡å®š {len(roots)} ä¸ªæ¯è¯")
        print(f"ğŸ¯ å½“å‰æ¿€æ´»è¯æ ¹: {len(self.core_roots)} ä¸ª")
        
        # æ˜¾ç¤ºå‰10ä¸ªæ¿€æ´»è¯æ ¹
        if self.core_roots:
            print(f"ğŸ“‹ æ¿€æ´»è¯æ ¹é¢„è§ˆ: {', '.join(self.core_roots[:10])}{'...' if len(self.core_roots) > 10 else ''}")
    
    def get_root_stats(self) -> Dict[str, Any]:
        """è·å–è¯æ ¹ç»Ÿè®¡ä¿¡æ¯"""
        return self.root_manager.get_stats()
    
    def discover_keywords_from_seeds(self, 
                                   seed_words: List[str], 
                                   target_count: int = 50,
                                   include_ai: bool = True) -> List[str]:
        """
        ä»ç§å­è¯å‘ç°å…³é”®è¯
        
        Args:
            seed_words: ç§å­è¯åˆ—è¡¨
            target_count: ç›®æ ‡å…³é”®è¯æ•°é‡
            include_ai: æ˜¯å¦åŒ…å«AIå‰ç¼€ç»„åˆ
            
        Returns:
            å‘ç°çš„å…³é”®è¯åˆ—è¡¨
        """
        print(f"ğŸŒ± å¼€å§‹ä» {len(seed_words)} ä¸ªç§å­è¯å‘ç°å…³é”®è¯...")
        print(f"ğŸ¯ ç›®æ ‡æ•°é‡: {target_count}")
        
        # ä½¿ç”¨è¯æ ¹ç®¡ç†å™¨ç”Ÿæˆå…³é”®è¯ç»„åˆ
        combinations = self.root_manager.generate_keyword_combinations(
            seed_words=seed_words,
            include_ai=include_ai,
            max_combinations=target_count
        )
        
        print(f"âœ… å‘ç°å®Œæˆ: {len(combinations)} ä¸ªå…³é”®è¯")
        return combinations
    
    def analyze_root_coverage(self, keywords: List[str]) -> Dict[str, Any]:
        """
        åˆ†æå…³é”®è¯çš„è¯æ ¹è¦†ç›–æƒ…å†µ
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            è¯æ ¹è¦†ç›–åˆ†æç»“æœ
        """
        return self.root_manager.analyze_root_coverage(keywords)
    
    def suggest_missing_opportunities(self, current_keywords: List[str]) -> List[str]:
        """
        åŸºäºè¯æ ¹è¦†ç›–åˆ†æï¼Œå»ºè®®ç¼ºå¤±çš„æœºä¼šå…³é”®è¯
        
        Args:
            current_keywords: å½“å‰å…³é”®è¯åˆ—è¡¨
            
        Returns:
            å»ºè®®çš„æ–°å…³é”®è¯åˆ—è¡¨
        """
        return self.root_manager.suggest_missing_opportunities(current_keywords)
    
    def get_category_keywords(self, category: str, seed_words: List[str]) -> List[str]:
        """
        è·å–æŒ‡å®šç±»åˆ«çš„å…³é”®è¯
        
        Args:
            category: å·¥å…·ç±»åˆ« (å¦‚ 'content_creation', 'data_processing' ç­‰)
            seed_words: ç§å­è¯åˆ—è¡¨
            
        Returns:
            è¯¥ç±»åˆ«çš„å…³é”®è¯åˆ—è¡¨
        """
        category_roots = self.root_manager.get_category_roots(category)
        if not category_roots:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ« '{category}' çš„è¯æ ¹")
            return []
        
        print(f"ğŸ·ï¸ ç”Ÿæˆ '{category}' ç±»åˆ«å…³é”®è¯...")
        print(f"ğŸ“Š ç±»åˆ«è¯æ ¹: {len(category_roots)} ä¸ª")
        
        keywords = []
        for seed in seed_words:
            for root in category_roots:
                keywords.extend([
                    f"{seed} {root}",
                    f"{root} {seed}",
                    f"ai {seed} {root}",
                    f"free {seed} {root}",
                    f"online {seed} {root}"
                ])
        
        # å»é‡
        unique_keywords = list(set(keywords))
        print(f"âœ… ç”Ÿæˆå®Œæˆ: {len(unique_keywords)} ä¸ª '{category}' ç±»åˆ«å…³é”®è¯")
        
        return unique_keywords
    
    def run_enhanced_analysis(self, 
                            input_source: str,
                            analysis_type: str = 'file',
                            output_dir: str = None) -> Dict[str, Any]:
        """
        è¿è¡Œå¢å¼ºç‰ˆåˆ†æ
        
        Args:
            input_source: è¾“å…¥æºï¼ˆæ–‡ä»¶è·¯å¾„æˆ–ç§å­è¯åˆ—è¡¨ï¼‰
            analysis_type: åˆ†æç±»å‹ ('file' æˆ– 'seeds')
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å¢å¼ºåˆ†æç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜åˆ†æ")
        print(f"ğŸ“Š åˆ†æç±»å‹: {analysis_type}")
        
        if analysis_type == 'file':
            # æ–‡ä»¶åˆ†ææ¨¡å¼
            if not os.path.exists(input_source):
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_source}")
            
            # è°ƒç”¨çˆ¶ç±»çš„å…³é”®è¯åˆ†ææ–¹æ³•
            results = self.analyze_keywords(input_source, output_dir)
            
            # æ·»åŠ è¯æ ¹è¦†ç›–åˆ†æ
            keywords_list = [kw['keyword'] for kw in results.get('keywords', [])]
            coverage_analysis = self.analyze_root_coverage(keywords_list)
            results['root_coverage'] = coverage_analysis
            
            # æ·»åŠ æœºä¼šå»ºè®®
            opportunity_suggestions = self.suggest_missing_opportunities(keywords_list)
            results['opportunity_suggestions'] = opportunity_suggestions
            
        elif analysis_type == 'seeds':
            # ç§å­è¯åˆ†ææ¨¡å¼
            seed_words = input_source if isinstance(input_source, list) else [input_source]
            
            # ä»ç§å­è¯å‘ç°å…³é”®è¯
            discovered_keywords = self.discover_keywords_from_seeds(seed_words, target_count=100)
            
            # åˆ›å»ºä¸´æ—¶DataFrameè¿›è¡Œåˆ†æ
            temp_df = pd.DataFrame([{'query': kw} for kw in discovered_keywords])
            
            # åˆ†æå‘ç°çš„å…³é”®è¯
            results = {
                'total_keywords': len(discovered_keywords),
                'analysis_time': datetime.now().isoformat(),
                'seed_words': seed_words,
                'discovered_keywords': discovered_keywords,
                'keywords': [],
                'intent_summary': {},
                'market_insights': {},
                'recommendations': []
            }
            
            # é€ä¸ªåˆ†æå…³é”®è¯
            for keyword in discovered_keywords:
                # æ„å›¾åˆ†æ
                intent_result = self._analyze_keyword_intent(keyword)
                
                # å¸‚åœºåˆ†æ
                market_result = self._analyze_keyword_market(keyword)
                
                # æ•´åˆç»“æœ
                keyword_result = {
                    'keyword': keyword,
                    'intent': intent_result,
                    'market': market_result,
                    'opportunity_score': self._calculate_opportunity_score(intent_result, market_result)
                }
                
                results['keywords'].append(keyword_result)
            
            # ç”Ÿæˆæ‘˜è¦
            results['intent_summary'] = self._generate_intent_summary(results['keywords'])
            results['market_insights'] = self._generate_market_insights(results['keywords'])
            results['recommendations'] = self._generate_recommendations(results['keywords'])
            
            # æ·»åŠ è¯æ ¹è¦†ç›–åˆ†æ
            coverage_analysis = self.analyze_root_coverage(discovered_keywords)
            results['root_coverage'] = coverage_analysis
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
        
        # ä¿å­˜å¢å¼ºåˆ†æç»“æœ
        if output_dir:
            output_path = self._save_enhanced_results(results, output_dir, analysis_type)
            results['output_path'] = output_path
        
        print(f"âœ… å¢å¼ºç‰ˆåˆ†æå®Œæˆ!")
        return results
    
    def _save_enhanced_results(self, 
                             results: Dict[str, Any], 
                             output_dir: str,
                             analysis_type: str) -> str:
        """ä¿å­˜å¢å¼ºåˆ†æç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ä¸»è¦ç»“æœ
        main_results_path = self._save_analysis_results(results, output_dir)
        
        # ä¿å­˜è¯æ ¹è¦†ç›–åˆ†æ
        if 'root_coverage' in results:
            coverage_path = os.path.join(output_dir, f'root_coverage_{timestamp}.json')
            import json
            with open(coverage_path, 'w', encoding='utf-8') as f:
                json.dump(results['root_coverage'], f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š è¯æ ¹è¦†ç›–åˆ†æå·²ä¿å­˜: {coverage_path}")
        
        # ä¿å­˜æœºä¼šå»ºè®®
        if 'opportunity_suggestions' in results:
            suggestions_path = os.path.join(output_dir, f'opportunity_suggestions_{timestamp}.csv')
            suggestions_df = pd.DataFrame([
                {'suggested_keyword': kw} for kw in results['opportunity_suggestions']
            ])
            suggestions_df.to_csv(suggestions_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¡ æœºä¼šå»ºè®®å·²ä¿å­˜: {suggestions_path}")
        
        # ä¿å­˜å‘ç°çš„å…³é”®è¯ï¼ˆç§å­è¯æ¨¡å¼ï¼‰
        if analysis_type == 'seeds' and 'discovered_keywords' in results:
            discovered_path = os.path.join(output_dir, f'discovered_keywords_{timestamp}.csv')
            discovered_df = pd.DataFrame([
                {'keyword': kw} for kw in results['discovered_keywords']
            ])
            discovered_df.to_csv(discovered_path, index=False, encoding='utf-8-sig')
            print(f"ğŸŒ± å‘ç°çš„å…³é”®è¯å·²ä¿å­˜: {discovered_path}")
        
        return main_results_path
    
    def export_root_config(self, output_path: str) -> None:
        """å¯¼å‡ºè¯æ ¹é…ç½®"""
        self.root_manager.export_config(output_path)
    
    def generate_comprehensive_report(self, 
                                    analysis_results: Dict[str, Any],
                                    output_dir: str) -> str:
        """
        ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        
        Args:
            analysis_results: åˆ†æç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = os.path.join(output_dir, f'comprehensive_report_{timestamp}.md')
        
        # è·å–ç»Ÿè®¡æ•°æ®
        total_keywords = analysis_results.get('total_keywords', 0)
        root_coverage = analysis_results.get('root_coverage', {})
        opportunity_suggestions = analysis_results.get('opportunity_suggestions', [])
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ç»¼åˆæŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚è§ˆ
- **åˆ†ææ—¶é—´**: {analysis_results.get('analysis_time', '')}
- **å…³é”®è¯æ€»æ•°**: {total_keywords}
- **è¯æ ¹è¦†ç›–ç‡**: {root_coverage.get('coverage_rate', 0)}%
- **æœºä¼šå»ºè®®æ•°**: {len(opportunity_suggestions)}

## ğŸ¯ è¯æ ¹ä½¿ç”¨æƒ…å†µ

### è¯æ ¹è¦†ç›–ç»Ÿè®¡
- **æ€»è¯æ ¹æ•°**: {root_coverage.get('total_roots', 0)}
- **å·²è¦†ç›–è¯æ ¹**: {root_coverage.get('covered_roots', 0)}
- **è¦†ç›–ç‡**: {root_coverage.get('coverage_rate', 0)}%

### ä½¿ç”¨æœ€å¤šçš„è¯æ ¹
"""
        
        # æ·»åŠ è¯æ ¹ä½¿ç”¨æ’è¡Œ
        if 'root_usage' in root_coverage:
            top_roots = list(root_coverage['root_usage'].items())[:10]
            for i, (root, count) in enumerate(top_roots, 1):
                report_content += f"{i}. **{root}**: {count} æ¬¡ä½¿ç”¨\n"
        
        # æ·»åŠ æœªä½¿ç”¨çš„è¯æ ¹
        unused_roots = root_coverage.get('unused_roots', [])
        if unused_roots:
            report_content += f"\n### æœªä½¿ç”¨çš„è¯æ ¹ ({len(unused_roots)} ä¸ª)\n"
            for root in unused_roots[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                report_content += f"- {root}\n"
        
        # æ·»åŠ æœºä¼šå»ºè®®
        if opportunity_suggestions:
            report_content += f"\n## ğŸ’¡ æœºä¼šå»ºè®®å…³é”®è¯ (å‰20ä¸ª)\n"
            for i, suggestion in enumerate(opportunity_suggestions[:20], 1):
                report_content += f"{i}. {suggestion}\n"
        
        # æ·»åŠ é«˜ä»·å€¼å…³é”®è¯
        high_value_keywords = [
            kw for kw in analysis_results.get('keywords', [])
            if kw.get('opportunity_score', 0) >= 70
        ]
        
        if high_value_keywords:
            report_content += f"\n## ğŸŒŸ é«˜ä»·å€¼å…³é”®è¯ ({len(high_value_keywords)} ä¸ª)\n"
            for kw in high_value_keywords[:15]:
                report_content += f"- **{kw['keyword']}** (åˆ†æ•°: {kw.get('opportunity_score', 0)})\n"
        
        # æ·»åŠ å»ºè®®
        report_content += f"""
## ğŸ“ˆ ä¼˜åŒ–å»ºè®®

### è¯æ ¹ä¼˜åŒ–
- é‡ç‚¹å…³æ³¨æœªä½¿ç”¨çš„ {len(unused_roots)} ä¸ªè¯æ ¹ï¼Œå¯èƒ½å­˜åœ¨æ–°æœºä¼š
- æ·±å…¥æŒ–æ˜ä½¿ç”¨é¢‘ç‡ä½çš„è¯æ ¹ç»„åˆ
- è€ƒè™‘æ·»åŠ è¡Œä¸šç‰¹å®šçš„è¯æ ¹

### å…³é”®è¯æ‹“å±•
- åŸºäºé«˜ä»·å€¼å…³é”®è¯è¿›è¡Œé•¿å°¾æ‹“å±•
- ç»“åˆAIå‰ç¼€åˆ›é€ æ–°çš„å…³é”®è¯ç»„åˆ
- å…³æ³¨ç«äº‰åº¦è¾ƒä½çš„æ–°å…´è¯æ ¹

### å¸‚åœºæœºä¼š
- ä¼˜å…ˆå¼€å‘æœºä¼šåˆ†æ•°70+çš„å…³é”®è¯å¯¹åº”äº§å“
- å…³æ³¨AIç›¸å…³å’Œæ–°å…´æŠ€æœ¯å…³é”®è¯
- å»ºç«‹å…³é”®è¯ç›‘æ§å’Œå®šæœŸæ›´æ–°æœºåˆ¶

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜å·¥å…· - æ•´åˆ51ä¸ªç½‘ç»œæ”¶é›†è¯æ ¹')
    parser.add_argument('--action', choices=['analyze', 'discover', 'category', 'report', 'help'], 
                       default='help', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--input', help='è¾“å…¥æ–‡ä»¶è·¯å¾„æˆ–ç§å­è¯ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--root-config', help='è¯æ ¹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--manual-roots', help='æ‰‹åŠ¨æŒ‡å®šè¯æ ¹ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--category', help='å·¥å…·ç±»åˆ«åç§°')
    parser.add_argument('--target-count', type=int, default=50, help='ç›®æ ‡å…³é”®è¯æ•°é‡')
    
    args = parser.parse_args()
    
    if args.action == 'help':
        print("""
ğŸš€ å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜å·¥å…· - æ•´åˆ51ä¸ªç½‘ç»œæ”¶é›†è¯æ ¹

ä½¿ç”¨æ–¹æ³•:
  # åˆ†æå…³é”®è¯æ–‡ä»¶
  python enhanced_demand_mining.py --action analyze --input data/keywords.csv
  
  # ä»ç§å­è¯å‘ç°å…³é”®è¯
  python enhanced_demand_mining.py --action discover --input "image,text,video"
  
  # ç”Ÿæˆç‰¹å®šç±»åˆ«å…³é”®è¯
  python enhanced_demand_mining.py --action category --category content_creation --input "image,text"
  
  # æ‰‹åŠ¨æŒ‡å®šè¯æ ¹
  python enhanced_demand_mining.py --action discover --input "image,text" --manual-roots "generator,converter,editor"

æ“ä½œè¯´æ˜:
  analyze   - åˆ†æå…³é”®è¯æ–‡ä»¶
  discover  - ä»ç§å­è¯å‘ç°å…³é”®è¯
  category  - ç”Ÿæˆç‰¹å®šç±»åˆ«å…³é”®è¯
  report    - ç”Ÿæˆç»¼åˆæŠ¥å‘Š
  help      - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç±»åˆ«é€‰é¡¹:
  content_creation  - å†…å®¹åˆ›ä½œå·¥å…·
  data_processing   - æ•°æ®å¤„ç†å·¥å…·
  media_editing     - åª’ä½“ç¼–è¾‘å·¥å…·
  quality_assurance - è´¨é‡ä¿è¯å·¥å…·
  productivity      - ç”Ÿäº§åŠ›å·¥å…·
  search_discovery  - æœç´¢å‘ç°å·¥å…·
  file_management   - æ–‡ä»¶ç®¡ç†å·¥å…·
  maintenance       - ç»´æŠ¤å·¥å…·
        """)
        return
    
    try:
        # åˆå§‹åŒ–å¢å¼ºç‰ˆç®¡ç†å™¨
        manager = EnhancedDemandMiningManager(args.config, args.root_config)
        
        # æ‰‹åŠ¨æŒ‡å®šè¯æ ¹
        if args.manual_roots:
            manual_roots = [root.strip() for root in args.manual_roots.split(',')]
            manager.set_manual_roots(manual_roots)
        
        # æ˜¾ç¤ºè¯æ ¹ç»Ÿè®¡
        stats = manager.get_root_stats()
        print(f"\nğŸ“Š è¯æ ¹ç»Ÿè®¡: {stats}")
        
        if args.action == 'analyze':
            if not args.input:
                print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶ (--input)")
                return
            
            results = manager.run_enhanced_analysis(
                input_source=args.input,
                analysis_type='file',
                output_dir=args.output or 'output/enhanced_analysis'
            )
            
            print(f"ğŸ‰ æ–‡ä»¶åˆ†æå®Œæˆ! å…±åˆ†æ {results['total_keywords']} ä¸ªå…³é”®è¯")
            print(f"ğŸ“Š è¯æ ¹è¦†ç›–ç‡: {results.get('root_coverage', {}).get('coverage_rate', 0)}%")
            
        elif args.action == 'discover':
            if not args.input:
                print("âŒ é”™è¯¯: è¯·æŒ‡å®šç§å­è¯ (--input)")
                return
            
            seed_words = [word.strip() for word in args.input.split(',')]
            results = manager.run_enhanced_analysis(
                input_source=seed_words,
                analysis_type='seeds',
                output_dir=args.output or 'output/enhanced_analysis'
            )
            
            print(f"ğŸŒ± ç§å­è¯å‘ç°å®Œæˆ! ä» {len(seed_words)} ä¸ªç§å­è¯å‘ç° {len(results['discovered_keywords'])} ä¸ªå…³é”®è¯")
            
        elif args.action == 'category':
            if not args.category or not args.input:
                print("âŒ é”™è¯¯: è¯·æŒ‡å®šç±»åˆ«å’Œç§å­è¯ (--category --input)")
                return
            
            seed_words = [word.strip() for word in args.input.split(',')]
            category_keywords = manager.get_category_keywords(args.category, seed_words)
            
            # ä¿å­˜ç±»åˆ«å…³é”®è¯
            output_dir = args.output or 'output/category_keywords'
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = os.path.join(output_dir, f'{args.category}_keywords_{timestamp}.csv')
            
            df = pd.DataFrame([{'keyword': kw} for kw in category_keywords])
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ·ï¸ {args.category} ç±»åˆ«å…³é”®è¯ç”Ÿæˆå®Œæˆ!")
            print(f"ğŸ“Š ç”Ÿæˆæ•°é‡: {len(category_keywords)}")
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
            
        elif args.action == 'report':
            print("ğŸ“‹ ç”Ÿæˆç»¼åˆæŠ¥å‘ŠåŠŸèƒ½éœ€è¦å…ˆè¿è¡Œåˆ†æ...")
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()