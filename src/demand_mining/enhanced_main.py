#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ä¸»ç¨‹åº
æ·»åŠ è‡ªåŠ¨åŒ–è°ƒåº¦ã€ç«å“ç›‘æ§ã€è¶‹åŠ¿é¢„æµ‹ç­‰é«˜çº§åŠŸèƒ½
"""

import argparse
import sys
import os
import schedule
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.demand_mining.demand_mining_main import DemandMiningManager
from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery

class EnhancedDemandMiningManager(DemandMiningManager):
    """å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = None):
        super().__init__(config_path)
        self.scheduler_running = False
        
    def monitor_competitors(self, competitor_sites: List[str], output_dir: str = None) -> Dict[str, Any]:
        """ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–"""
        print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(competitor_sites)} ä¸ªç«å“ç½‘ç«™...")
        
        results = {
            'monitoring_date': datetime.now().isoformat(),
            'competitors': [],
            'new_keywords': [],
            'trending_keywords': [],
            'recommendations': []
        }
        
        for site in competitor_sites:
            print(f"ğŸ“Š åˆ†æç«å“: {site}")
            
            # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ç«å“åˆ†æAPI
            # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            competitor_data = {
                'site': site,
                'top_keywords': [
                    {'keyword': f'{site} ai tool', 'volume': 1000, 'difficulty': 0.6},
                    {'keyword': f'{site} alternative', 'volume': 800, 'difficulty': 0.4},
                    {'keyword': f'best {site} features', 'volume': 600, 'difficulty': 0.5}
                ],
                'new_keywords_count': 15,
                'trending_keywords_count': 8
            }
            
            results['competitors'].append(competitor_data)
        
        # ä¿å­˜ç›‘æ§ç»“æœ
        if output_dir:
            self._save_competitor_monitoring_results(results, output_dir)
        
        return results
    
    def predict_keyword_trends(self, timeframe: str = "30d", output_dir: str = None) -> Dict[str, Any]:
        """é¢„æµ‹å…³é”®è¯è¶‹åŠ¿"""
        print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
        
        # åŸºäºå†å²æ•°æ®å’Œå½“å‰è¶‹åŠ¿è¿›è¡Œé¢„æµ‹
        predictions = {
            'prediction_date': datetime.now().isoformat(),
            'timeframe': timeframe,
            'rising_keywords': [
                {'keyword': 'AI video generator', 'predicted_growth': '+150%', 'confidence': 0.85},
                {'keyword': 'AI code assistant', 'predicted_growth': '+120%', 'confidence': 0.78},
                {'keyword': 'AI image upscaler', 'predicted_growth': '+90%', 'confidence': 0.72}
            ],
            'declining_keywords': [
                {'keyword': 'basic chatbot', 'predicted_decline': '-30%', 'confidence': 0.65},
                {'keyword': 'simple ai writer', 'predicted_decline': '-20%', 'confidence': 0.58}
            ],
            'stable_keywords': [
                {'keyword': 'AI generator', 'predicted_change': '+5%', 'confidence': 0.90},
                {'keyword': 'AI assistant', 'predicted_change': '+10%', 'confidence': 0.88}
            ]
        }
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if output_dir:
            self._save_trend_predictions(predictions, output_dir)
        
        return predictions
    
    def generate_seo_audit(self, domain: str, keywords: List[str] = None) -> Dict[str, Any]:
        """ç”ŸæˆSEOä¼˜åŒ–å»ºè®®"""
        print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {domain}")
        
        audit_results = {
            'domain': domain,
            'audit_date': datetime.now().isoformat(),
            'keyword_opportunities': [],
            'content_gaps': [],
            'technical_recommendations': [],
            'competitor_analysis': {}
        }
        
        # å…³é”®è¯æœºä¼šåˆ†æ
        if keywords:
            for keyword in keywords[:10]:  # é™åˆ¶åˆ†ææ•°é‡
                opportunity = {
                    'keyword': keyword,
                    'current_ranking': 'Not ranking',
                    'difficulty': 0.6,
                    'opportunity_score': 75,
                    'recommended_actions': [
                        f'åˆ›å»ºé’ˆå¯¹"{keyword}"çš„ä¸“é—¨é¡µé¢',
                        f'ä¼˜åŒ–é¡µé¢æ ‡é¢˜åŒ…å«"{keyword}"',
                        f'å¢åŠ ç›¸å…³çš„å†…éƒ¨é“¾æ¥'
                    ]
                }
                audit_results['keyword_opportunities'].append(opportunity)
        
        # å†…å®¹ç¼ºå£åˆ†æ
        audit_results['content_gaps'] = [
            'ç¼ºå°‘AIå·¥å…·æ¯”è¾ƒé¡µé¢',
            'éœ€è¦æ›´å¤šæ•™ç¨‹å†…å®¹',
            'ç¼ºå°‘ç”¨æˆ·æ¡ˆä¾‹ç ”ç©¶'
        ]
        
        # æŠ€æœ¯å»ºè®®
        audit_results['technical_recommendations'] = [
            'ä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦',
            'æ”¹å–„ç§»åŠ¨ç«¯ä½“éªŒ',
            'æ·»åŠ ç»“æ„åŒ–æ•°æ®',
            'ä¼˜åŒ–å›¾ç‰‡altæ ‡ç­¾'
        ]
        
        return audit_results
    
    def batch_build_websites(self, top_keywords: int = 10, output_dir: str = None) -> Dict[str, Any]:
        """æ‰¹é‡ç”Ÿæˆç½‘ç«™"""
        print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {top_keywords} ä¸ªé«˜ä»·å€¼å…³é”®è¯çš„ç½‘ç«™...")
        
        # è·å–é«˜ä»·å€¼å…³é”®è¯
        # è¿™é‡Œåº”è¯¥ä»ä¹‹å‰çš„åˆ†æç»“æœä¸­è·å–
        high_value_keywords = [
            'AI image generator',
            'AI writing assistant', 
            'AI code helper',
            'AI video creator',
            'AI design tool'
        ][:top_keywords]
        
        build_results = {
            'build_date': datetime.now().isoformat(),
            'total_websites': len(high_value_keywords),
            'successful_builds': 0,
            'failed_builds': 0,
            'websites': []
        }
        
        for keyword in high_value_keywords:
            try:
                print(f"ğŸ”¨ æ„å»ºç½‘ç«™: {keyword}")
                
                # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ç½‘ç«™æ„å»ºé€»è¾‘
                website_info = {
                    'keyword': keyword,
                    'domain_suggestion': keyword.replace(' ', '-').lower() + '.com',
                    'status': 'success',
                    'pages_created': 5,
                    'estimated_build_time': '2 minutes'
                }
                
                build_results['websites'].append(website_info)
                build_results['successful_builds'] += 1
                
            except Exception as e:
                print(f"âŒ æ„å»ºå¤±è´¥ {keyword}: {e}")
                build_results['failed_builds'] += 1
        
        return build_results
    
    def setup_scheduler(self, schedule_type: str, time_str: str, action: str, **kwargs):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        print(f"â° è®¾ç½®å®šæ—¶ä»»åŠ¡: {schedule_type} at {time_str} - {action}")
        
        if schedule_type == 'daily':
            schedule.every().day.at(time_str).do(self._scheduled_task, action, **kwargs)
        elif schedule_type == 'weekly':
            schedule.every().week.at(time_str).do(self._scheduled_task, action, **kwargs)
        elif schedule_type == 'hourly':
            schedule.every().hour.do(self._scheduled_task, action, **kwargs)
        
        print("âœ… å®šæ—¶ä»»åŠ¡å·²è®¾ç½®")
    
    def _scheduled_task(self, action: str, **kwargs):
        """æ‰§è¡Œå®šæ—¶ä»»åŠ¡"""
        print(f"ğŸ¤– æ‰§è¡Œå®šæ—¶ä»»åŠ¡: {action} at {datetime.now()}")
        
        try:
            if action == 'discover':
                search_terms = kwargs.get('search_terms', ['AI tool', 'AI generator'])
                discoverer = MultiPlatformKeywordDiscovery()
                df = discoverer.discover_all_platforms(search_terms)
                if not df.empty:
                    analysis = discoverer.analyze_keyword_trends(df)
                    print(f"âœ… å®šæ—¶å‘ç°å®Œæˆ: {analysis['total_keywords']} ä¸ªå…³é”®è¯")
            
            elif action == 'analyze':
                keywords_file = kwargs.get('keywords_file')
                if keywords_file and os.path.exists(keywords_file):
                    result = self.analyze_keywords(keywords_file)
                    print(f"âœ… å®šæ—¶åˆ†æå®Œæˆ: {result['total_keywords']} ä¸ªå…³é”®è¯")
            
            elif action == 'monitor':
                sites = kwargs.get('sites', ['canva.com', 'midjourney.com'])
                result = self.monitor_competitors(sites)
                print(f"âœ… å®šæ—¶ç›‘æ§å®Œæˆ: {len(result['competitors'])} ä¸ªç«å“")
                
        except Exception as e:
            print(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        print("ğŸš€ å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨...")
        self.scheduler_running = True
        
        try:
            while self.scheduler_running:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print("\nâš ï¸ è°ƒåº¦å™¨è¢«ç”¨æˆ·ä¸­æ–­")
            self.scheduler_running = False
    
    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.scheduler_running = False
        schedule.clear()
        print("ğŸ›‘ è°ƒåº¦å™¨å·²åœæ­¢")
    
    def _save_competitor_monitoring_results(self, results: Dict, output_dir: str):
        """ä¿å­˜ç«å“ç›‘æ§ç»“æœ"""
        import json
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_path = os.path.join(output_dir, f'competitor_monitoring_{timestamp}.json')
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ç«å“ç›‘æ§ç»“æœå·²ä¿å­˜: {file_path}")
    
    def _save_trend_predictions(self, predictions: Dict, output_dir: str):
        """ä¿å­˜è¶‹åŠ¿é¢„æµ‹ç»“æœ"""
        import json
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_path = os.path.join(output_dir, f'trend_predictions_{timestamp}.json')
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(predictions, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ è¶‹åŠ¿é¢„æµ‹ç»“æœå·²ä¿å­˜: {file_path}")


def main():
    """å¢å¼ºç‰ˆä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¢å¼ºç‰ˆéœ€æ±‚æŒ–æ˜åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸš€ å¢å¼ºåŠŸèƒ½:
  --monitor-competitors    ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–
  --predict-trends        é¢„æµ‹å…³é”®è¯è¶‹åŠ¿
  --seo-audit            ç”ŸæˆSEOä¼˜åŒ–å»ºè®®
  --build-websites       æ‰¹é‡ç”Ÿæˆç½‘ç«™
  --schedule             è®¾ç½®å®šæ—¶ä»»åŠ¡

ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹:
  # ç›‘æ§ç«å“
  python enhanced_main.py --monitor-competitors --sites canva.com midjourney.com
  
  # é¢„æµ‹è¶‹åŠ¿
  python enhanced_main.py --predict-trends --timeframe 30d
  
  # SEOå®¡è®¡
  python enhanced_main.py --seo-audit --domain your-site.com --keywords "ai tool" "ai generator"
  
  # æ‰¹é‡å»ºç«™
  python enhanced_main.py --build-websites --top-keywords 5
  
  # è®¾ç½®å®šæ—¶ä»»åŠ¡
  python enhanced_main.py --schedule daily --time "09:00" --action discover
        """
    )
    
    # åŸºç¡€åŠŸèƒ½
    parser.add_argument('--input', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--keywords', nargs='+', help='ç›´æ¥è¾“å…¥å…³é”®è¯')
    parser.add_argument('--discover', nargs='*', help='å¤šå¹³å°å…³é”®è¯å‘ç°')
    
    # å¢å¼ºåŠŸèƒ½
    parser.add_argument('--monitor-competitors', action='store_true', help='ç›‘æ§ç«å“')
    parser.add_argument('--sites', nargs='+', help='ç«å“ç½‘ç«™åˆ—è¡¨')
    parser.add_argument('--predict-trends', action='store_true', help='é¢„æµ‹å…³é”®è¯è¶‹åŠ¿')
    parser.add_argument('--timeframe', default='30d', help='é¢„æµ‹æ—¶é—´èŒƒå›´')
    parser.add_argument('--seo-audit', action='store_true', help='SEOå®¡è®¡')
    parser.add_argument('--domain', help='è¦å®¡è®¡çš„åŸŸå')
    parser.add_argument('--build-websites', action='store_true', help='æ‰¹é‡ç”Ÿæˆç½‘ç«™')
    parser.add_argument('--top-keywords', type=int, default=10, help='ä½¿ç”¨å‰Nä¸ªå…³é”®è¯')
    
    # è°ƒåº¦åŠŸèƒ½
    parser.add_argument('--schedule', choices=['daily', 'weekly', 'hourly'], help='è®¾ç½®å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--time', help='æ‰§è¡Œæ—¶é—´ (HH:MM)')
    parser.add_argument('--action', help='å®šæ—¶æ‰§è¡Œçš„åŠ¨ä½œ')
    parser.add_argument('--run-scheduler', action='store_true', help='è¿è¡Œè°ƒåº¦å™¨')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--output', default='src/demand_mining/reports', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆç®¡ç†å™¨
        manager = EnhancedDemandMiningManager(args.config)
        
        if args.monitor_competitors:
            # ç«å“ç›‘æ§
            sites = args.sites or ['canva.com', 'midjourney.com', 'openai.com']
            result = manager.monitor_competitors(sites, args.output)
            print(f"âœ… ç«å“ç›‘æ§å®Œæˆ: åˆ†æäº† {len(result['competitors'])} ä¸ªç«å“")
        
        elif args.predict_trends:
            # è¶‹åŠ¿é¢„æµ‹
            result = manager.predict_keyword_trends(args.timeframe, args.output)
            print(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆ: é¢„æµ‹äº† {len(result['rising_keywords'])} ä¸ªä¸Šå‡å…³é”®è¯")
        
        elif args.seo_audit:
            # SEOå®¡è®¡
            if not args.domain:
                print("âŒ è¯·æŒ‡å®šè¦å®¡è®¡çš„åŸŸå (--domain)")
                return
            
            result = manager.generate_seo_audit(args.domain, args.keywords)
            print(f"âœ… SEOå®¡è®¡å®Œæˆ: å‘ç° {len(result['keyword_opportunities'])} ä¸ªå…³é”®è¯æœºä¼š")
        
        elif args.build_websites:
            # æ‰¹é‡å»ºç«™
            result = manager.batch_build_websites(args.top_keywords, args.output)
            print(f"âœ… æ‰¹é‡å»ºç«™å®Œæˆ: æˆåŠŸæ„å»º {result['successful_builds']} ä¸ªç½‘ç«™")
        
        elif args.schedule:
            # è®¾ç½®å®šæ—¶ä»»åŠ¡
            if not args.time or not args.action:
                print("âŒ è¯·æŒ‡å®šæ‰§è¡Œæ—¶é—´ (--time) å’ŒåŠ¨ä½œ (--action)")
                return
            
            manager.setup_scheduler(args.schedule, args.time, args.action)
            
            if args.run_scheduler:
                manager.run_scheduler()
        
        elif args.run_scheduler:
            # ä»…è¿è¡Œè°ƒåº¦å™¨
            manager.run_scheduler()
        
        else:
            # å›é€€åˆ°åŸºç¡€åŠŸèƒ½
            if args.input:
                result = manager.analyze_keywords(args.input, args.output)
                print(f"âœ… åˆ†æå®Œæˆ: {result['total_keywords']} ä¸ªå…³é”®è¯")
            elif args.discover is not None:
                search_terms = args.discover if args.discover else ['AI tool', 'AI generator']
                discoverer = MultiPlatformKeywordDiscovery()
                df = discoverer.discover_all_platforms(search_terms)
                if not df.empty:
                    analysis = discoverer.analyze_keyword_trends(df)
                    print(f"âœ… å‘ç°å®Œæˆ: {analysis['total_keywords']} ä¸ªå…³é”®è¯")
            else:
                print("è¯·æŒ‡å®šè¦æ‰§è¡Œçš„æ“ä½œï¼Œä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")
    
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()