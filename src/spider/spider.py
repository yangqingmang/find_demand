#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
lummstudio_spider.py
--------------------
1. è¯·æ±‚é¡µé¢ HTML
2. è§£ææ­£æ–‡ (article æ ‡ç­¾)
3. ä¸‹è½½æ­£æ–‡å†…å›¾ç‰‡ï¼Œæ›¿æ¢ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
4. å°† HTML è½¬ Markdownï¼Œè½ç›˜ä¿å­˜
"""

import re
import os
import time
import pathlib
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md

# ------------------ å…¨å±€é…ç½® ------------------ #
URLS = [
    "https://www.lummstudio.com/docs/20240802",
    "https://www.lummstudio.com/docs/20240801",
]

HEADERS = {
    "user-agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )
}

ROOT_DIR = pathlib.Path(__file__).resolve().parent
ASSET_DIR = ROOT_DIR / "assets"
MD_DIR = ROOT_DIR / "markdown"

ASSET_DIR.mkdir(exist_ok=True)
MD_DIR.mkdir(exist_ok=True)


# ------------------ å·¥å…·å‡½æ•° ------------------ #
def slugify(text: str) -> str:
    """å°†ä¸­æ–‡ / ç©ºæ ¼æ ‡é¢˜è½¬åŒ–ä¸ºæ–‡ä»¶å®‰å…¨çš„ slug."""
    text = re.sub(r"[^\w\s-]", "", text, flags=re.U).strip().lower()
    return re.sub(r"[-\s]+", "-", text)


def download_img(img_url: str) -> str:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ° assets/ ç›®å½•ï¼Œè¿”å›ä¿å­˜æ–‡ä»¶å"""
    # å¤„ç†ç›¸å¯¹åœ°å€
    if img_url.startswith("//"):
        img_url = "https:" + img_url
    elif img_url.startswith("/"):
        img_url = "https://www.lummstudio.com" + img_url

    # æ ¹æ® URL åç¼€å†³å®šæ–‡ä»¶å
    filename = slugify(pathlib.Path(img_url).name.split("?")[0])
    ext = pathlib.Path(filename).suffix or ".jpg"
    local_name = f"{int(time.time()*1000)}{ext}"
    local_path = ASSET_DIR / local_name

    # å·²å­˜åœ¨åˆ™è·³è¿‡
    if local_path.exists():
        return local_name

    try:
        resp = requests.get(img_url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        with open(local_path, "wb") as f:
            f.write(resp.content)
        print(f"[âœ“] Image saved: {local_name}")
        return local_name
    except Exception as e:
        print(f"[!] Fail download {img_url} -> {e}")
        return img_url  # å›é€€åˆ°åŸåœ°å€


def extract_article(soup: BeautifulSoup) -> BeautifulSoup:
    """
    Lumm Studio æ–‡ç« çš„æ­£æ–‡éƒ½åŒ…åœ¨ <article> æ ‡ç­¾é‡Œï¼Œ
    å¦‚æœç»“æ„æ”¹å˜ï¼Œå¯æŒ‰ class åæˆ– id é‡æ–°å®šä½ã€‚
    """
    article = soup.find("article")
    if not article:
        raise RuntimeError("æ­£æ–‡ <article> æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥é¡µé¢ç»“æ„ï¼")
    return article


def html_to_markdown(html_str: str) -> str:
    """HTML â†’ Markdownï¼›è®¾ç½®å„ç§è½¬æ¢ç»†èŠ‚"""
    return md(
        html_str,
        heading_style="ATX",
        strip=["span", "font"],
        # ä¿ç•™è¡¨æ ¼ / å¼•ç”¨ / å›¾ç‰‡
    )


# ------------------ ä¸»æµç¨‹ ------------------ #
for url in URLS:
    print(f"\n=== Crawling {url} ===")
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        r.raise_for_status()
    except Exception as e:
        print(f"[!] è¯·æ±‚å¤±è´¥: {e}")
        continue

    soup = BeautifulSoup(r.text, "lxml")

    # ------- 1. æå–æ ‡é¢˜ã€æ­£æ–‡ ------- #
    title_tag = soup.find(["h1", "title"])
    title = title_tag.get_text(strip=True) if title_tag else "untitled"
    slug = slugify(title)

    article = extract_article(soup)

    # ------- 2. ä¸‹è½½å¹¶æ›¿æ¢å›¾ç‰‡ ------- #
    for img in article.find_all("img"):
        src = img.get("src")
        if not src:
            continue
        local_name = download_img(src)
        # å°† src æ›¿æ¢ä¸ºç›¸å¯¹è·¯å¾„
        img["src"] = f"./assets/{local_name}"

    # ------- 3. HTML â†’ Markdown ------- #
    markdown_body = html_to_markdown(str(article))
    md_path = MD_DIR / f"{slug}.md"

    front_matter = (
        f"---\n"
        f"title: \"{title}\"\n"
        f"origin: {url}\n"
        f"date: {time.strftime('%Y-%m-%d')}\n"
        f"---\n\n"
    )

    with open(md_path, "w", encoding="utf-8") as md_file:
        md_file.write(front_matter + markdown_body)

    print(f"[âœ“] Markdown saved: {md_path.relative_to(ROOT_DIR)}")

print("\nğŸŒŸ All done!")
