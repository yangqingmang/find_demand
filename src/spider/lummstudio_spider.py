import requests
from bs4 import BeautifulSoup
import time
import json
import os
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime
import html
import hashlib
from pathlib import Path

class LummStudioSpider:
    def __init__(self):
        self.base_url = "https://www.lummstudio.com"
        self.start_url = "https://www.lummstudio.com/docs/seo/miniclass"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.scraped_urls = set()
        self.data = []
        
    def get_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            # æ›´æ–°è¯·æ±‚å¤´ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„å‹ç¼©è®¾ç½®
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Connection': 'keep-alive',
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def parse_article_links(self, html_content):
        """è§£ææ–‡ç« é“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« å¡ç‰‡é“¾æ¥ - å°è¯•å¤šç§é€‰æ‹©å™¨
        article_links = []
        
        # æ–¹æ³•1: åŸå§‹é€‰æ‹©å™¨
        article_links.extend(soup.find_all('a', class_='card padding--lg cardContainer_ob4h'))
        
        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«cardContainerçš„ç±»
        article_links.extend(soup.find_all('a', class_=lambda x: x and 'cardContainer' in x))
        
        # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰æŒ‡å‘/docs/çš„é“¾æ¥
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '')
            if href.startswith('/docs/') and 'miniclass' not in href:
                article_links.append(link)
        
        # å»é‡
        seen_urls = set()
        unique_links = []
        for link in article_links:
            href = link.get('href', '')
            if href and href not in seen_urls:
                seen_urls.add(href)
                unique_links.append(link)
        
        print(f"æ‰¾åˆ° {len(unique_links)} ä¸ªå€™é€‰é“¾æ¥")
        
        for link in unique_links:
            href = link.get('href')
            if href and href.startswith('/docs/'):
                full_url = urljoin(self.base_url, href)
                
                # è·å–æ ‡é¢˜ - å°è¯•å¤šç§æ–¹å¼
                title = ''
                title_elem = link.find('h2', class_=lambda x: x and 'cardTitle' in x) if link.find('h2') else None
                if title_elem:
                    title = title_elem.get('title', '') or title_elem.get_text()
                    title = title.replace('ğŸ“„ï¸', '').replace('<!-- -->', '').strip()
                else:
                    # å°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–æ ‡é¢˜
                    title = link.get_text().strip()
                
                # è·å–æ—¥æœŸ
                date = ''
                date_elem = link.find('p', class_=lambda x: x and 'cardDescription' in x) if link.find('p') else None
                if date_elem:
                    date_text = date_elem.get('title', '') or date_elem.get_text()
                    date = date_text.replace('æ—¥æœŸï¼š', '').strip()
                
                if title:  # åªæ·»åŠ æœ‰æ ‡é¢˜çš„é“¾æ¥
                    links.append({
                        'url': full_url,
                        'title': title,
                        'date': date
                    })
                    print(f"  - {title} ({date})")
        
        return links
    
    def parse_article_content(self, html_content, url):
        """è§£ææ–‡ç« å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # è·å–æ ‡é¢˜
        title_elem = soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else ''
        
        # è·å–ä¸»è¦å†…å®¹åŒºåŸŸ
        content_elem = soup.find('article') or soup.find('main') or soup.find('div', class_='markdown')
        
        if not content_elem:
            # å°è¯•å…¶ä»–å¯èƒ½çš„å†…å®¹å®¹å™¨
            content_elem = soup.find('div', {'class': re.compile(r'.*content.*|.*article.*|.*post.*')})
        
        content = ''
        if content_elem:
            # ç§»é™¤å¯¼èˆªå’Œå…¶ä»–ä¸ç›¸å…³å…ƒç´ 
            for elem in content_elem.find_all(['nav', 'aside', 'footer', 'header']):
                elem.decompose()
            
            # è·å–æ–‡æœ¬å†…å®¹
            content = content_elem.get_text(separator='\n', strip=True)
        
        # è·å–é¡µé¢æè¿°
        description = ''
        desc_elem = soup.find('meta', {'name': 'description'})
        if desc_elem:
            description = desc_elem.get('content', '')
        
        return {
            'url': url,
            'title': title,
            'description': description,
            'content': content,
            'scraped_at': datetime.now().isoformat()
        }
    
    def save_data(self, filename=None):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"lummstudio_data_{timestamp}.json"
        
        filepath = os.path.join('data', filename)
        os.makedirs('data', exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def save_markdown(self, filename=None):
        """ä¿å­˜ä¸ºMarkdownæ ¼å¼"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"lummstudio_articles_{timestamp}.md"
        
        filepath = os.path.join('src/spider/markdown', filename)
        os.makedirs('src/spider/markdown', exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("# å“¥é£å°è¯¾å ‚ - SEOæ–‡ç« åˆé›†\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            for item in self.data:
                f.write(f"## {item['title']}\n\n")
                f.write(f"**URL**: {item['url']}\n\n")
                if item.get('description'):
                    f.write(f"**æè¿°**: {item['description']}\n\n")
                f.write(f"**å†…å®¹**:\n\n{item['content']}\n\n")
                f.write("---\n\n")
        
        print(f"Markdownæ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def save_individual_html(self, article_data, html_content):
        """ä¿å­˜å•ä¸ªæ–‡ç« ä¸ºHTMLæ–‡ä»¶"""
        # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', article_data.get('original_title', article_data.get('title', 'untitled')))
        safe_title = safe_title[:100]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        
        # æ·»åŠ æ—¥æœŸå‰ç¼€
        date_prefix = article_data.get('date', '').replace('-', '')
        if date_prefix:
            filename = f"{date_prefix}_{safe_title}.html"
        else:
            filename = f"{safe_title}.html"
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = os.path.join('src/spider/html_pages')
        os.makedirs(save_dir, exist_ok=True)
        
        filepath = os.path.join(save_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§æ–‡ä»¶
        if os.path.exists(filepath):
            os.remove(filepath)
            print(f"åˆ é™¤æ—§æ–‡ä»¶: {filename}")
        
        # åˆ›å»ºå®Œæ•´çš„HTMLé¡µé¢
        full_html = self.create_complete_html(article_data, html_content)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(full_html)
        
        print(f"HTMLæ–‡ä»¶å·²ä¿å­˜: {filename}")
        return filepath
    
    def create_complete_html(self, article_data, original_html):
        """åˆ›å»ºå®Œæ•´çš„HTMLé¡µé¢"""
        soup = BeautifulSoup(original_html, 'html.parser')
        
        # æå–ä¸»è¦å†…å®¹
        main_content = soup.find('main') or soup.find('article') or soup.find('div', {'class': re.compile(r'.*content.*')})
        
        if not main_content:
            # å¦‚æœæ‰¾ä¸åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨æ•´ä¸ªbody
            main_content = soup.find('body') or soup
        
        # å¤„ç†å›¾ç‰‡ä¸‹è½½
        print(f"  æ­£åœ¨å¤„ç†å›¾ç‰‡...")
        main_content, failed_images = self.process_images_in_html(main_content, article_data.get('original_title', article_data.get('title', 'untitled')))
        
        # å°†å¤±è´¥çš„å›¾ç‰‡ä¿¡æ¯æ·»åŠ åˆ°æ–‡ç« æ•°æ®ä¸­
        if failed_images:
            article_data['failed_images'] = failed_images
        
        # åˆ›å»ºæ–°çš„HTMLæ–‡æ¡£
        html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{html.escape(article_data.get('title', ''))}</title>
    <meta name="description" content="{html.escape(article_data.get('description', ''))}">
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }}
        .article-header {{
            border-bottom: 2px solid #eee;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }}
        .article-title {{
            font-size: 2.5em;
            margin-bottom: 10px;
            color: #2c3e50;
        }}
        .article-meta {{
            color: #666;
            font-size: 0.9em;
        }}
        .article-content {{
            font-size: 1.1em;
        }}
        .article-content h1, .article-content h2, .article-content h3 {{
            color: #2c3e50;
            margin-top: 30px;
        }}
        .article-content p {{
            margin-bottom: 15px;
        }}
        .article-content code {{
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }}
        .article-content pre {{
            background-color: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }}
        .article-content blockquote {{
            border-left: 4px solid #ddd;
            margin: 20px 0;
            padding-left: 20px;
            color: #666;
        }}
        .article-content img {{
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .article-content figure {{
            margin: 20px 0;
            text-align: center;
        }}
        .article-content figure img {{
            max-width: 100%;
            height: auto;
        }}
        .source-info {{
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            color: #666;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="article-header">
        <h1 class="article-title">{html.escape(article_data.get('title', ''))}</h1>
        <div class="article-meta">
            <p><strong>åŸå§‹é“¾æ¥:</strong> <a href="{article_data.get('url', '')}" target="_blank">{article_data.get('url', '')}</a></p>
            {f'<p><strong>å‘å¸ƒæ—¥æœŸ:</strong> {article_data.get("date", "")}</p>' if article_data.get('date') else ''}
            {f'<p><strong>æè¿°:</strong> {html.escape(article_data.get("description", ""))}</p>' if article_data.get('description') else ''}
            <p><strong>çˆ¬å–æ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
    
    <div class="article-content">
        {str(main_content)}
    </div>
    
    <div class="source-info">
        <p>æœ¬æ–‡ç« æ¥æºäº: <a href="https://www.lummstudio.com" target="_blank">è·¯æ¼«æ¼« - LummStudio</a></p>
        <p>çˆ¬å–å·¥å…·: LummStudio Spider</p>
    </div>
</body>
</html>"""
        
        return html_template
    
    def download_image(self, img_url, save_dir, max_retries=3):
        """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        img_name = os.path.basename(parsed_url.path)
        if not img_name or '.' not in img_name:
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶åæˆ–æ‰©å±•åï¼Œä½¿ç”¨URLçš„hashå€¼
            img_hash = hashlib.md5(img_url.encode()).hexdigest()[:10]
            img_name = f"image_{img_hash}.jpg"
        
        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        img_name = re.sub(r'[<>:"/\\|?*]', '_', img_name)
        img_path = os.path.join(save_dir, img_name)
        
        # å¦‚æœå›¾ç‰‡å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if os.path.exists(img_path):
            return img_name, None
        
        # é‡è¯•ä¸‹è½½
        for attempt in range(max_retries):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Referer': 'https://www.lummstudio.com/'
                }
                
                response = requests.get(img_url, headers=headers, timeout=15, stream=True)
                response.raise_for_status()
                
                with open(img_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                print(f"  å›¾ç‰‡å·²ä¸‹è½½: {img_name}")
                return img_name, None
                
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"  å›¾ç‰‡ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {img_name}")
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    print(f"  å›¾ç‰‡ä¸‹è½½æœ€ç»ˆå¤±è´¥ {img_url}: {e}")
                    # è¿”å›å¤±è´¥ä¿¡æ¯ï¼ŒåŒ…å«URLå’Œå»ºè®®çš„æ–‡ä»¶å
                    return None, {'url': img_url, 'filename': img_name, 'path': img_path}
        
        return None, {'url': img_url, 'filename': img_name, 'path': img_path}
    
    def process_images_in_html(self, soup, article_title):
        """å¤„ç†HTMLä¸­çš„å›¾ç‰‡ï¼Œä¸‹è½½åˆ°æœ¬åœ°å¹¶æ›´æ–°é“¾æ¥"""
        # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', article_title)[:50]
        img_dir = os.path.join('src/spider/html_pages/images', safe_title)
        os.makedirs(img_dir, exist_ok=True)
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
        images = soup.find_all('img')
        downloaded_count = 0
        failed_images = []
        
        for img in images:
            src = img.get('src')
            if not src:
                continue
            
            # è½¬æ¢ä¸ºç»å¯¹URL
            if src.startswith('//'):
                img_url = 'https:' + src
            elif src.startswith('/'):
                img_url = urljoin(self.base_url, src)
            elif src.startswith('http'):
                img_url = src
            else:
                continue
            
            # ä¸‹è½½å›¾ç‰‡
            local_img_name, failed_info = self.download_image(img_url, img_dir)
            if local_img_name:
                # æ›´æ–°å›¾ç‰‡é“¾æ¥ä¸ºç›¸å¯¹è·¯å¾„
                relative_path = f"images/{safe_title}/{local_img_name}"
                img['src'] = relative_path
                downloaded_count += 1
            elif failed_info:
                # è®°å½•å¤±è´¥çš„å›¾ç‰‡ä¿¡æ¯
                failed_images.append(failed_info)
        
        if downloaded_count > 0:
            print(f"  å…±ä¸‹è½½ {downloaded_count} å¼ å›¾ç‰‡")
        
        # è¿”å›soupå’Œå¤±è´¥çš„å›¾ç‰‡åˆ—è¡¨
        return soup, failed_images
    
    def cleanup_old_html_files(self):
        """æ¸…ç†æ—§çš„HTMLæ–‡ä»¶å’Œå›¾ç‰‡"""
        save_dir = os.path.join('src/spider/html_pages')
        if os.path.exists(save_dir):
            # æ¸…ç†HTMLæ–‡ä»¶
            html_files = [f for f in os.listdir(save_dir) if f.endswith('.html') and f != 'index.html']
            if html_files:
                print(f"æ¸…ç† {len(html_files)} ä¸ªæ—§HTMLæ–‡ä»¶...")
                for file in html_files:
                    file_path = os.path.join(save_dir, file)
                    try:
                        os.remove(file_path)
                    except Exception as e:
                        print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file}: {e}")
            
            # æ¸…ç†å›¾ç‰‡ç›®å½•
            img_dir = os.path.join(save_dir, 'images')
            if os.path.exists(img_dir):
                print("æ¸…ç†æ—§å›¾ç‰‡æ–‡ä»¶...")
                import shutil
                try:
                    shutil.rmtree(img_dir)
                    print("å›¾ç‰‡ç›®å½•æ¸…ç†å®Œæˆ")
                except Exception as e:
                    print(f"æ¸…ç†å›¾ç‰‡ç›®å½•å¤±è´¥: {e}")
            
            if html_files:
                print("æ—§æ–‡ä»¶æ¸…ç†å®Œæˆ")
    
    def crawl(self):
        """å¼€å§‹çˆ¬å–"""
        print("å¼€å§‹çˆ¬å– LummStudio SEO å°è¯¾å ‚...")
        
        # æ¸…ç†æ—§çš„HTMLæ–‡ä»¶
        self.cleanup_old_html_files()
        
        # è·å–ä¸»é¡µé¢å†…å®¹
        print(f"æ­£åœ¨è·å–ä¸»é¡µé¢: {self.start_url}")
        main_content = self.get_page_content(self.start_url)
        if not main_content:
            print("æ— æ³•è·å–ä¸»é¡µé¢å†…å®¹")
            return
        
        # è§£ææ–‡ç« é“¾æ¥
        article_links = self.parse_article_links(main_content)
        print(f"æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« ")
        
        # çˆ¬å–æ¯ç¯‡æ–‡ç« 
        html_files = []
        all_failed_images = []  # æ”¶é›†æ‰€æœ‰å¤±è´¥çš„å›¾ç‰‡
        for i, link_info in enumerate(article_links, 1):
            url = link_info['url']
            if url in self.scraped_urls:
                continue
                
            print(f"æ­£åœ¨çˆ¬å– ({i}/{len(article_links)}): {link_info['title']}")
            
            content = self.get_page_content(url)
            if content:
                article_data = self.parse_article_content(content, url)
                article_data.update({
                    'original_title': link_info['title'],
                    'date': link_info['date']
                })
                self.data.append(article_data)
                self.scraped_urls.add(url)
                
                # ä¿å­˜å•ä¸ªHTMLæ–‡ä»¶
                html_file = self.save_individual_html(article_data, content)
                html_files.append(html_file)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(1)
            else:
                print(f"è·³è¿‡æ— æ³•è·å–çš„é¡µé¢: {url}")
        
        print(f"çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.data)} ç¯‡æ–‡ç« ")
        
        # ä¿å­˜æ•°æ®
        json_file = self.save_data()
        md_file = self.save_markdown()
        
        return {
            'total_articles': len(self.data),
            'json_file': json_file,
            'markdown_file': md_file,
            'html_files': html_files,
            'data': self.data
        }

def main():
    spider = LummStudioSpider()
    result = spider.crawl()
    
    print("\n=== çˆ¬å–ç»“æœ ===")
    print(f"æ€»æ–‡ç« æ•°: {result['total_articles']}")
    print(f"JSONæ–‡ä»¶: {result['json_file']}")
    print(f"Markdownæ–‡ä»¶: {result['markdown_file']}")

if __name__ == "__main__":
    main()