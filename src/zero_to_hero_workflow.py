#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä»é›¶å¼€å§‹çš„éœ€æ±‚æŒ–æ˜å·¥ä½œæµ
æ— éœ€é¢„å…ˆå‡†å¤‡å…³é”®è¯æ–‡ä»¶ï¼Œä»ç§å­è¯å¼€å§‹è‡ªåŠ¨æŒ–æ˜
"""

import os
import sys
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.demand_mining.demand_mining_main import DemandMiningManager
from src.integrated_workflow import IntegratedWorkflow


class ZeroToHeroWorkflow:
    """
    ä»é›¶å¼€å§‹çš„å®Œæ•´å·¥ä½œæµ
    åŸºäºè·¯æ¼«æ¼«åˆ†äº«çš„å…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•ï¼Œä»ç§å­è¯å¼€å§‹è‡ªåŠ¨å‘ç°éœ€æ±‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ä»é›¶å¼€å§‹å·¥ä½œæµ"""
        self.demand_miner = DemandMiningManager()
        self.output_dir = "output/zero_to_hero"
        self._ensure_output_dirs()
        
        print("ğŸŒ± ä»é›¶å¼€å§‹éœ€æ±‚æŒ–æ˜å·¥ä½œæµåˆå§‹åŒ–å®Œæˆ")
        print("ğŸ¯ æ”¯æŒä»ç§å­è¯è‡ªåŠ¨å‘ç°é«˜ä»·å€¼å…³é”®è¯")
    
    def _ensure_output_dirs(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'discovered_keywords'), exist_ok=True)
    
    def discover_keywords_from_seeds(self, seed_words: List[str], target_count: int = 50) -> str:
        """
        ä»ç§å­è¯å¼€å§‹å‘ç°å…³é”®è¯
        
        Args:
            seed_words: ç§å­å…³é”®è¯åˆ—è¡¨
            target_count: ç›®æ ‡å‘ç°çš„å…³é”®è¯æ•°é‡
            
        Returns:
            ç”Ÿæˆçš„å…³é”®è¯æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸŒ± å¼€å§‹ä» {len(seed_words)} ä¸ªç§å­è¯å‘ç°å…³é”®è¯...")
        print(f"ğŸ¯ ç›®æ ‡å‘ç° {target_count} ä¸ªå…³é”®è¯")
        
        discovered_keywords = []
        
        # æ–¹æ³•ä¸€ï¼šåŸºäºè¯æ ¹å…³é”®è¯æ‹“å±•
        print("\nğŸ“Š æ–¹æ³•ä¸€ï¼šåŸºäºè¯æ ¹å…³é”®è¯æ‹“å±•")
        root_keywords = self._expand_from_roots(seed_words)
        discovered_keywords.extend(root_keywords)
        print(f"   å‘ç° {len(root_keywords)} ä¸ªè¯æ ¹æ‹“å±•å…³é”®è¯")
        
        # æ–¹æ³•äºŒï¼šåŸºäºAIå‰ç¼€ç»„åˆ
        print("\nğŸ¤– æ–¹æ³•äºŒï¼šåŸºäºAIå‰ç¼€ç»„åˆ")
        ai_keywords = self._generate_ai_combinations(seed_words)
        discovered_keywords.extend(ai_keywords)
        print(f"   å‘ç° {len(ai_keywords)} ä¸ªAIç»„åˆå…³é”®è¯")
        
        # æ–¹æ³•ä¸‰ï¼šåŸºäºç«å“åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰
        print("\nğŸ” æ–¹æ³•ä¸‰ï¼šåŸºäºç«å“åˆ†æ")
        competitor_keywords = self._analyze_competitors(seed_words)
        discovered_keywords.extend(competitor_keywords)
        print(f"   å‘ç° {len(competitor_keywords)} ä¸ªç«å“å…³é”®è¯")
        
        # æ–¹æ³•å››ï¼šåŸºäºæœç´¢å»ºè®®ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("\nğŸ’¡ æ–¹æ³•å››ï¼šåŸºäºæœç´¢å»ºè®®")
        suggestion_keywords = self._get_search_suggestions(seed_words)
        discovered_keywords.extend(suggestion_keywords)
        print(f"   å‘ç° {len(suggestion_keywords)} ä¸ªæœç´¢å»ºè®®å…³é”®è¯")
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_keywords = list(set(discovered_keywords))[:target_count]
        
        # ç”Ÿæˆå…³é”®è¯æ–‡ä»¶
        keywords_file = self._save_discovered_keywords(unique_keywords)
        
        print(f"\nâœ… å…³é”®è¯å‘ç°å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å‘ç° {len(unique_keywords)} ä¸ªç‹¬ç‰¹å…³é”®è¯")
        print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {keywords_file}")
        
        return keywords_file
    
    def _expand_from_roots(self, seed_words: List[str]) -> List[str]:
        """åŸºäºæ ¸å¿ƒè¯æ ¹æ‹“å±•å…³é”®è¯"""
        expanded = []
        
        # ä½¿ç”¨éœ€æ±‚æŒ–æ˜ç®¡ç†å™¨çš„æ ¸å¿ƒè¯æ ¹
        core_roots = self.demand_miner.core_roots
        ai_prefixes = self.demand_miner.ai_prefixes
        
        for seed in seed_words:
            for root in core_roots[:20]:  # ä½¿ç”¨å‰20ä¸ªè¯æ ¹
                # ç»„åˆæ–¹å¼1: seed + root
                expanded.append(f"{seed} {root}")
                # ç»„åˆæ–¹å¼2: root + seed  
                expanded.append(f"{root} {seed}")
                
                # AIç›¸å…³ç»„åˆ
                for ai_prefix in ai_prefixes[:3]:  # ä½¿ç”¨å‰3ä¸ªAIå‰ç¼€
                    expanded.append(f"{ai_prefix} {seed} {root}")
        
        return expanded
    
    def _generate_ai_combinations(self, seed_words: List[str]) -> List[str]:
        """ç”ŸæˆAIç›¸å…³å…³é”®è¯ç»„åˆ"""
        ai_combinations = []
        
        ai_prefixes = ['ai', 'artificial intelligence', 'machine learning', 'automated']
        ai_suffixes = ['tool', 'generator', 'assistant', 'maker', 'creator', 'builder']
        modifiers = ['free', 'online', 'best', 'professional', 'advanced']
        
        for seed in seed_words:
            for prefix in ai_prefixes:
                for suffix in ai_suffixes:
                    # åŸºæœ¬ç»„åˆ
                    ai_combinations.append(f"{prefix} {seed} {suffix}")
                    
                    # å¸¦ä¿®é¥°è¯çš„ç»„åˆ
                    for modifier in modifiers[:2]:  # åªç”¨å‰2ä¸ªä¿®é¥°è¯
                        ai_combinations.append(f"{modifier} {prefix} {seed} {suffix}")
        
        return ai_combinations
    
    def _analyze_competitors(self, seed_words: List[str]) -> List[str]:
        """åˆ†æç«å“å…³é”®è¯ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        competitor_keywords = []
        
        # æ¨¡æ‹Ÿç«å“åˆ†æç»“æœ
        competitor_patterns = [
            "{seed} alternative",
            "{seed} vs",
            "best {seed} tool",
            "{seed} comparison",
            "free {seed} online",
            "{seed} without registration",
            "{seed} no watermark",
            "professional {seed}",
            "{seed} for business",
            "{seed} api"
        ]
        
        for seed in seed_words:
            for pattern in competitor_patterns:
                competitor_keywords.append(pattern.format(seed=seed))
        
        return competitor_keywords
    
    def _get_search_suggestions(self, seed_words: List[str]) -> List[str]:
        """è·å–æœç´¢å»ºè®®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        suggestions = []
        
        # æ¨¡æ‹Ÿæœç´¢å»ºè®®æ¨¡å¼
        suggestion_patterns = [
            "how to {seed}",
            "{seed} tutorial",
            "{seed} step by step",
            "learn {seed}",
            "{seed} for beginners",
            "{seed} tips",
            "{seed} examples",
            "{seed} guide",
            "{seed} best practices",
            "why use {seed}"
        ]
        
        for seed in seed_words:
            for pattern in suggestion_patterns:
                suggestions.append(pattern.format(seed=seed))
        
        return suggestions
    
    def _save_discovered_keywords(self, keywords: List[str]) -> str:
        """ä¿å­˜å‘ç°çš„å…³é”®è¯åˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"discovered_keywords_{timestamp}.csv"
        filepath = os.path.join(self.output_dir, 'discovered_keywords', filename)
        
        # åˆ›å»ºDataFrameå¹¶æ·»åŠ æ¨¡æ‹Ÿæ•°æ®
        df_data = []
        for i, keyword in enumerate(keywords):
            # æ¨¡æ‹Ÿæœç´¢é‡ã€ç«äº‰åº¦å’ŒCPCæ•°æ®
            search_volume = max(100, 10000 - i * 100)  # é€’å‡çš„æœç´¢é‡
            competition = min(0.9, 0.2 + i * 0.01)     # é€’å¢çš„ç«äº‰åº¦
            cpc = max(0.5, 3.0 - i * 0.05)             # é€’å‡çš„CPC
            
            df_data.append({
                'query': keyword,
                'search_volume': search_volume,
                'competition': round(competition, 2),
                'cpc': round(cpc, 2)
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        return filepath
    
    def run_complete_discovery_workflow(self, seed_words: List[str], target_count: int = 30) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„ä»é›¶å¼€å§‹å‘ç°å·¥ä½œæµ
        
        Args:
            seed_words: ç§å­å…³é”®è¯
            target_count: ç›®æ ‡å…³é”®è¯æ•°é‡
            
        Returns:
            å®Œæ•´å·¥ä½œæµç»“æœ
        """
        print("ğŸš€ å¼€å§‹ä»é›¶åˆ°è‹±é›„çš„å®Œæ•´å·¥ä½œæµ")
        print("=" * 50)
        
        workflow_results = {
            'start_time': datetime.now().isoformat(),
            'seed_words': seed_words,
            'target_count': target_count,
            'steps_completed': [],
            'status': 'running'
        }
        
        try:
            # æ­¥éª¤1: ä»ç§å­è¯å‘ç°å…³é”®è¯
            print(f"ğŸŒ± æ­¥éª¤1: ä»ç§å­è¯å‘ç°å…³é”®è¯")
            keywords_file = self.discover_keywords_from_seeds(seed_words, target_count)
            workflow_results['steps_completed'].append('keyword_discovery')
            workflow_results['keywords_file'] = keywords_file
            
            # æ­¥éª¤2: è¿è¡Œé›†æˆå·¥ä½œæµ
            print(f"\nğŸ”„ æ­¥éª¤2: è¿è¡Œé›†æˆåˆ†æå’Œå»ºç«™å·¥ä½œæµ")
            integrated_workflow = IntegratedWorkflow({
                'min_opportunity_score': 50,  # é™ä½é˜ˆå€¼ä»¥ä¾¿æ¼”ç¤º
                'max_projects_per_batch': 3,
                'auto_deploy': False
            })
            
            integrated_results = integrated_workflow.run_complete_workflow(keywords_file)
            workflow_results['steps_completed'].append('integrated_workflow')
            workflow_results['integrated_results'] = integrated_results
            
            workflow_results['end_time'] = datetime.now().isoformat()
            workflow_results['status'] = 'success'
            
            print(f"\nğŸ‰ ä»é›¶åˆ°è‹±é›„å·¥ä½œæµå®Œæˆï¼")
            print(f"ğŸŒ± ç§å­è¯æ•°é‡: {len(seed_words)}")
            print(f"ğŸ“Š å‘ç°å…³é”®è¯: {target_count}")
            print(f"ğŸ¯ é«˜ä»·å€¼å…³é”®è¯: {len(integrated_results.get('high_value_keywords', []))}")
            print(f"ğŸ—ï¸ ç”Ÿæˆç½‘ç«™: {len(integrated_results.get('generated_projects', []))}")
            
            return workflow_results
            
        except Exception as e:
            workflow_results['end_time'] = datetime.now().isoformat()
            workflow_results['status'] = 'failed'
            workflow_results['error'] = str(e)
            print(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return workflow_results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ± ä»é›¶å¼€å§‹éœ€æ±‚æŒ–æ˜æ¼”ç¤º")
    print("åŸºäºè·¯æ¼«æ¼«åˆ†äº«çš„å…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•")
    print("=" * 50)
    
    # å®šä¹‰ç§å­è¯ï¼ˆä½ çš„ç›®æ ‡é¢†åŸŸï¼‰
    seed_words = [
        'image',      # å›¾åƒå¤„ç†
        'text',       # æ–‡æœ¬å¤„ç†  
        'video',      # è§†é¢‘å¤„ç†
        'code',       # ä»£ç å·¥å…·
        'pdf'         # æ–‡æ¡£å¤„ç†
    ]
    
    print(f"ğŸ¯ ç§å­è¯: {', '.join(seed_words)}")
    print(f"ğŸ¯ ç›®æ ‡: å‘ç°AIå·¥å…·ç›¸å…³çš„é«˜ä»·å€¼å…³é”®è¯")
    
    try:
        # åˆ›å»ºä»é›¶å¼€å§‹å·¥ä½œæµ
        workflow = ZeroToHeroWorkflow()
        
        # è¿è¡Œå®Œæ•´å‘ç°æµç¨‹
        results = workflow.run_complete_discovery_workflow(
            seed_words=seed_words,
            target_count=25  # å‘ç°25ä¸ªå…³é”®è¯ç”¨äºæ¼”ç¤º
        )
        
        if results['status'] == 'success':
            print(f"\nâœ… æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ å…³é”®è¯æ–‡ä»¶: {results.get('keywords_file', '')}")
            
            integrated_results = results.get('integrated_results', {})
            if integrated_results.get('report_path'):
                print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {integrated_results['report_path']}")
        else:
            print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {results.get('error', '')}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}")
        return 1


if __name__ == '__main__':
    import sys
    sys.exit(main())