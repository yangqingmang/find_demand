#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éœ€æ±‚æŒ–æ˜ â†’ æ„å›¾åˆ†æ â†’ å»ºç«™éƒ¨ç½² å®Œæ•´é›†æˆå·¥ä½œæµ
æ•´åˆä¸‰å¤§æ ¸å¿ƒæ¨¡å—ï¼Œå®ç°ä»éœ€æ±‚å‘ç°åˆ°ç½‘ç«™ä¸Šçº¿çš„å…¨è‡ªåŠ¨åŒ–æµç¨‹
"""

import os
import sys
import json
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.demand_mining.managers.keyword_manager import KeywordManager
from src.website_builder.builder_core import IntentBasedWebsiteBuilder
from src.demand_mining.analyzers.new_word_detector import NewWordDetector
from src.demand_mining.managers.discovery_manager import DiscoveryManager


class IntegratedWorkflow:
    """
    é›†æˆå·¥ä½œæµç®¡ç†å™¨
    å®ç°éœ€æ±‚æŒ–æ˜ â†’ æ„å›¾åˆ†æ â†’ ç½‘ç«™å»ºè®¾ â†’ è‡ªåŠ¨éƒ¨ç½²çš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–é›†æˆå·¥ä½œæµ"""
        self.config = config or self._get_default_config()
        self.output_base_dir = "output/integrated_projects"
        self._ensure_output_dirs()
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.demand_miner = KeywordManager()
        self.discovery_manager = DiscoveryManager()
        
        print("ğŸš€ é›†æˆå·¥ä½œæµåˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“Š æ”¯æŒåŠŸèƒ½ï¼šéœ€æ±‚æŒ–æ˜ â†’ å¤šå¹³å°å…³é”®è¯å‘ç° â†’ æ„å›¾åˆ†æ â†’ ç½‘ç«™ç”Ÿæˆ â†’ è‡ªåŠ¨éƒ¨ç½²")

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'min_opportunity_score': 50,  # æœ€ä½æœºä¼šåˆ†æ•°é˜ˆå€¼ï¼ˆæ”¾å®½ä»¥æ•è·æ›´å¤šæ½œåŠ›é•¿å°¾ï¼‰
            'max_projects_per_batch': 5,  # æ¯æ‰¹æ¬¡æœ€å¤§é¡¹ç›®æ•°
            'auto_deploy': True,          # æ˜¯å¦è‡ªåŠ¨éƒ¨ç½²
            'deployment_platform': 'cloudflare',  # éƒ¨ç½²å¹³å°
            'use_tailwind': True,         # ä½¿ç”¨TailwindCSS
            'generate_reports': True      # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        }
    
    def _ensure_output_dirs(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        dirs = [
            self.output_base_dir,
            os.path.join(self.output_base_dir, 'demand_analysis'),
            os.path.join(self.output_base_dir, 'multi_platform_keywords'),
            os.path.join(self.output_base_dir, 'intent_analysis'),
            os.path.join(self.output_base_dir, 'websites'),
            os.path.join(self.output_base_dir, 'reports')
        ]
        
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
    
    def run_complete_workflow(self, keywords_file: str) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            keywords_file: å…³é”®è¯è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå®Œæ•´å·¥ä½œæµ: {keywords_file}")
        
        workflow_results = {
            'start_time': datetime.now().isoformat(),
            'input_file': keywords_file,
            'steps_completed': [],
            'generated_projects': [],
            'deployment_results': [],
            'summary': {}
        }
        
        try:
            # æ­¥éª¤1: éœ€æ±‚æŒ–æ˜ä¸æ„å›¾åˆ†æ
            print("\nğŸ“Š æ­¥éª¤1: æ‰§è¡Œéœ€æ±‚æŒ–æ˜ä¸æ„å›¾åˆ†æ...")
            demand_results = self._run_demand_mining(keywords_file)
            workflow_results['steps_completed'].append('demand_mining')
            workflow_results['demand_analysis'] = demand_results
            self._print_new_word_summary(demand_results.get('new_word_summary'))
            self._print_top_new_words(demand_results)
            
            # æ­¥éª¤2: å¤šå¹³å°å…³é”®è¯å‘ç°
            print("\nğŸ” æ­¥éª¤2: æ‰§è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°...")
            # ä»éœ€æ±‚æŒ–æ˜ç»“æœä¸­æå–å…³é”®è¯
            initial_keywords = [kw['keyword'] for kw in demand_results.get('keywords', [])[:10]]
            if not initial_keywords and 'keywords' in demand_results:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é”®å
                for key in ['query', 'term']:
                    if key in demand_results['keywords'][0]:
                        initial_keywords = [kw[key] for kw in demand_results.get('keywords', [])[:10]]
                        break
            
            # å¦‚æœä»ç„¶æ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯
            if not initial_keywords:
                seeds_cfg = self.config.get('discovery_seeds', {}) if isinstance(self.config, dict) else {}
                fallback_profile = seeds_cfg.get('default_profile')
                seed_limit = seeds_cfg.get('min_terms')
                discoverer = self.discovery_manager.discoverer
                if discoverer is None:
                    from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery
                    discoverer = MultiPlatformKeywordDiscovery()
                initial_keywords = discoverer.get_seed_terms(profile=fallback_profile, limit=seed_limit)
                if initial_keywords:
                    print(f"âš ï¸ æœªä»éœ€æ±‚æŒ–æ˜ç»“æœä¸­æ‰¾åˆ°å…³é”®è¯ï¼Œä½¿ç”¨é…ç½®ç§å­: {', '.join(initial_keywords)}")
                else:
                    initial_keywords = ['AI tool', 'AI generator', 'AI writer']
                    print("âš ï¸ æœªä»éœ€æ±‚æŒ–æ˜ç»“æœä¸­æ‰¾åˆ°å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯")
            
            # æ‰§è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°
            discovery_results = self._run_multi_platform_discovery(initial_keywords)
            workflow_results['steps_completed'].append('multi_platform_discovery')
            workflow_results['multi_platform_discovery'] = discovery_results
            
            # æ­¥éª¤3: ç­›é€‰é«˜ä»·å€¼å…³é”®è¯
            print("\nğŸ¯ æ­¥éª¤3: ç­›é€‰é«˜ä»·å€¼å…³é”®è¯...")
            high_value_keywords = self._filter_high_value_keywords(demand_results)
            workflow_results['steps_completed'].append('keyword_filtering')
            workflow_results['high_value_keywords'] = high_value_keywords
            
            # æ­¥éª¤3: æ‰¹é‡ç”Ÿæˆç½‘ç«™
            print("\nğŸ—ï¸ æ­¥éª¤3: æ‰¹é‡ç”Ÿæˆç½‘ç«™...")
            website_results = self._batch_generate_websites(high_value_keywords)
            workflow_results['steps_completed'].append('website_generation')
            workflow_results['generated_projects'] = website_results
            
            # æ­¥éª¤4: è‡ªåŠ¨éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰
            if self.config.get('auto_deploy', False):
                print("\nğŸš€ æ­¥éª¤4: è‡ªåŠ¨éƒ¨ç½²ç½‘ç«™...")
                deployment_results = self._batch_deploy_websites(website_results)
                workflow_results['steps_completed'].append('deployment')
                workflow_results['deployment_results'] = deployment_results
            
            # æ­¥éª¤5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            print("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
            report_path = self._generate_workflow_report(workflow_results)
            workflow_results['steps_completed'].append('report_generation')
            workflow_results['report_path'] = report_path
            
            workflow_results['end_time'] = datetime.now().isoformat()
            workflow_results['status'] = 'success'
            
            print(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼")
            print(f"ğŸ“Š åˆ†æäº† {len(demand_results.get('keywords', []))} ä¸ªå…³é”®è¯")
            print(f"ğŸ¯ ç­›é€‰å‡º {len(high_value_keywords)} ä¸ªé«˜ä»·å€¼å…³é”®è¯")
            print(f"ğŸ—ï¸ ç”Ÿæˆäº† {len(website_results)} ä¸ªç½‘ç«™é¡¹ç›®")
            print(f"ğŸ“‹ æŠ¥å‘Šè·¯å¾„: {report_path}")
            
        return workflow_results

    @staticmethod
    def _print_new_word_summary(summary: Optional[Dict[str, Any]]) -> None:
        if not summary or not isinstance(summary, dict):
            return

        total = summary.get('total_analyzed')
        detected = summary.get('new_words_detected')
        high_conf = summary.get('high_confidence_new_words')
        breakout = summary.get('breakout_keywords')
        rising = summary.get('rising_keywords')
        percentage = summary.get('new_word_percentage')

        print("\nğŸ” æ–°è¯æ£€æµ‹æ‘˜è¦ï¼š")
        print(f"   â€¢ æ£€æµ‹æ€»æ•°: {total}")
        print(f"   â€¢ æ–°è¯æ•°é‡: {detected} / é«˜ç½®ä¿¡åº¦: {high_conf}")
        if breakout is not None or rising is not None:
            print(f"   â€¢ Breakout: {breakout} / Rising: {rising}")
        if percentage is not None:
            print(f"   â€¢ æ–°è¯å æ¯”: {percentage}%")

        report_files = summary.get('report_files')
        if isinstance(report_files, dict) and report_files:
            print("   â€¢ å¯¼å‡ºæ–‡ä»¶:")
            for label, path in report_files.items():
                print(f"     - {label}: {path}")

    @staticmethod
    def _print_top_new_words(result: Optional[Dict[str, Any]], limit: int = 5) -> None:
        if not result or 'keywords' not in result:
            return

        candidates = []
        for item in result['keywords']:
            nwd = item.get('new_word_detection') if isinstance(item, dict) else None
            if not nwd or not nwd.get('is_new_word'):
                continue
            candidates.append({
                'keyword': item.get('keyword') or item.get('query'),
                'score': float(nwd.get('new_word_score', 0.0) or 0.0),
                'momentum': nwd.get('trend_momentum'),
                'delta': float(nwd.get('avg_7d_delta', 0.0) or 0.0),
                'grade': nwd.get('new_word_grade', 'D'),
                'confidence': nwd.get('confidence_level', 'low')
            })

        if not candidates:
            return

        candidates.sort(key=lambda x: (x['momentum'] == 'breakout', x['score']), reverse=True)
        print("\nğŸ”¥ Top æ–°è¯å€™é€‰:")
        for idx, item in enumerate(candidates[:limit], 1):
            print(
                f"   {idx}. {item['keyword']} | åˆ†æ•° {item['score']:.1f} | åŠ¨é‡ {item['momentum']} | "
                f"Î”7D {item['delta']:.1f} | ç­‰çº§ {item['grade']} | ç½®ä¿¡åº¦ {item['confidence']}"
            )
            
        except Exception as e:
            workflow_results['end_time'] = datetime.now().isoformat()
            workflow_results['status'] = 'failed'
            workflow_results['error'] = str(e)
            print(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return workflow_results
    
    def _run_multi_platform_discovery(self, initial_keywords: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°"""
        output_dir = os.path.join(self.output_base_dir, 'multi_platform_keywords')
        
        try:
            print(f"ğŸ” å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°ï¼ŒåŸºäº {len(initial_keywords)} ä¸ªåˆå§‹å…³é”®è¯...")
            print(f"ğŸ“Š åˆå§‹å…³é”®è¯: {', '.join(initial_keywords[:5])}{'...' if len(initial_keywords) > 5 else ''}")
            
            # ä½¿ç”¨å‘ç°ç®¡ç†å™¨æ‰§è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°
            seeds_config = self.config.get('discovery_seeds', {}) if isinstance(self.config, dict) else {}
            default_profile = seeds_config.get('default_profile')
            min_terms = seeds_config.get('min_terms')
            discovery_results = self.discovery_manager.analyze(
                search_terms=initial_keywords,
                output_dir=output_dir,
                seed_profile=default_profile,
                min_terms=min_terms
            )
            
            # å¦‚æœå‘ç°äº†å…³é”®è¯ï¼Œæ˜¾ç¤ºæ‘˜è¦
            if discovery_results and 'total_keywords' in discovery_results and discovery_results['total_keywords'] > 0:
                print(f"âœ… å¤šå¹³å°å…³é”®è¯å‘ç°å®Œæˆï¼Œå‘ç° {discovery_results['total_keywords']} ä¸ªå…³é”®è¯")
                
                # æ˜¾ç¤ºå¹³å°åˆ†å¸ƒ
                if 'platform_distribution' in discovery_results:
                    platforms = discovery_results['platform_distribution']
                    print(f"ğŸ“Š å¹³å°åˆ†å¸ƒ: {', '.join([f'{p}({c})' for p, c in platforms.items()])}")
                
                # æ˜¾ç¤ºçƒ­é—¨å…³é”®è¯
                if 'top_keywords_by_score' in discovery_results and discovery_results['top_keywords_by_score']:
                    print("ğŸ† çƒ­é—¨å…³é”®è¯:")
                    for i, kw in enumerate(discovery_results['top_keywords_by_score'][:5], 1):
                        print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            else:
                print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯")
            
            return discovery_results
            
        except Exception as e:
            print(f"âŒ å¤šå¹³å°å…³é”®è¯å‘ç°å¤±è´¥: {e}")
            return {
                'error': str(e),
                'total_keywords': 0,
                'platform_distribution': {},
                'top_keywords_by_score': []
            }
    
    def _run_demand_mining(self, keywords_file: str) -> Dict[str, Any]:
        """æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æï¼ˆåŒ…å«æ–°è¯æ£€æµ‹ï¼‰"""
        output_dir = os.path.join(self.output_base_dir, 'demand_analysis')
        
        # æ‰§è¡ŒåŸºç¡€éœ€æ±‚æŒ–æ˜
        demand_results = self.demand_miner.analyze_keywords(keywords_file, output_dir)
        
        # æ·»åŠ æ–°è¯æ£€æµ‹
        try:
            print("ğŸ” æ­£åœ¨è¿›è¡Œæ–°è¯æ£€æµ‹...")
            
            # è¯»å–å…³é”®è¯æ•°æ®
            import pandas as pd
            df = pd.read_csv(keywords_file)
            
            # æ‰§è¡Œæ–°è¯æ£€æµ‹
        try:
            from src.demand_mining.analyzers.new_word_detector_singleton import get_new_word_detector
            new_word_detector = get_new_word_detector()
            new_word_results = new_word_detector.detect_new_words(df)
            
            # å°†æ–°è¯æ£€æµ‹ç»“æœåˆå¹¶åˆ°éœ€æ±‚æŒ–æ˜ç»“æœä¸­
            if 'keywords' in demand_results:
                for i, keyword_data in enumerate(demand_results['keywords']):
                    if i < len(new_word_results):
                        # æ·»åŠ æ–°è¯æ£€æµ‹ä¿¡æ¯
                        row = new_word_results.iloc[i]
                        keyword_data['new_word_detection'] = {
                            'is_new_word': bool(row.get('is_new_word', False)),
                            'new_word_score': float(row.get('new_word_score', 0)),
                            'new_word_grade': str(row.get('new_word_grade', 'D')),
                            'growth_rate_7d': float(row.get('growth_rate_7d', 0)),
                            'growth_rate_7d_vs_30d': float(row.get('growth_rate_7d_vs_30d', 0)),
                            'mom_growth': float(row.get('mom_growth', 0)),
                            'yoy_growth': float(row.get('yoy_growth', 0)),
                            'explosion_index': float(row.get('explosion_index', 1.0)),
                            'confidence_level': str(row.get('confidence_level', 'low')),
                            'historical_pattern': str(row.get('historical_pattern', 'unknown')),
                            'trend_momentum': str(row.get('trend_momentum', 'unknown')),
                            'growth_label': str(row.get('growth_label', 'unknown')),
                            'trend_fetch_timeframe': row.get('trend_fetch_timeframe'),
                            'trend_fetch_geo': row.get('trend_fetch_geo'),
                            'avg_30d_delta': float(row.get('avg_30d_delta', 0.0) or 0.0),
                            'avg_7d_delta': float(row.get('avg_7d_delta', 0.0) or 0.0),
                            'detection_reasons': str(row.get('detection_reasons', ''))
                        }
                        
                        # å¦‚æœæ˜¯æ–°è¯ï¼Œå¢åŠ æœºä¼šåˆ†æ•°åŠ æˆ
                        if row.get('is_new_word', False):
                            original_score = keyword_data.get('opportunity_score', 0)
                            new_word_bonus = row.get('new_word_score', 0) * 0.1  # 10%åŠ æˆ
                            keyword_data['opportunity_score'] = min(100, original_score + new_word_bonus)
                            keyword_data['new_word_bonus'] = new_word_bonus
            
            # ç”Ÿæˆæ–°è¯æ£€æµ‹æ‘˜è¦
            new_words_count = len(new_word_results[new_word_results['is_new_word'] == True])
            high_confidence_count = len(new_word_results[new_word_results['confidence_level'] == 'high'])
            
            momentum_counts = new_word_results.get('trend_momentum', pd.Series(dtype=str)).value_counts() if 'trend_momentum' in new_word_results.columns else {}
            breakout_count = int(momentum_counts.get('breakout', 0)) if momentum_counts is not None else 0
            rising_count = int(momentum_counts.get('rising', 0)) if momentum_counts is not None else 0

            demand_results['new_word_summary'] = {
                'total_analyzed': len(new_word_results),
                'new_words_detected': new_words_count,
                'high_confidence_new_words': high_confidence_count,
                'new_word_percentage': round(new_words_count / len(new_word_results) * 100, 1) if len(new_word_results) > 0 else 0,
                'breakout_keywords': breakout_count,
                'rising_keywords': rising_count
            }

            exported_reports = self._export_new_word_reports(new_word_results)
            if exported_reports:
                demand_results['new_word_summary']['report_files'] = exported_reports

            print(
                f"âœ… æ–°è¯æ£€æµ‹å®Œæˆ: å‘ç° {new_words_count} ä¸ªæ–°è¯ "
                f"({high_confidence_count} ä¸ªé«˜ç½®ä¿¡åº¦, {demand_results['new_word_summary']['breakout_keywords']} ä¸ª Breakout)"
            )
            
        except Exception as e:
            print(f"âš ï¸ æ–°è¯æ£€æµ‹å¤±è´¥: {e}")
            demand_results['new_word_summary'] = {
                'error': str(e),
                'new_words_detected': 0
            }
        
        return demand_results
    
    def _filter_high_value_keywords(self, demand_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç­›é€‰é«˜ä»·å€¼å…³é”®è¯"""
        min_score = self.config.get('min_opportunity_score', 60)
        max_projects = self.config.get('max_projects_per_batch', 5)
        
        # è·å–æ‰€æœ‰å…³é”®è¯
        all_keywords = demand_results.get('keywords', [])
        
        # æŒ‰æœºä¼šåˆ†æ•°ç­›é€‰å’Œæ’åº
        high_value = [
            kw for kw in all_keywords 
            if kw.get('opportunity_score', 0) >= min_score
        ]
        
        # æŒ‰åˆ†æ•°é™åºæ’åºï¼Œå–å‰Nä¸ª
        high_value.sort(key=lambda x: x.get('opportunity_score', 0), reverse=True)
        
        return high_value[:max_projects]
    
    def _batch_generate_websites(self, keywords: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆç½‘ç«™ï¼ˆåŸºäºå»ºç«™å»ºè®®ï¼‰"""
        website_results = []
        
        for i, keyword_data in enumerate(keywords, 1):
            keyword = keyword_data['keyword']
            intent_info = keyword_data.get('intent', {})
            website_recommendations = intent_info.get('website_recommendations', {})
            
            print(f"ğŸ—ï¸ ç”Ÿæˆç½‘ç«™ ({i}/{len(keywords)}): {keyword}")
            
            # æ˜¾ç¤ºå»ºç«™å»ºè®®ä¿¡æ¯
            website_type = website_recommendations.get('website_type', 'æœªçŸ¥')
            ai_category = website_recommendations.get('ai_tool_category', 'æœªçŸ¥')
            development_priority = website_recommendations.get('development_priority', {})
            priority_level = development_priority.get('level', 'æœªçŸ¥') if isinstance(development_priority, dict) else 'æœªçŸ¥'
            
            print(f"   æ¨èç½‘ç«™ç±»å‹: {website_type}")
            print(f"   AIå·¥å…·ç±»åˆ«: {ai_category}")
            print(f"   å¼€å‘ä¼˜å…ˆçº§: {priority_level}")
            
            try:
                # å‡†å¤‡æ„å›¾æ•°æ®æ–‡ä»¶
                intent_data = self._prepare_intent_data(keyword_data)
                intent_file_path = self._save_intent_data(intent_data, keyword)
                
                # ç”Ÿæˆé¡¹ç›®åç§°ï¼ˆåŸºäºå»ºç«™å»ºè®®ï¼‰
                project_name = self._generate_project_name_with_recommendations(keyword, website_recommendations)
                
                # åˆ›å»ºé¡¹ç›®é…ç½®ï¼ˆåŸºäºå»ºç«™å»ºè®®ï¼‰
                project_config = self._create_project_config(website_recommendations, project_name)
                
                # åˆ›å»ºç½‘ç«™å»ºè®¾å™¨
                builder = IntentBasedWebsiteBuilder(
                    intent_data_path=intent_file_path,
                    output_dir=os.path.join(self.output_base_dir, 'websites'),
                    config=project_config
                )
                
                # æ‰§è¡Œå»ºç«™æµç¨‹
                if builder.load_intent_data():
                    structure = builder.generate_website_structure()
                    content_plan = builder.create_content_plan()
                    source_dir = builder.generate_website_source()
                    
                    if source_dir:
                        website_results.append({
                            'keyword': keyword,
                            'project_name': project_name,
                            'source_dir': source_dir,
                            'intent_info': intent_info,
                            'website_recommendations': website_recommendations,
                            'opportunity_score': keyword_data.get('opportunity_score', 0),
                            'development_priority': priority_level,
                            'website_type': website_type,
                            'ai_category': ai_category,
                            'status': 'success'
                        })
                        print(f"âœ… ç½‘ç«™ç”ŸæˆæˆåŠŸ: {source_dir}")
                        
                        # æ˜¾ç¤ºåŸŸåå»ºè®®
                        domain_suggestions = website_recommendations.get('domain_suggestions', [])
                        if domain_suggestions:
                            print(f"   æ¨èåŸŸå: {', '.join(domain_suggestions[:3])}")
                    else:
                        website_results.append({
                            'keyword': keyword,
                            'project_name': project_name,
                            'website_type': website_type,
                            'status': 'failed',
                            'error': 'æºä»£ç ç”Ÿæˆå¤±è´¥'
                        })
                        print(f"âŒ ç½‘ç«™ç”Ÿæˆå¤±è´¥: {keyword}")
                else:
                    website_results.append({
                        'keyword': keyword,
                        'project_name': project_name,
                        'website_type': website_type,
                        'status': 'failed',
                        'error': 'æ„å›¾æ•°æ®åŠ è½½å¤±è´¥'
                    })
                    print(f"âŒ æ„å›¾æ•°æ®åŠ è½½å¤±è´¥: {keyword}")
                    
            except Exception as e:
                website_results.append({
                    'keyword': keyword,
                    'project_name': project_name if 'project_name' in locals() else 'unknown',
                    'website_type': website_type,
                    'status': 'failed',
                    'error': str(e)
                })
                print(f"âŒ ç½‘ç«™ç”Ÿæˆå¼‚å¸¸: {keyword} - {e}")
        
        return website_results
    
    def _generate_project_name_with_recommendations(self, keyword: str, recommendations: Dict[str, Any]) -> str:
        """åŸºäºå»ºç«™å»ºè®®ç”Ÿæˆé¡¹ç›®åç§°"""
        # æ¸…ç†å…³é”®è¯
        clean_keyword = keyword.lower().replace(' ', '_').replace('-', '_')
        clean_keyword = ''.join(c for c in clean_keyword if c.isalnum() or c == '_')
        
        # æ ¹æ®ç½‘ç«™ç±»å‹æ·»åŠ å‰ç¼€
        website_type = recommendations.get('website_type', '')
        if 'AIå·¥å…·ç«™' in website_type:
            prefix = 'ai_tool'
        elif 'è¯„æµ‹ç«™' in website_type:
            prefix = 'review'
        elif 'æ•™ç¨‹ç«™' in website_type:
            prefix = 'tutorial'
        elif 'å¯¼èˆªç«™' in website_type:
            prefix = 'nav'
        else:
            prefix = 'website'
        
        return f"{prefix}_{clean_keyword}"
    
    def _create_project_config(self, recommendations: Dict[str, Any], project_name: str) -> Dict[str, Any]:
        """åŸºäºå»ºç«™å»ºè®®åˆ›å»ºé¡¹ç›®é…ç½®"""
        config = {
            'project_name': project_name,
            'website_type': recommendations.get('website_type', 'é€šç”¨ç½‘ç«™'),
            'ai_category': recommendations.get('ai_tool_category', 'éAIå·¥å…·'),
            'domain_options': recommendations.get('domain_suggestions', []),
            'monetization_strategies': recommendations.get('monetization_strategy', []),
            'technical_requirements': recommendations.get('technical_requirements', []),
            'content_strategies': recommendations.get('content_strategy', []),
            'development_priority': recommendations.get('development_priority', {})
        }
        
        # æ ¹æ®AIå·¥å…·ç±»åˆ«è°ƒæ•´é…ç½®
        if 'AI' in config['ai_category']:
            config['use_ai_features'] = True
            config['api_integration'] = True
        
        # æ ¹æ®ç½‘ç«™ç±»å‹è°ƒæ•´æ¨¡æ¿
        if 'SaaS' in config['website_type']:
            config['template_type'] = 'saas'
        elif 'è¯„æµ‹' in config['website_type']:
            config['template_type'] = 'review'
        elif 'æ•™ç¨‹' in config['website_type']:
            config['template_type'] = 'tutorial'
        else:
            config['template_type'] = 'default'
        
        return config
    
    def _prepare_intent_data(self, keyword_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å‡†å¤‡æ„å›¾æ•°æ®æ ¼å¼"""
        intent_info = keyword_data.get('intent', {})
        market_info = keyword_data.get('market', {})
        
        return [{
            'query': keyword_data['keyword'],
            'intent_primary': intent_info.get('primary_intent', 'I'),
            'intent_secondary': intent_info.get('secondary_intent', ''),
            'sub_intent': intent_info.get('primary_intent', 'I') + '1',
            'probability': intent_info.get('confidence', 0.8),
            'probability_secondary': 0.2,
            'search_volume': market_info.get('search_volume', 1000),
            'competition': market_info.get('competition', 0.5),
            'opportunity_score': keyword_data.get('opportunity_score', 70),
            'ai_bonus': market_info.get('ai_bonus', 0),
            'commercial_value': market_info.get('commercial_value', 0)
        }]
    
    def _save_intent_data(self, intent_data: List[Dict[str, Any]], keyword: str) -> str:
        """ä¿å­˜æ„å›¾æ•°æ®åˆ°æ–‡ä»¶"""
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_keyword = "".join(c for c in keyword if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_keyword = safe_keyword.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦
        
        file_path = os.path.join(
            self.output_base_dir, 
            'intent_analysis', 
            f'intent_{safe_keyword}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(intent_data, f, ensure_ascii=False, indent=2)
        
        return file_path
    
    def _generate_project_name(self, keyword: str) -> str:
        """ç”Ÿæˆé¡¹ç›®åç§°"""
        # æ¸…ç†å…³é”®è¯ï¼Œç”Ÿæˆåˆé€‚çš„é¡¹ç›®å
        clean_name = "".join(c for c in keyword if c.isalnum() or c in (' ', '-')).strip()
        clean_name = clean_name.replace(' ', '-').lower()
        
        # é™åˆ¶é•¿åº¦å¹¶æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%m%d_%H%M")
        return f"{clean_name[:30]}-{timestamp}"
    
    def _batch_deploy_websites(self, website_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡éƒ¨ç½²ç½‘ç«™"""
        deployment_results = []
        
        successful_websites = [w for w in website_results if w.get('status') == 'success']
        
        for website in successful_websites:
            keyword = website['keyword']
            source_dir = website['source_dir']
            
            print(f"ğŸš€ éƒ¨ç½²ç½‘ç«™: {keyword}")
            
            try:
                # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„éƒ¨ç½²é€»è¾‘
                deployment_url = f"https://{website['project_name']}.pages.dev"
                
                deployment_results.append({
                    'keyword': keyword,
                    'project_name': website['project_name'],
                    'deployment_url': deployment_url,
                    'platform': self.config.get('deployment_platform', 'cloudflare'),
                    'status': 'success'
                })
                
                print(f"âœ… éƒ¨ç½²æˆåŠŸ: {deployment_url}")
                
            except Exception as e:
                deployment_results.append({
                    'keyword': keyword,
                    'project_name': website['project_name'],
                    'status': 'failed',
                    'error': str(e)
                })
                print(f"âŒ éƒ¨ç½²å¤±è´¥: {keyword} - {e}")
        
        return deployment_results
    
    def _generate_workflow_report(self, workflow_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå·¥ä½œæµç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = os.path.join(
            self.output_base_dir, 
            'reports', 
            f'integrated_workflow_report_{timestamp}.md'
        )
        
        # ç»Ÿè®¡æ•°æ®
        total_keywords = len(workflow_results.get('demand_analysis', {}).get('keywords', []))
        discovered_keywords = workflow_results.get('multi_platform_discovery', {}).get('total_keywords', 0)
        high_value_count = len(workflow_results.get('high_value_keywords', []))
        successful_websites = len([w for w in workflow_results.get('generated_projects', []) if w.get('status') == 'success'])
        successful_deployments = len([d for d in workflow_results.get('deployment_results', []) if d.get('status') == 'success'])
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# é›†æˆå·¥ä½œæµæ‰§è¡ŒæŠ¥å‘Š

## ğŸ“Š æ‰§è¡Œæ¦‚è§ˆ
- **æ‰§è¡Œæ—¶é—´**: {workflow_results.get('start_time', '')} - {workflow_results.get('end_time', '')}
- **è¾“å…¥æ–‡ä»¶**: {workflow_results.get('input_file', '')}
- **æ‰§è¡ŒçŠ¶æ€**: {workflow_results.get('status', '')}

## ğŸ“ˆ æ•°æ®ç»Ÿè®¡
- **æ€»å…³é”®è¯æ•°**: {total_keywords}
- **å¤šå¹³å°å‘ç°å…³é”®è¯**: {discovered_keywords}
- **é«˜ä»·å€¼å…³é”®è¯**: {high_value_count}
- **æˆåŠŸç”Ÿæˆç½‘ç«™**: {successful_websites}
- **æˆåŠŸéƒ¨ç½²ç½‘ç«™**: {successful_deployments}

## ğŸ” å¤šå¹³å°å…³é”®è¯å‘ç°
"""
        
        # æ·»åŠ å¤šå¹³å°å…³é”®è¯å‘ç°ç»“æœ
        discovery_results = workflow_results.get('multi_platform_discovery', {})
        if discovery_results and 'total_keywords' in discovery_results and discovery_results['total_keywords'] > 0:
            # å¹³å°åˆ†å¸ƒ
            if 'platform_distribution' in discovery_results:
                report_content += "### å¹³å°åˆ†å¸ƒ\n"
                for platform, count in discovery_results['platform_distribution'].items():
                    report_content += f"- **{platform}**: {count} ä¸ªå…³é”®è¯\n"
                report_content += "\n"
            
            # çƒ­é—¨å…³é”®è¯
            if 'top_keywords_by_score' in discovery_results and discovery_results['top_keywords_by_score']:
                report_content += "### çƒ­é—¨å…³é”®è¯\n"
                for i, kw in enumerate(discovery_results['top_keywords_by_score'][:10], 1):
                    report_content += f"{i}. **{kw['keyword']}** (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})\n"
                report_content += "\n"
            
            # å¸¸è§è¯æ±‡
            if 'common_terms' in discovery_results and discovery_results['common_terms']:
                report_content += "### å¸¸è§è¯æ±‡\n"
                for word, count in list(discovery_results['common_terms'].items())[:10]:
                    report_content += f"- **{word}**: {count}æ¬¡\n"
                report_content += "\n"
        else:
            report_content += "æœªå‘ç°å¤šå¹³å°å…³é”®è¯æˆ–å‘ç°è¿‡ç¨‹å¤±è´¥ã€‚\n\n"
        
        # æ–°è¯æ£€æµ‹æ‘˜è¦
        new_word_summary = workflow_results.get('demand_analysis', {}).get('new_word_summary')
        if new_word_summary:
            report_content += "## ğŸ” æ–°è¯æ£€æµ‹æ‘˜è¦\n"
            report_content += f"- **æ£€æµ‹æ€»æ•°**: {new_word_summary.get('total_analyzed', 0)}\n"
            report_content += f"- **æ–°è¯æ•°é‡**: {new_word_summary.get('new_words_detected', 0)}\n"
            report_content += f"- **é«˜ç½®ä¿¡åº¦æ–°è¯**: {new_word_summary.get('high_confidence_new_words', 0)}\n"
            report_content += f"- **Breakout çº§åˆ«**: {new_word_summary.get('breakout_keywords', 0)}\n"
            report_content += f"- **Rising çº§åˆ«**: {new_word_summary.get('rising_keywords', 0)}\n"
            report_content += f"- **æ–°è¯å æ¯”**: {new_word_summary.get('new_word_percentage', 0)}%\n\n"

            # åˆ—å‡ºéƒ¨åˆ† Breakout æ–°è¯
            demand_keywords = workflow_results.get('demand_analysis', {}).get('keywords', [])
            breakout_keywords = [
                kw for kw in demand_keywords
                if kw.get('new_word_detection', {}).get('trend_momentum') == 'breakout'
            ]
            if breakout_keywords:
                report_content += "### ğŸ”¥ Breakout æ–°è¯æ ·ä¾‹\n"
                for kw in breakout_keywords[:5]:
                    nwd = kw.get('new_word_detection', {})
                    report_content += (
                        f"- **{kw.get('keyword')}** (åˆ†æ•°: {nwd.get('new_word_score', 0)}, "
                        f"åŠ¨é‡: {nwd.get('trend_momentum', '-')}, çªå¢: {nwd.get('avg_7d_delta', 0)})\n"
                    )
                report_content += "\n"

        report_content += "## ğŸ¯ é«˜ä»·å€¼å…³é”®è¯åˆ—è¡¨\n"
        
        # æ·»åŠ é«˜ä»·å€¼å…³é”®è¯è¯¦æƒ…
        for kw in workflow_results.get('high_value_keywords', [])[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            report_content += f"- **{kw['keyword']}** (æœºä¼šåˆ†æ•°: {kw.get('opportunity_score', 0)})\n"
            report_content += f"  - ä¸»è¦æ„å›¾: {kw.get('intent', {}).get('primary_intent', 'Unknown')}\n"
            report_content += f"  - AIåŠ åˆ†: {kw.get('market', {}).get('ai_bonus', 0)}\n"
            report_content += f"  - å•†ä¸šä»·å€¼: {kw.get('market', {}).get('commercial_value', 0)}\n"

            nwd = kw.get('new_word_detection', {})
            if nwd:
                report_content += (
                    "  - æ–°è¯ä¿¡æ¯: "
                    f"{'âœ…' if nwd.get('is_new_word') else 'âŒ'} ç­‰çº§ {nwd.get('new_word_grade', '-')}, "
                    f"åŠ¨é‡ {nwd.get('trend_momentum', '-')}, Î”30D {nwd.get('avg_30d_delta', 0)}\n"
                )
            report_content += "\n"
        
        # æ·»åŠ ç”Ÿæˆçš„ç½‘ç«™é¡¹ç›®
        report_content += "\n## ğŸ—ï¸ ç”Ÿæˆçš„ç½‘ç«™é¡¹ç›®\n"
        for website in workflow_results.get('generated_projects', []):
            status_icon = "âœ…" if website.get('status') == 'success' else "âŒ"
            report_content += f"{status_icon} **{website['keyword']}**\n"
            if website.get('status') == 'success':
                report_content += f"  - é¡¹ç›®ç›®å½•: {website.get('source_dir', '')}\n"
            else:
                report_content += f"  - é”™è¯¯: {website.get('error', '')}\n"
        
        # æ·»åŠ éƒ¨ç½²ç»“æœ
        if workflow_results.get('deployment_results'):
            report_content += "\n## ğŸš€ éƒ¨ç½²ç»“æœ\n"
            for deployment in workflow_results.get('deployment_results', []):
                status_icon = "âœ…" if deployment.get('status') == 'success' else "âŒ"
                report_content += f"{status_icon} **{deployment['keyword']}**\n"
                if deployment.get('status') == 'success':
                    report_content += f"  - éƒ¨ç½²åœ°å€: {deployment.get('deployment_url', '')}\n"
                else:
                    report_content += f"  - é”™è¯¯: {deployment.get('error', '')}\n"
        
        # æ·»åŠ å»ºè®®
        report_content += f"""
## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### å…³é”®è¯ä¼˜åŒ–
- ç»§ç»­æŒ–æ˜ç›¸å…³é•¿å°¾å…³é”®è¯
- å…³æ³¨AIç›¸å…³é«˜ä»·å€¼å…³é”®è¯
- å®šæœŸæ›´æ–°å…³é”®è¯æœºä¼šåˆ†æ•°

### ç½‘ç«™ä¼˜åŒ–
- ä¼˜åŒ–SEOå…ƒæ•°æ®å’Œç»“æ„
- æ·»åŠ æ›´å¤šäº¤äº’åŠŸèƒ½
- å®Œå–„ç§»åŠ¨ç«¯é€‚é…

### è¿è¥å»ºè®®
- æäº¤åˆ°AIå·¥å…·å¯¼èˆªç«™
- å»ºç«‹ç¤¾äº¤åª’ä½“æ¨å¹¿è®¡åˆ’
- ç›‘æ§ç½‘ç«™æµé‡å’Œè½¬åŒ–

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return report_path

    def _export_new_word_reports(self, new_word_results: pd.DataFrame) -> Dict[str, str]:
        if new_word_results is None or new_word_results.empty:
            return {}

        reports_dir = os.path.join(self.output_base_dir, 'reports', 'new_words')
        os.makedirs(reports_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        exports: Dict[str, str] = {}

        def _save(df: pd.DataFrame, filename: str, key: str) -> None:
            if df is None or df.empty:
                return
            path = os.path.join(reports_dir, filename)
            df.to_csv(path, index=False)
            exports[key] = path

        try:
            breakout_df = new_word_results[new_word_results.get('trend_momentum') == 'breakout']
            _save(breakout_df, f'breakout_new_words_{timestamp}.csv', 'breakout')

            rising_df = new_word_results[new_word_results.get('trend_momentum').isin(['breakout', 'rising'])]
            _save(rising_df, f'rising_new_words_{timestamp}.csv', 'rising')

            top_df = new_word_results.sort_values('new_word_score', ascending=False).head(30)
            _save(top_df, f'top_new_words_{timestamp}.csv', 'top')
        except Exception as exc:
            print(f"âš ï¸ å¯¼å‡ºæ–°è¯æŠ¥å‘Šå¤±è´¥: {exc}")

        return exports


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é›†æˆå·¥ä½œæµï¼šéœ€æ±‚æŒ–æ˜ â†’ æ„å›¾åˆ†æ â†’ ç½‘ç«™ç”Ÿæˆ â†’ è‡ªåŠ¨éƒ¨ç½²')
    parser.add_argument('--input', '-i', required=True, help='å…³é”®è¯è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--min-score', type=int, default=60, help='æœ€ä½æœºä¼šåˆ†æ•°é˜ˆå€¼')
    parser.add_argument('--max-projects', type=int, default=5, help='æœ€å¤§é¡¹ç›®æ•°é‡')
    parser.add_argument('--no-deploy', action='store_true', help='è·³è¿‡è‡ªåŠ¨éƒ¨ç½²')
    
    args = parser.parse_args()
    
    # å‡†å¤‡é…ç½®
    config = {
        'min_opportunity_score': args.min_score,
        'max_projects_per_batch': args.max_projects,
        'auto_deploy': not args.no_deploy,
        'deployment_platform': 'cloudflare',
        'use_tailwind': True,
        'generate_reports': True
    }
    
    # å¦‚æœæœ‰é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        import json
        with open(args.config, 'r', encoding='utf-8') as f:
            user_config = json.load(f)
            config.update(user_config)
    
    try:
        # åˆ›å»ºå·¥ä½œæµå®ä¾‹
        workflow = IntegratedWorkflow(config)
        
        # æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        results = workflow.run_complete_workflow(args.input)
        
        if results['status'] == 'success':
            print(f"\nğŸ‰ é›†æˆå·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼")
            print(f"ğŸ” å¤šå¹³å°å‘ç°äº† {results.get('multi_platform_discovery', {}).get('total_keywords', 0)} ä¸ªå…³é”®è¯")
            print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {results.get('report_path', '')}")
            return 0
        else:
            print(f"\nâŒ é›†æˆå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {results.get('error', '')}")
            return 1
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
