#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TrendingKeywords.net æ•°æ®æ”¶é›†å™¨
ä» https://trendingkeywords.net/ è·å–çƒ­é—¨å…³é”®è¯æ•°æ®
"""

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import random
from typing import List, Dict, Optional
import logging


class TrendingKeywordsCollector:
    """TrendingKeywords.net æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.base_url = "https://trendingkeywords.net/"
        self.session = self._create_session()
        self.ssl_retry_attempts = 3
        self.ssl_retry_backoff = 1.2
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)

    def _create_session(self) -> requests.Session:
        """Create a session with retry-aware adapters."""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        retry_config = Retry(
            total=3,
            connect=3,
            read=2,
            backoff_factor=0.8,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=None
        )
        adapter = HTTPAdapter(max_retries=retry_config, pool_maxsize=5, pool_block=True)
        session.mount('https://', adapter)
        session.mount('http://', adapter)
        return session

    def _reset_session(self) -> None:
        """Recreate the underlying HTTP session after persistent TLS failures."""
        try:
            self.session.close()
        finally:
            self.session = self._create_session()

    def _request_with_retries(self, url: str, timeout: int = 10) -> requests.Response:
        """Perform a GET request with defensive TLS retries."""
        last_error: Optional[Exception] = None
        for attempt in range(1, self.ssl_retry_attempts + 1):
            try:
                return self.session.get(url, timeout=timeout)
            except requests.exceptions.SSLError as ssl_error:
                last_error = ssl_error
                self.logger.warning(
                    "TLS æ¡æ‰‹å¼‚å¸¸ (%s/%s): %s", attempt, self.ssl_retry_attempts, ssl_error
                )
                self._reset_session()
                sleep_time = min(self.ssl_retry_backoff * attempt, 3)
                if sleep_time > 0:
                    time.sleep(sleep_time)
            except requests.RequestException as request_error:
                last_error = request_error
                if attempt >= self.ssl_retry_attempts:
                    break
                sleep_time = min(self.ssl_retry_backoff * attempt, 3)
                if sleep_time > 0:
                    time.sleep(sleep_time)
        if last_error:
            raise last_error
        raise RuntimeError("è¯·æ±‚æœªèƒ½æˆåŠŸä¸”æœªæ•è·å…·ä½“å¼‚å¸¸")
        
    def fetch_trending_keywords(self, max_pages: int = 3, delay_range: tuple = (1, 3)) -> pd.DataFrame:
        """
        è·å–çƒ­é—¨å…³é”®è¯æ•°æ®
        
        Args:
            max_pages: æœ€å¤§æŠ“å–é¡µæ•°
            delay_range: è¯·æ±‚é—´éš”æ—¶é—´èŒƒå›´(ç§’)
            
        Returns:
            åŒ…å«å…³é”®è¯æ•°æ®çš„DataFrame
        """
        all_keywords = []
        
        try:
            for page in range(1, max_pages + 1):
                self.logger.info(f"æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ...")
                
                # æ„å»ºé¡µé¢URL
                if page == 1:
                    url = self.base_url
                else:
                    url = f"{self.base_url}?page={page}"
                
                # è·å–é¡µé¢æ•°æ®
                keywords = self._fetch_page_keywords(url)
                if keywords:
                    all_keywords.extend(keywords)
                    self.logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
                else:
                    self.logger.warning(f"ç¬¬ {page} é¡µæœªè·å–åˆ°æ•°æ®")
                    break
                
                # éšæœºå»¶è¿Ÿé¿å…è¢«å°
                if page < max_pages:
                    delay = random.uniform(*delay_range)
                    time.sleep(delay)
            
            # è½¬æ¢ä¸ºDataFrame
            if all_keywords:
                df = pd.DataFrame(all_keywords)
                self.logger.info(f"æ€»å…±è·å–åˆ° {len(df)} ä¸ªå…³é”®è¯")
                return df
            else:
                self.logger.warning("æœªè·å–åˆ°ä»»ä½•å…³é”®è¯æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"è·å–çƒ­é—¨å…³é”®è¯å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _fetch_page_keywords(self, url: str) -> List[Dict]:
        """
        è·å–å•é¡µå…³é”®è¯æ•°æ®
        
        Args:
            url: é¡µé¢URL
            
        Returns:
            å…³é”®è¯æ•°æ®åˆ—è¡¨
        """
        try:
            response = self._request_with_retries(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            keywords = []
            
            # æŸ¥æ‰¾å…³é”®è¯æ¡ç›®
            # æ ¹æ®ç½‘ç«™ç»“æ„ï¼Œå…³é”®è¯é€šå¸¸åœ¨ç‰¹å®šçš„å®¹å™¨ä¸­
            keyword_containers = self._find_keyword_containers(soup)
            
            for container in keyword_containers:
                keyword_data = self._extract_keyword_data(container)
                if keyword_data:
                    keywords.append(keyword_data)
            
            return keywords
            
        except requests.RequestException as e:
            self.logger.error(f"è¯·æ±‚é¡µé¢å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥ {url}: {e}")
            return []
    
    def _find_keyword_containers(self, soup: BeautifulSoup) -> List:
        """
        æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å®¹å™¨å…ƒç´ 
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            å…³é”®è¯å®¹å™¨åˆ—è¡¨
        """
        containers = []
        
        # æ ¹æ®è°ƒè¯•ç»“æœï¼ŒæŸ¥æ‰¾åŒ…å«æœç´¢é‡çš„å¡ç‰‡å®¹å™¨
        # æ¯ä¸ªå…³é”®è¯éƒ½åœ¨ä¸€ä¸ªå¸¦æœ‰ç‰¹å®šclassçš„divå®¹å™¨ä¸­
        card_containers = soup.find_all('div', class_=lambda x: x and 'max-w-sm' in x and 'bg-white' in x and 'rounded-lg' in x)
        
        if card_containers:
            self.logger.info(f"æ‰¾åˆ° {len(card_containers)} ä¸ªå¡ç‰‡å®¹å™¨")
            return card_containers[:50]  # é™åˆ¶æ•°é‡
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾åŒ…å«æœç´¢é‡æ–‡æœ¬çš„å…ƒç´ 
        volume_pattern = re.compile(r'\d+k?/Month', re.IGNORECASE)
        volume_elements = soup.find_all(string=volume_pattern)
        
        for element in volume_elements:
            # å‘ä¸ŠæŸ¥æ‰¾å¡ç‰‡å®¹å™¨ (é€šå¸¸æ˜¯3-4çº§çˆ¶å…ƒç´ )
            parent = element.parent
            for _ in range(4):  # å‘ä¸ŠæŸ¥æ‰¾4çº§
                if parent and parent.name == 'div':
                    classes = parent.get('class', [])
                    # æŸ¥æ‰¾åŒ…å«å¡ç‰‡æ ·å¼çš„å®¹å™¨
                    if any('bg-white' in str(classes) or 'shadow' in str(classes) or 'rounded' in str(classes)):
                        if parent not in containers:
                            containers.append(parent)
                        break
                if parent:
                    parent = parent.parent
                else:
                    break
        
        return containers[:50]  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
    
    def _extract_keyword_data(self, container) -> Optional[Dict]:
        """
        ä»å®¹å™¨å…ƒç´ ä¸­æå–å…³é”®è¯æ•°æ®
        
        Args:
            container: åŒ…å«å…³é”®è¯ä¿¡æ¯çš„HTMLå…ƒç´ 
            
        Returns:
            å…³é”®è¯æ•°æ®å­—å…¸æˆ–None
        """
        try:
            # æå–å…³é”®è¯åç§°
            keyword = self._extract_keyword_name(container)
            if not keyword:
                return None
            
            # æå–æœç´¢é‡
            volume = self._extract_search_volume(container)
            
            # æå–æè¿°
            description = self._extract_description(container)
            
            # æå–ç±»åˆ«/æ ‡ç­¾
            category = self._extract_category(container)
            
            return {
                'keyword': keyword.strip(),
                'search_volume': volume,
                'volume_text': self._extract_volume_text(container),
                'description': description.strip() if description else '',
                'category': category.strip() if category else '',
                'source': 'TrendingKeywords.net',
                'query': keyword.strip()  # ä¸ºäº†å…¼å®¹ç°æœ‰ç³»ç»Ÿ
            }
            
        except Exception as e:
            self.logger.debug(f"æå–å…³é”®è¯æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_keyword_name(self, container) -> Optional[str]:
        """æå–å…³é”®è¯åç§°"""
        # æ ¹æ®è°ƒè¯•ç»“æœï¼Œå…³é”®è¯åç§°é€šå¸¸æ˜¯å®¹å™¨ä¸­çš„ç¬¬ä¸€è¡Œæ–‡æœ¬
        full_text = container.get_text()
        lines = [line.strip() for line in full_text.split('\n') if line.strip()]
        
        if lines:
            # ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯å…³é”®è¯åç§°
            first_line = lines[0]
            
            # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤æœç´¢é‡ä¿¡æ¯
            clean_line = re.sub(r'\(\d+k?/Month\)', '', first_line).strip()
            clean_line = re.sub(r'\d+k?/Month', '', clean_line).strip()
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å…³é”®è¯
            if not clean_line:
                return None
            if len(clean_line) <= 1 or len(clean_line) >= 100:
                return None
            if clean_line.startswith('http'):
                return None
            if re.match(r'^\d+$', clean_line):
                return None
            if 'Last 90 days' in clean_line or clean_line == 'Detail':
                return None
            return clean_line
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•å…¶ä»–é€‰æ‹©å™¨
        selectors = [
            'h3', 'h4', 'h5',
            '.keyword-name', '.title', '.name',
            'strong', 'b'
        ]
        
        for selector in selectors:
            element = container.select_one(selector)
            if element:
                text = element.get_text().strip()
                # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤æœç´¢é‡ä¿¡æ¯
                text = re.sub(r'\(\d+k?/Month\)', '', text).strip()
                text = re.sub(r'\d+k?/Month', '', text).strip()
                if text and len(text) > 1 and len(text) < 100:
                    return text
        
        return None
    
    def _extract_search_volume(self, container) -> int:
        """æå–æœç´¢é‡æ•°å€¼"""
        volume_text = self._extract_volume_text(container)
        if volume_text:
            # æå–æ•°å­—
            match = re.search(r'(\d+(?:\.\d+)?)k?', volume_text, re.IGNORECASE)
            if match:
                num = float(match.group(1))
                # å¦‚æœåŒ…å«kï¼Œä¹˜ä»¥1000
                if 'k' in volume_text.lower():
                    num *= 1000
                return int(num)
        return 0
    
    def _extract_volume_text(self, container) -> Optional[str]:
        """æå–æœç´¢é‡æ–‡æœ¬"""
        text = container.get_text()
        # æŸ¥æ‰¾æœç´¢é‡æ¨¡å¼
        match = re.search(r'\d+k?/Month', text, re.IGNORECASE)
        return match.group(0) if match else None
    
    def _extract_description(self, container) -> Optional[str]:
        """æå–æè¿°ä¿¡æ¯"""
        # æ ¹æ®è°ƒè¯•ç»“æœï¼Œæè¿°é€šå¸¸æ˜¯ç¬¬ä¸‰è¡Œæ–‡æœ¬
        full_text = container.get_text()
        lines = [line.strip() for line in full_text.split('\n') if line.strip()]
        
        # æŸ¥æ‰¾æè¿°è¡Œï¼ˆé€šå¸¸æ˜¯è¾ƒé•¿çš„æ–‡æœ¬ï¼Œä¸åŒ…å«æœç´¢é‡ï¼‰
        for line in lines:
            # è·³è¿‡å…³é”®è¯åç§°ã€æœç´¢é‡ã€é“¾æ¥æ–‡æœ¬
            if (re.search(r'\d+k?/Month', line, re.IGNORECASE) or
                'Last 90 days' in line or
                'Detail' == line.strip() or
                len(line) < 20):
                continue
            
            # æ‰¾åˆ°æè¿°æ–‡æœ¬
            if len(line) > 20 and len(line) < 500:
                # æ¸…ç†æè¿°æ–‡æœ¬
                desc = re.sub(r'\d+k?/Month', '', line).strip()
                if desc:
                    return desc
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾ç‰¹å®šå…ƒç´ 
        desc_selectors = [
            '.description', '.desc', '.summary',
            'p', '.content', '.detail'
        ]
        
        for selector in desc_selectors:
            element = container.select_one(selector)
            if element:
                desc = element.get_text().strip()
                # è¿‡æ»¤æ‰æœç´¢é‡ä¿¡æ¯
                desc = re.sub(r'\d+k?/Month', '', desc).strip()
                if desc and len(desc) > 10:
                    return desc
        
        return None
    
    def _extract_category(self, container) -> Optional[str]:
        """æå–ç±»åˆ«ä¿¡æ¯"""
        # æŸ¥æ‰¾ç±»åˆ«æ ‡ç­¾
        cat_selectors = [
            '.category', '.tag', '.label',
            '.badge', '.chip'
        ]
        
        for selector in cat_selectors:
            element = container.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return None
    
    def get_trending_keywords_for_analysis(self, max_keywords: int = 50) -> pd.DataFrame:
        """
        è·å–ç”¨äºåˆ†æçš„çƒ­é—¨å…³é”®è¯
        
        Args:
            max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡
            
        Returns:
            æ ¼å¼åŒ–çš„å…³é”®è¯DataFrame
        """
        # è·å–åŸå§‹æ•°æ®
        df = self.fetch_trending_keywords(max_pages=3)
        
        if df.empty:
            return pd.DataFrame(columns=['query'])
        
        # æŒ‰æœç´¢é‡æ’åº
        if 'search_volume' in df.columns:
            df = df.sort_values('search_volume', ascending=False)
        
        # é™åˆ¶æ•°é‡
        df = df.head(max_keywords)
        
        # ç¡®ä¿æœ‰queryåˆ—ç”¨äºåç»­åˆ†æ
        if 'query' not in df.columns and 'keyword' in df.columns:
            df['query'] = df['keyword']
        
        try:
            from src.pipeline.cleaning.cleaner import clean_terms
            cleaned = clean_terms(df['query'].astype(str).tolist())
            if not cleaned:
                return pd.DataFrame(columns=['query'])
            return pd.DataFrame({'query': cleaned})
        except Exception:
            return df[['query']].copy()
    
    def save_results(self, df: pd.DataFrame, output_dir: str) -> str:
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            df: å…³é”®è¯DataFrame
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        import os
        from datetime import datetime
        
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"trending_keywords_{timestamp}.csv"
        filepath = os.path.join(output_dir, filename)
        
        df.to_csv(filepath, index=False, encoding='utf-8')
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


def main():
    """æµ‹è¯•å‡½æ•°"""
    collector = TrendingKeywordsCollector()
    
    print("ğŸ” å¼€å§‹è·å– TrendingKeywords.net æ•°æ®...")
    df = collector.fetch_trending_keywords(max_pages=2)
    
    if not df.empty:
        print(f"âœ… æˆåŠŸè·å– {len(df)} ä¸ªå…³é”®è¯")
        print("\nğŸ“Š æ ·æœ¬æ•°æ®:")
        print(df.head().to_string())
        
        # ä¿å­˜ç»“æœ
        output_file = collector.save_results(df, "output")
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print("âŒ æœªè·å–åˆ°æ•°æ®")


if __name__ == "__main__":
    main()
