#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºåŠŸèƒ½æ¨¡å—
ä¸ºä¸»ç¨‹åºæä¾›é¢å¤–çš„å¢å¼ºåŠŸèƒ½
"""

from datetime import datetime
from typing import List, Dict, Any

def monitor_competitors(sites: List[str], output_dir: str = None) -> Dict[str, Any]:
    """ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–"""
    print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(sites)} ä¸ªç«å“ç½‘ç«™...")
    
    results = {
        'monitoring_date': datetime.now().isoformat(),
        'competitors': [],
        'new_keywords': [],
        'trending_keywords': [],
        'recommendations': []
    }
    
    for site in sites:
        print(f"ğŸ“Š åˆ†æç«å“: {site}")
        
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ç«å“åˆ†æAPI
        # ç«å“æ•°æ®
        competitor_data = {
            'site': site,
            'top_keywords': [
                {'keyword': f'{site} ai tool', 'volume': 1000, 'difficulty': 0.6},
                {'keyword': f'{site} alternative', 'volume': 800, 'difficulty': 0.4},
                {'keyword': f'best {site} features', 'volume': 600, 'difficulty': 0.5}
            ],
            'new_keywords_count': 15,
            'trending_keywords_count': 8
        }
        
        results['competitors'].append(competitor_data)
    
    # ä¿å­˜ç›‘æ§ç»“æœ
    if output_dir:
        _save_competitor_monitoring_results(results, output_dir)
    
    return results

def predict_keyword_trends(timeframe: str = "30d", output_dir: str = None, keywords: List[str] = None, use_real_data: bool = True) -> Dict[str, Any]:
    """é¢„æµ‹å…³é”®è¯è¶‹åŠ¿
    
    Args:
        timeframe: é¢„æµ‹æ—¶é—´èŒƒå›´
        output_dir: è¾“å‡ºç›®å½•
        keywords: è¦åˆ†æçš„å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯
        use_real_data: æ˜¯å¦ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œé¢„æµ‹ï¼ŒFalseæ—¶è¿”å›æ¼”ç¤ºæ•°æ®
    
    Returns:
        è¶‹åŠ¿é¢„æµ‹ç»“æœ
    """
    print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
    
    if not use_real_data:
        # è¿”å›æ¼”ç¤ºæ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        print("âš ï¸ ä½¿ç”¨æ¼”ç¤ºæ•°æ®æ¨¡å¼ï¼ŒéçœŸå®é¢„æµ‹ç»“æœ")
        predictions = {
            'prediction_date': datetime.now().isoformat(),
            'timeframe': timeframe,
            'data_source': 'demo_data',
            'rising_keywords': [
                {'keyword': 'AI video generator', 'predicted_growth': '+150%', 'confidence': 0.85},
                {'keyword': 'AI code assistant', 'predicted_growth': '+120%', 'confidence': 0.78},
                {'keyword': 'AI image upscaler', 'predicted_growth': '+90%', 'confidence': 0.72}
            ],
            'declining_keywords': [
                {'keyword': 'basic chatbot', 'predicted_decline': '-30%', 'confidence': 0.65},
                {'keyword': 'simple ai writer', 'predicted_decline': '-20%', 'confidence': 0.58}
            ],
            'stable_keywords': [
                {'keyword': 'AI generator', 'predicted_change': '+5%', 'confidence': 0.90},
                {'keyword': 'AI assistant', 'predicted_change': '+10%', 'confidence': 0.88}
            ]
        }
    else:
        # ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œé¢„æµ‹
        print("âœ… ä½¿ç”¨çœŸå®Google Trendsæ•°æ®è¿›è¡Œé¢„æµ‹")
        predictions = _predict_with_real_data(keywords, timeframe)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    if output_dir:
        _save_trend_predictions(predictions, output_dir)
    
    return predictions

def _predict_with_real_data(keywords: List[str] = None, timeframe: str = "30d") -> Dict[str, Any]:
    """ä½¿ç”¨çœŸå®Google Trendsæ•°æ®è¿›è¡Œè¶‹åŠ¿é¢„æµ‹"""
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import sys
        import os
        import pandas as pd
        
        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(current_dir))
        if project_root not in sys.path:
            sys.path.insert(0, project_root)
        
        from src.demand_mining.managers.trend_manager import TrendManager
        from src.demand_mining.analyzers.timeliness_analyzer import TimelinessAnalyzer
        
        # å¦‚æœæ²¡æœ‰æä¾›å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤çš„AIç›¸å…³å…³é”®è¯
        if not keywords:
            keywords = [
                'ai tools', 'chatgpt', 'claude ai', 'gemini ai', 
                'ai generator', 'ai assistant', 'machine learning',
                'artificial intelligence', 'ai image generator', 'ai code assistant'
            ]
        
        print(f"ğŸ” åˆ†æ {len(keywords)} ä¸ªå…³é”®è¯çš„çœŸå®è¶‹åŠ¿æ•°æ®...")
        
        # ä½¿ç”¨å®æ—¶æ€§åˆ†æå™¨è·å–çœŸå®è¶‹åŠ¿æ•°æ®
        analyzer = TimelinessAnalyzer()
        df = pd.DataFrame({'query': keywords})
        
        # æ‰§è¡Œå®æ—¶æ€§åˆ†æ
        result_df = analyzer.analyze_timeliness(df)
        
        # åŸºäºåˆ†æç»“æœè¿›è¡Œé¢„æµ‹åˆ†ç±»
        predictions = {
            'prediction_date': datetime.now().isoformat(),
            'timeframe': timeframe,
            'data_source': 'google_trends_real_data',
            'analysis_method': 'timeliness_based_prediction',
            'total_keywords_analyzed': len(keywords),
            'rising_keywords': [],
            'stable_keywords': [],
            'declining_keywords': []
        }
        
        for _, row in result_df.iterrows():
            keyword_data = {
                'keyword': row['query'],
                'timeliness_score': float(row.get('timeliness_score', 0)),
                'trend_direction': row.get('trend_direction', 'stable'),
                'growth_rate': f"{row.get('growth_rate', 0):+.1f}%",
                'confidence': min(0.95, max(0.3, float(row.get('timeliness_score', 50)) / 100)),
                'current_interest': float(row.get('current_interest', 0)),
                'peak_interest': float(row.get('peak_interest', 0))
            }
            
            # æ ¹æ®è¶‹åŠ¿æ–¹å‘åˆ†ç±»
            if row.get('trend_direction') == 'rising':
                predictions['rising_keywords'].append(keyword_data)
            elif row.get('trend_direction') == 'falling':
                predictions['declining_keywords'].append(keyword_data)
            else:
                predictions['stable_keywords'].append(keyword_data)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        predictions['rising_keywords'].sort(key=lambda x: x['confidence'], reverse=True)
        predictions['declining_keywords'].sort(key=lambda x: x['confidence'], reverse=True)
        predictions['stable_keywords'].sort(key=lambda x: x['confidence'], reverse=True)
        
        print(f"âœ… é¢„æµ‹å®Œæˆ: {len(predictions['rising_keywords'])} ä¸Šå‡, {len(predictions['stable_keywords'])} ç¨³å®š, {len(predictions['declining_keywords'])} ä¸‹é™")
        
        return predictions
        
    except Exception as e:
        print(f"âŒ çœŸå®æ•°æ®é¢„æµ‹å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°æ¼”ç¤ºæ•°æ®æ¨¡å¼")
        
        # å›é€€åˆ°æ¼”ç¤ºæ•°æ®
        return {
            'prediction_date': datetime.now().isoformat(),
            'timeframe': timeframe,
            'data_source': 'demo_data_fallback',
            'error': str(e),
            'rising_keywords': [
                {'keyword': 'AI video generator', 'predicted_growth': '+150%', 'confidence': 0.85},
                {'keyword': 'AI code assistant', 'predicted_growth': '+120%', 'confidence': 0.78}
            ],
            'declining_keywords': [
                {'keyword': 'basic chatbot', 'predicted_decline': '-30%', 'confidence': 0.65}
            ],
            'stable_keywords': [
                {'keyword': 'AI generator', 'predicted_change': '+5%', 'confidence': 0.90}
            ]
        }

def generate_seo_audit(domain: str, keywords: List[str] = None) -> Dict[str, Any]:
    """ç”ŸæˆSEOä¼˜åŒ–å»ºè®®"""
    print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {domain}")
    
    audit_results = {
        'domain': domain,
        'audit_date': datetime.now().isoformat(),
        'keyword_opportunities': [],
        'content_gaps': [],
        'technical_recommendations': [],
        'competitor_analysis': {}
    }
    
    # å…³é”®è¯æœºä¼šåˆ†æ
    if keywords:
        for keyword in keywords[:10]:  # é™åˆ¶åˆ†ææ•°é‡
            opportunity = {
                'keyword': keyword,
                'current_ranking': 'Not ranking',
                'difficulty': 0.6,
                'opportunity_score': 75,
                'recommended_actions': [
                    f'åˆ›å»ºé’ˆå¯¹"{keyword}"çš„ä¸“é—¨é¡µé¢',
                    f'ä¼˜åŒ–é¡µé¢æ ‡é¢˜åŒ…å«"{keyword}"',
                    f'å¢åŠ ç›¸å…³çš„å†…éƒ¨é“¾æ¥'
                ]
            }
            audit_results['keyword_opportunities'].append(opportunity)
    
    # å†…å®¹ç¼ºå£åˆ†æ
    audit_results['content_gaps'] = [
        'ç¼ºå°‘AIå·¥å…·æ¯”è¾ƒé¡µé¢',
        'éœ€è¦æ›´å¤šæ•™ç¨‹å†…å®¹',
        'ç¼ºå°‘ç”¨æˆ·æ¡ˆä¾‹ç ”ç©¶'
    ]
    
    # æŠ€æœ¯å»ºè®®
    audit_results['technical_recommendations'] = [
        'ä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦',
        'æ”¹å–„ç§»åŠ¨ç«¯ä½“éªŒ',
        'æ·»åŠ ç»“æ„åŒ–æ•°æ®',
        'ä¼˜åŒ–å›¾ç‰‡altæ ‡ç­¾'
    ]
    
    return audit_results

def batch_build_websites(top_keywords: int = 10) -> Dict[str, Any]:
    """æ‰¹é‡ç”Ÿæˆç½‘ç«™"""
    print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {top_keywords} ä¸ªé«˜ä»·å€¼å…³é”®è¯çš„ç½‘ç«™...")
    
    # è·å–é«˜ä»·å€¼å…³é”®è¯
    # è¿™é‡Œåº”è¯¥ä»ä¹‹å‰çš„åˆ†æç»“æœä¸­è·å–
    high_value_keywords = [
        'AI image generator',
        'AI writing assistant', 
        'AI code helper',
        'AI video creator',
        'AI design tool'
    ][:top_keywords]
    
    build_results = {
        'build_date': datetime.now().isoformat(),
        'total_websites': len(high_value_keywords),
        'successful_builds': 0,
        'failed_builds': 0,
        'websites': []
    }
    
    for keyword in high_value_keywords:
        try:
            print(f"ğŸ”¨ æ„å»ºç½‘ç«™: {keyword}")
            
            # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ç½‘ç«™æ„å»ºé€»è¾‘
            website_info = {
                'keyword': keyword,
                'domain_suggestion': keyword.replace(' ', '-').lower() + '.com',
                'status': 'success',
                'pages_created': 5,
                'estimated_build_time': '2 minutes'
            }
            
            build_results['websites'].append(website_info)
            build_results['successful_builds'] += 1
            
        except Exception as e:
            print(f"âŒ æ„å»ºå¤±è´¥ {keyword}: {e}")
            build_results['failed_builds'] += 1
    
    return build_results

def _scheduled_task(action: str, **kwargs):
    """æ‰§è¡Œå®šæ—¶ä»»åŠ¡"""
    print(f"ğŸ¤– æ‰§è¡Œå®šæ—¶ä»»åŠ¡: {action} at {datetime.now()}")
    
    try:
        if action == 'discover':
            search_terms = kwargs.get('search_terms', ['AI tool', 'AI generator'])
            from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery
            discoverer = MultiPlatformKeywordDiscovery()
            df = discoverer.discover_all_platforms(search_terms)
            if not df.empty:
                analysis = discoverer.analyze_keyword_trends(df)
                print(f"âœ… å®šæ—¶å‘ç°å®Œæˆ: {analysis['total_keywords']} ä¸ªå…³é”®è¯")
        
        elif action == 'monitor':
            sites = kwargs.get('sites', ['canva.com', 'midjourney.com'])
            result = monitor_competitors(sites)
            print(f"âœ… å®šæ—¶ç›‘æ§å®Œæˆ: {len(result['competitors'])} ä¸ªç«å“")
            
    except Exception as e:
        print(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

def _save_competitor_monitoring_results(results: Dict, output_dir: str):
    """ä¿å­˜ç«å“ç›‘æ§ç»“æœ"""
    from src.utils.file_utils import save_results_with_timestamp
    
    file_path = save_results_with_timestamp(results, output_dir, 'competitor_monitoring')
    
    print(f"ğŸ“ ç«å“ç›‘æ§ç»“æœå·²ä¿å­˜: {file_path}")

def _save_trend_predictions(predictions: Dict, output_dir: str):
    """ä¿å­˜è¶‹åŠ¿é¢„æµ‹ç»“æœ"""
    from src.utils.file_utils import save_results_with_timestamp
    
    file_path = save_results_with_timestamp(predictions, output_dir, 'trend_predictions')
    
    print(f"ğŸ“ è¶‹åŠ¿é¢„æµ‹ç»“æœå·²ä¿å­˜: {file_path}")
