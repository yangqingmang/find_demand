"""
SEOé›†æˆè„šæœ¬ - å°†SEOä¼˜åŒ–åŠŸèƒ½é›†æˆåˆ°ç°æœ‰å»ºç«™æµç¨‹
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from .seo_optimizer import SEOOptimizer
from .enhanced_website_builder import EnhancedWebsiteBuilder


class SEOIntegrationManager:
    """SEOé›†æˆç®¡ç†å™¨"""
    
    def __init__(self):
        self.seo_optimizer = SEOOptimizer()
        self.enhanced_builder = EnhancedWebsiteBuilder()
    
    def upgrade_existing_website(self, website_path: str, config: Optional[Dict] = None) -> Dict:
        """
        å‡çº§ç°æœ‰ç½‘ç«™ï¼Œæ·»åŠ SEOä¼˜åŒ–
        
        Args:
            website_path: ç°æœ‰ç½‘ç«™è·¯å¾„
            config: å¯é€‰çš„SEOé…ç½®
        
        Returns:
            å‡çº§ç»“æœ
        """
        try:
            website_path = Path(website_path)
            
            if not website_path.exists():
                return {'success': False, 'error': f'ç½‘ç«™è·¯å¾„ä¸å­˜åœ¨: {website_path}'}
            
            # 1. è¯»å–ç°æœ‰HTMLæ–‡ä»¶
            index_file = website_path / 'index.html'
            if not index_file.exists():
                return {'success': False, 'error': 'æœªæ‰¾åˆ°index.htmlæ–‡ä»¶'}
            
            original_html = index_file.read_text(encoding='utf-8')
            
            # 2. è¯»å–é¡¹ç›®ä¿¡æ¯
            project_info = self._load_project_info(website_path)
            
            # 3. ç”ŸæˆSEOé…ç½®
            if config is None:
                config = self._generate_default_seo_config(project_info, original_html)
            
            # 4. åº”ç”¨SEOä¼˜åŒ–
            optimized_html = self.seo_optimizer.optimize_html(original_html, config)
            
            # 5. å¤‡ä»½åŸæ–‡ä»¶
            backup_path = self._create_backup(website_path)
            
            # 6. ä¿å­˜ä¼˜åŒ–åçš„æ–‡ä»¶
            index_file.write_text(optimized_html, encoding='utf-8')
            
            # 7. ç”Ÿæˆé¢å¤–çš„SEOæ–‡ä»¶
            self._generate_seo_files(website_path, config)
            
            # 8. ç”Ÿæˆå‡çº§æŠ¥å‘Š
            upgrade_report = self._generate_upgrade_report(config, original_html, optimized_html)
            
            return {
                'success': True,
                'website_path': str(website_path),
                'backup_path': str(backup_path),
                'seo_config': config,
                'upgrade_report': upgrade_report
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def batch_upgrade_websites(self, websites_dir: str) -> Dict:
        """
        æ‰¹é‡å‡çº§ç½‘ç«™ç›®å½•ä¸‹çš„æ‰€æœ‰ç½‘ç«™
        
        Args:
            websites_dir: ç½‘ç«™ç›®å½•è·¯å¾„
        
        Returns:
            æ‰¹é‡å‡çº§ç»“æœ
        """
        websites_dir = Path(websites_dir)
        results = []
        
        if not websites_dir.exists():
            return {'success': False, 'error': f'ç½‘ç«™ç›®å½•ä¸å­˜åœ¨: {websites_dir}'}
        
        # æŸ¥æ‰¾æ‰€æœ‰ç½‘ç«™é¡¹ç›®
        for project_dir in websites_dir.iterdir():
            if project_dir.is_dir() and (project_dir / 'index.html').exists():
                print(f"æ­£åœ¨å‡çº§ç½‘ç«™: {project_dir.name}")
                result = self.upgrade_existing_website(str(project_dir))
                results.append({
                    'project': project_dir.name,
                    'result': result
                })
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r['result']['success'])
        failed = len(results) - successful
        
        return {
            'success': True,
            'total_projects': len(results),
            'successful': successful,
            'failed': failed,
            'results': results
        }
    
    def _load_project_info(self, website_path: Path) -> Dict:
        """åŠ è½½é¡¹ç›®ä¿¡æ¯"""
        project_info_file = website_path / 'project_info.json'
        
        if project_info_file.exists():
            try:
                return json.loads(project_info_file.read_text(encoding='utf-8'))
            except:
                pass
        
        # å¦‚æœæ²¡æœ‰é¡¹ç›®ä¿¡æ¯æ–‡ä»¶ï¼Œä»ç›®å½•åæ¨æ–­
        dir_name = website_path.name
        return {
            'project_name': dir_name.replace('_', ' ').title(),
            'main_keyword': dir_name.split('_')[0] if '_' in dir_name else dir_name,
            'theme': 'general',
            'created_at': datetime.now().isoformat()
        }
    
    def _generate_default_seo_config(self, project_info: Dict, html_content: str) -> Dict:
        """ç”Ÿæˆé»˜è®¤SEOé…ç½®"""
        main_keyword = project_info.get('main_keyword', 'website')
        
        # ä»HTMLä¸­æå–ç°æœ‰ä¿¡æ¯
        existing_title = self._extract_title_from_html(html_content)
        existing_description = self._extract_description_from_html(html_content)
        
        config = {
            'title': existing_title or f"{main_keyword.title()} - Professional Guide & Resources",
            'description': existing_description or f"Comprehensive guide and resources for {main_keyword}. Get expert insights, tips, and tools.",
            'keywords': f"{main_keyword}, guide, resources, tips, tools, professional",
            'canonical_url': '',  # éœ€è¦ç”¨æˆ·å¡«å†™
            'robots': 'index, follow',
            'language': 'en',
            'author': 'Expert Team',
            'copyright': f'Â© {datetime.now().year} {main_keyword.title()} Guide',
            'theme_color': '#3498db',
            'locale': 'en_US',
            'site_name': f'{main_keyword.title()} Guide',
            'twitter_card_type': 'summary_large_image',
            'add_breadcrumb': True,
            'faqs': self._generate_default_faqs(main_keyword)
        }
        
        return config
    
    def _extract_title_from_html(self, html_content: str) -> Optional[str]:
        """ä»HTMLä¸­æå–æ ‡é¢˜"""
        import re
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
        return title_match.group(1).strip() if title_match else None
    
    def _extract_description_from_html(self, html_content: str) -> Optional[str]:
        """ä»HTMLä¸­æå–æè¿°"""
        import re
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']+)["\']', html_content, re.IGNORECASE)
        return desc_match.group(1).strip() if desc_match else None
    
    def _generate_default_faqs(self, main_keyword: str) -> List[Dict]:
        """ç”Ÿæˆé»˜è®¤FAQ"""
        return [
            {
                'question': f'What is {main_keyword}?',
                'answer': f'{main_keyword.title()} is a comprehensive solution that provides users with advanced features and capabilities for their specific needs.'
            },
            {
                'question': f'How do I get started with {main_keyword}?',
                'answer': f'Getting started with {main_keyword} is easy. Simply follow our step-by-step guide and you\'ll be up and running in no time.'
            },
            {
                'question': f'Is {main_keyword} suitable for beginners?',
                'answer': f'Yes, {main_keyword} is designed to be user-friendly for both beginners and advanced users. We provide comprehensive documentation and support.'
            }
        ]
    
    def _create_backup(self, website_path: Path) -> Path:
        """åˆ›å»ºå¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = website_path.parent / f"{website_path.name}_backup_{timestamp}"
        
        # å¤åˆ¶æ•´ä¸ªç›®å½•
        import shutil
        shutil.copytree(website_path, backup_path)
        
        return backup_path
    
    def _generate_seo_files(self, website_path: Path, config: Dict):
        """ç”ŸæˆSEOç›¸å…³æ–‡ä»¶"""
        # ç”Ÿæˆsitemap.xml
        sitemap_content = self._generate_sitemap(config)
        (website_path / 'sitemap.xml').write_text(sitemap_content, encoding='utf-8')
        
        # ç”Ÿæˆrobots.txt
        robots_content = self._generate_robots_txt(config)
        (website_path / 'robots.txt').write_text(robots_content, encoding='utf-8')
        
        # æ›´æ–°æˆ–åˆ›å»ºvercel.json
        vercel_config = self._generate_vercel_config()
        (website_path / 'vercel.json').write_text(vercel_config, encoding='utf-8')
    
    def _generate_sitemap(self, config: Dict) -> str:
        """ç”ŸæˆSitemap"""
        base_url = config.get('canonical_url', 'https://example.com')
        current_date = datetime.now().strftime('%Y-%m-%d')
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>{base_url}</loc>
        <lastmod>{current_date}</lastmod>
        <changefreq>weekly</changefreq>
        <priority>1.0</priority>
    </url>
</urlset>"""
    
    def _generate_robots_txt(self, config: Dict) -> str:
        """ç”Ÿæˆrobots.txt"""
        base_url = config.get('canonical_url', 'https://example.com')
        
        return f"""User-agent: *
Allow: /

Sitemap: {base_url}/sitemap.xml"""
    
    def _generate_vercel_config(self) -> str:
        """ç”ŸæˆVercelé…ç½®"""
        config = {
            "version": 2,
            "builds": [
                {
                    "src": "**/*",
                    "use": "@vercel/static"
                }
            ],
            "routes": [
                {
                    "src": "/(.*)",
                    "dest": "/$1"
                }
            ],
            "headers": [
                {
                    "source": "/(.*)",
                    "headers": [
                        {
                            "key": "X-Content-Type-Options",
                            "value": "nosniff"
                        },
                        {
                            "key": "X-Frame-Options",
                            "value": "DENY"
                        },
                        {
                            "key": "X-XSS-Protection",
                            "value": "1; mode=block"
                        }
                    ]
                }
            ]
        }
        
        return json.dumps(config, indent=2)
    
    def _generate_upgrade_report(self, config: Dict, original_html: str, optimized_html: str) -> Dict:
        """ç”Ÿæˆå‡çº§æŠ¥å‘Š"""
        # è®¡ç®—æ”¹è¿›é¡¹ç›®
        improvements = []
        
        # æ£€æŸ¥æ·»åŠ çš„æ ‡ç­¾
        if 'og:title' in optimized_html and 'og:title' not in original_html:
            improvements.append('æ·»åŠ äº†Open Graphæ ‡ç­¾')
        
        if 'twitter:card' in optimized_html and 'twitter:card' not in original_html:
            improvements.append('æ·»åŠ äº†Twitter Cards')
        
        if 'application/ld+json' in optimized_html and 'application/ld+json' not in original_html:
            improvements.append('æ·»åŠ äº†ç»“æ„åŒ–æ•°æ®')
        
        if 'breadcrumb' in optimized_html.lower() and 'breadcrumb' not in original_html.lower():
            improvements.append('æ·»åŠ äº†é¢åŒ…å±‘å¯¼èˆª')
        
        if 'faq' in optimized_html.lower() and 'faq' not in original_html.lower():
            improvements.append('æ·»åŠ äº†FAQéƒ¨åˆ†')
        
        # è®¡ç®—SEOå¾—åˆ†
        seo_score = self._calculate_seo_score(optimized_html)
        
        return {
            'timestamp': datetime.now().isoformat(),
            'seo_score': seo_score,
            'improvements_made': improvements,
            'files_added': ['sitemap.xml', 'robots.txt', 'vercel.json'],
            'recommendations': [
                'è®¾ç½®æ­£ç¡®çš„canonical_url',
                'æ·»åŠ é«˜è´¨é‡çš„ç½‘ç«™å›¾ç‰‡',
                'å®šæœŸæ›´æ–°å†…å®¹',
                'ç›‘æ§ç½‘ç«™æ€§èƒ½',
                'æµ‹è¯•ç§»åŠ¨ç«¯ä½“éªŒ'
            ]
        }
    
    def _calculate_seo_score(self, html_content: str) -> int:
        """è®¡ç®—SEOå¾—åˆ†"""
        score = 0
        
        # åŸºç¡€æ ‡ç­¾æ£€æŸ¥
        if '<title>' in html_content: score += 5
        if 'name="description"' in html_content: score += 5
        if 'name="keywords"' in html_content: score += 3
        if 'rel="canonical"' in html_content: score += 4
        if 'name="robots"' in html_content: score += 3
        
        # ç¤¾äº¤åª’ä½“æ ‡ç­¾
        if 'og:title' in html_content: score += 8
        if 'twitter:card' in html_content: score += 7
        
        # ç»“æ„åŒ–æ•°æ®
        if 'application/ld+json' in html_content: score += 20
        
        # è¯­ä¹‰åŒ–HTML
        if '<header>' in html_content: score += 5
        if '<main>' in html_content: score += 5
        if '<section>' in html_content: score += 5
        if '<footer>' in html_content: score += 5
        
        # å…¶ä»–ä¼˜åŒ–
        if 'viewport' in html_content: score += 8
        if 'breadcrumb' in html_content.lower(): score += 5
        if 'faq' in html_content.lower(): score += 5
        
        return min(score, 100)


def main():
    """ä¸»å‡½æ•° - æä¾›å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SEOé›†æˆå·¥å…·')
    parser.add_argument('--upgrade', type=str, help='å‡çº§æŒ‡å®šç½‘ç«™è·¯å¾„')
    parser.add_argument('--batch', type=str, help='æ‰¹é‡å‡çº§ç½‘ç«™ç›®å½•')
    parser.add_argument('--config', type=str, help='è‡ªå®šä¹‰SEOé…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    manager = SEOIntegrationManager()
    
    if args.upgrade:
        # å•ä¸ªç½‘ç«™å‡çº§
        config = None
        if args.config:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
        
        result = manager.upgrade_existing_website(args.upgrade, config)
        
        if result['success']:
            print(f"âœ… ç½‘ç«™å‡çº§æˆåŠŸ!")
            print(f"ğŸ“ ç½‘ç«™è·¯å¾„: {result['website_path']}")
            print(f"ğŸ’¾ å¤‡ä»½è·¯å¾„: {result['backup_path']}")
            print(f"ğŸ“Š SEOå¾—åˆ†: {result['upgrade_report']['seo_score']}/100")
            
            print("\nğŸ”§ åº”ç”¨çš„æ”¹è¿›:")
            for improvement in result['upgrade_report']['improvements_made']:
                print(f"  âœ“ {improvement}")
            
            print("\nğŸ“„ æ·»åŠ çš„æ–‡ä»¶:")
            for file in result['upgrade_report']['files_added']:
                print(f"  + {file}")
        else:
            print(f"âŒ ç½‘ç«™å‡çº§å¤±è´¥: {result['error']}")
    
    elif args.batch:
        # æ‰¹é‡å‡çº§
        result = manager.batch_upgrade_websites(args.batch)
        
        if result['success']:
            print(f"âœ… æ‰¹é‡å‡çº§å®Œæˆ!")
            print(f"ğŸ“Š æ€»é¡¹ç›®æ•°: {result['total_projects']}")
            print(f"âœ… æˆåŠŸ: {result['successful']}")
            print(f"âŒ å¤±è´¥: {result['failed']}")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
            for project_result in result['results']:
                status = "âœ…" if project_result['result']['success'] else "âŒ"
                print(f"  {status} {project_result['project']}")
        else:
            print(f"âŒ æ‰¹é‡å‡çº§å¤±è´¥: {result['error']}")
    
    else:
        print("è¯·æŒ‡å®š --upgrade æˆ– --batch å‚æ•°")
        print("ç¤ºä¾‹:")
        print("  python seo_integration_script.py --upgrade /path/to/website")
        print("  python seo_integration_script.py --batch /path/to/websites/directory")


if __name__ == "__main__":
    main()