#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘½ä»¤å¤„ç†å™¨æ¨¡å—
å¤„ç†å„ç§å‘½ä»¤è¡Œå‚æ•°å¯¹åº”çš„åŠŸèƒ½
"""

import os
import sys
import tempfile
import pandas as pd
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from src.cli_parser import print_quiet_summary
from src.utils.enhanced_features import (
    monitor_competitors, predict_keyword_trends, generate_seo_audit,
    batch_build_websites
)
from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery


def handle_stats_display(manager, args):
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤º"""
    if args.stats:
        stats = manager.get_manager_stats()
        print("\nğŸ“Š ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯:")
        for manager_name, manager_stats in stats.items():
            if isinstance(manager_stats, dict):
                print(f"\n{manager_name}:")
                for key, value in manager_stats.items():
                    print(f"  {key}: {value}")
            else:
                print(f"{manager_name}: {manager_stats}")
        return True
    return False


def handle_input_file_analysis(manager, args):
    """å¤„ç†å…³é”®è¯æ–‡ä»¶åˆ†æ"""
    if not args.input:
        return False
        
    # åˆ†æå…³é”®è¯æ–‡ä»¶
    if not args.quiet:
        print("ğŸš€ å¼€å§‹åˆ†æå…³é”®è¯æ–‡ä»¶...")
        if args.serp:
            print("ğŸ” å·²å¯ç”¨SERPåˆ†æåŠŸèƒ½")
    
    result = manager.analyze_keywords(args.input, args.output, enable_serp=args.serp)
    
    # æ˜¾ç¤ºç»“æœ
    if args.quiet:
        print_quiet_summary(result)
    else:
        print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
        print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
        print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
        
        # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
        if 'new_word_summary' in result and result['new_word_summary'].get('new_words_detected', 0) > 0:
            summary = result['new_word_summary']
            print(f"ğŸ” æ–°è¯æ£€æµ‹: å‘ç° {summary['new_words_detected']} ä¸ªæ–°è¯ ({summary['new_word_percentage']}%)")
            print(f"   é«˜ç½®ä¿¡åº¦æ–°è¯: {summary['high_confidence_new_words']} ä¸ª")

        # æ˜¾ç¤ºSERPåˆ†ææ‘˜è¦
        if 'serp_summary' in result and result['serp_summary'].get('serp_analysis_enabled', False):
            serp_summary = result['serp_summary']
            print(f"ğŸ” SERPåˆ†æ: åˆ†æäº† {serp_summary['total_analyzed']} ä¸ªå…³é”®è¯")
            print(f"   é«˜ç½®ä¿¡åº¦SERP: {serp_summary['high_confidence_serp']} ä¸ª")
            print(f"   å•†ä¸šæ„å›¾å…³é”®è¯: {serp_summary['commercial_intent_keywords']} ä¸ª")

        # æ˜¾ç¤ºTop 5å…³é”®è¯
        top_keywords = result['market_insights']['top_opportunities'][:5]
        if top_keywords:
            print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
            for i, kw in enumerate(top_keywords, 1):
                intent_desc = kw['intent']['intent_description']
                score = kw['opportunity_score']
                new_word_info = ""
                if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                    new_word_grade = kw['new_word_detection']['new_word_grade']
                    new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
    
    return True


def handle_keywords_analysis(manager, args):
    """å¤„ç†å•ä¸ªå…³é”®è¯åˆ†æ"""
    if not args.keywords:
        return False
        
    # åˆ†æå•ä¸ªå…³é”®è¯
    if not args.quiet:
        print("ğŸš€ å¼€å§‹åˆ†æè¾“å…¥çš„å…³é”®è¯...")
    
    # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶
    temp_df = pd.DataFrame([{'query': kw} for kw in args.keywords])
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
        temp_df.to_csv(f.name, index=False)
        temp_file = f.name
    
    try:
        result = manager.analyze_keywords(temp_file, args.output, enable_serp=args.serp)
        
        # æ˜¾ç¤ºç»“æœ
        if args.quiet:
            print_quiet_summary(result)
        else:
            print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {len(args.keywords)} ä¸ªå…³é”®è¯")
            
            # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„ç»“æœ
            print("\nğŸ“‹ å…³é”®è¯åˆ†æç»“æœ:")
            for kw_result in result['keywords']:
                keyword = kw_result['keyword']
                score = kw_result['opportunity_score']
                intent = kw_result['intent']['intent_description']
                print(f"   â€¢ {keyword}: æœºä¼šåˆ†æ•° {score}, æ„å›¾: {intent}")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file)
    
    return True


def handle_discover_analysis(manager, args):
    """å¤„ç†å¤šå¹³å°å…³é”®è¯å‘ç°"""
    if not args.discover:
        return False
        
    # å¤šå¹³å°å…³é”®è¯å‘ç°
    search_terms = args.discover if args.discover != ['default'] else ['AI tool', 'AI generator', 'AI assistant']
    
    if not args.quiet:
        print("ğŸ” å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
        print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
    
    try:
        # åˆ›å»ºå‘ç°å·¥å…·
        discoverer = MultiPlatformKeywordDiscovery()
        
        # æ‰§è¡Œå‘ç°
        df = discoverer.discover_all_platforms(search_terms)

        if not df.empty:
            # åˆ†æè¶‹åŠ¿
            analysis = discoverer.analyze_keyword_trends(df)
            
            # ä¿å­˜ç»“æœ
            output_dir = os.path.join(args.output, 'multi_platform_discovery')
            csv_path, json_path = discoverer.save_results(df, analysis, output_dir)
            
            if args.quiet:
                # é™é»˜æ¨¡å¼æ˜¾ç¤º
                print(f"\nğŸ¯ å¤šå¹³å°å…³é”®è¯å‘ç°ç»“æœ:")
                print(f"   â€¢ å‘ç°å…³é”®è¯: {analysis['total_keywords']} ä¸ª")
                print(f"   â€¢ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                
                # æ˜¾ç¤ºTop 3å…³é”®è¯
                top_keywords = analysis['top_keywords_by_score'][:3]
                if top_keywords:
                    print("\nğŸ† Top 3 çƒ­é—¨å…³é”®è¯:")
                    for i, kw in enumerate(top_keywords, 1):
                        print(f"   {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            else:
                # è¯¦ç»†æ¨¡å¼æ˜¾ç¤º
                print(f"\nğŸ‰ å¤šå¹³å°å…³é”®è¯å‘ç°å®Œæˆ!")
                print(f"ğŸ“Š å‘ç° {analysis['total_keywords']} ä¸ªå…³é”®è¯")
                print(f"ğŸŒ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                
                print(f"\nğŸ† çƒ­é—¨å…³é”®è¯:")
                for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                    print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            
            print(f"\nğŸ“ ç»“æœå·²ä¿å­˜:")
            print(f"  CSV: {csv_path}")
            print(f"  JSON: {json_path}")
            
            # è¯¢é—®æ˜¯å¦è¦ç«‹å³åˆ†æå‘ç°çš„å…³é”®è¯
            if not args.quiet:
                user_input = input("\nğŸ¤” æ˜¯å¦è¦ç«‹å³åˆ†æè¿™äº›å…³é”®è¯çš„æ„å›¾å’Œå¸‚åœºæœºä¼š? (y/n): ")
                if user_input.lower() in ['y', 'yes', 'æ˜¯']:
                    print("ğŸ”„ å¼€å§‹åˆ†æå‘ç°çš„å…³é”®è¯...")
                    result = manager.analyze_keywords(csv_path, args.output)
                    print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                    print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
        else:
            print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
            
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
    except Exception as e:
        print(f"âŒ å¤šå¹³å°å…³é”®è¯å‘ç°å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    
    return True


def handle_enhanced_features(manager, args):
    """å¤„ç†å¢å¼ºåŠŸèƒ½ (ç«å“ç›‘æ§ã€è¶‹åŠ¿é¢„æµ‹ã€SEOå®¡è®¡ã€æ‰¹é‡å»ºç«™)"""
    # ç«å“ç›‘æ§
    if args.monitor_competitors:
        sites = args.sites or ['canva.com', 'midjourney.com', 'openai.com']
        if not args.quiet:
            print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(sites)} ä¸ªç«å“ç½‘ç«™...")
        
        result = monitor_competitors(sites, args.output)
        print(f"âœ… ç«å“ç›‘æ§å®Œæˆ: åˆ†æäº† {len(result['competitors'])} ä¸ªç«å“")
        
        if not args.quiet:
            print("\nğŸ“Š ç›‘æ§ç»“æœæ‘˜è¦:")
            for comp in result['competitors'][:3]:
                print(f"  â€¢ {comp['site']}: {comp['new_keywords_count']} ä¸ªæ–°å…³é”®è¯")
        return True
    
    # è¶‹åŠ¿é¢„æµ‹
    if args.predict_trends:
        if not args.quiet:
            print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {args.timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
        
        result = predict_keyword_trends(args.timeframe, args.output)
        print(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆ: é¢„æµ‹äº† {len(result['rising_keywords'])} ä¸ªä¸Šå‡å…³é”®è¯")
        
        if not args.quiet:
            print("\nğŸ“ˆ è¶‹åŠ¿é¢„æµ‹æ‘˜è¦:")
            for kw in result['rising_keywords'][:3]:
                print(f"  ğŸ“ˆ {kw['keyword']}: {kw['predicted_growth']} (ç½®ä¿¡åº¦: {kw['confidence']:.0%})")
        return True
    
    # SEOå®¡è®¡
    if args.seo_audit:
        if not args.domain:
            print("âŒ è¯·æŒ‡å®šè¦å®¡è®¡çš„åŸŸå (--domain)")
            return True
        
        if not args.quiet:
            print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {args.domain}")
        
        result = generate_seo_audit(args.domain, args.keywords)
        print(f"âœ… SEOå®¡è®¡å®Œæˆ: å‘ç° {len(result['keyword_opportunities'])} ä¸ªå…³é”®è¯æœºä¼š")
        
        if not args.quiet:
            print("\nğŸ¯ SEOä¼˜åŒ–å»ºè®®:")
            for gap in result['content_gaps'][:3]:
                print(f"  â€¢ {gap}")
        return True
    
    # æ‰¹é‡å»ºç«™
    if args.build_websites:
        if not args.quiet:
            print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {args.top_keywords} ä¸ªç½‘ç«™...")
        
        result = batch_build_websites(args.top_keywords, args.output)
        print(f"âœ… æ‰¹é‡å»ºç«™å®Œæˆ: æˆåŠŸæ„å»º {result['successful_builds']} ä¸ªç½‘ç«™")
        
        if not args.quiet:
            print("\nğŸŒ æ„å»ºçš„ç½‘ç«™:")
            for site in result['websites'][:3]:
                print(f"  â€¢ {site['keyword']}: {site['domain_suggestion']}")
        return True
    
    return False


def handle_hot_keywords(manager, args):
    """å¤„ç†çƒ­é—¨å…³é”®è¯æœç´¢å’Œéœ€æ±‚æŒ–æ˜"""
    if not args.hotkeywords:
        return False
    
    # æœç´¢çƒ­é—¨å…³é”®è¯ï¼šä½¿ç”¨ fetch_rising_queries è·å–å…³é”®è¯å¹¶è¿›è¡Œéœ€æ±‚æŒ–æ˜
    if not args.quiet:
        print("ğŸ”¥ å¼€å§‹æœç´¢çƒ­é—¨å…³é”®è¯å¹¶è¿›è¡Œéœ€æ±‚æŒ–æ˜...")
    
    try:
        # ä½¿ç”¨å•ä¾‹è·å– TrendsCollector
        from src.collectors.trends_singleton import get_trends_collector
        
        # è·å– TrendsCollector å•ä¾‹å®ä¾‹
        trends_collector = get_trends_collector()
        
        # ä½¿ç”¨ fetch_rising_queries è·å–çƒ­é—¨å…³é”®è¯
        if not args.quiet:
            print("ğŸ” æ­£åœ¨è·å– Rising Queries...")
        
        rising_queries = trends_collector.fetch_rising_queries()
        
        # å°† rising queries è½¬æ¢ä¸ºDataFrameæ ¼å¼
        if isinstance(rising_queries, pd.DataFrame):
            # å¦‚æœå·²ç»æ˜¯DataFrameï¼Œç›´æ¥ä½¿ç”¨
            trending_df = rising_queries.head(20)  # é™åˆ¶å‰20ä¸ª
            # ç¡®ä¿æœ‰queryåˆ—
            if 'query' not in trending_df.columns:
                if 'title' in trending_df.columns:
                    trending_df = trending_df.rename(columns={'title': 'query'})
                elif len(trending_df.columns) > 0:
                    trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
        elif rising_queries and len(rising_queries) > 0:
            # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            if isinstance(rising_queries[0], str):
                trending_df = pd.DataFrame([
                    {'query': query}
                    for query in rising_queries[:20]  # é™åˆ¶å‰20ä¸ª
                ])
            # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸åˆ—è¡¨
            elif isinstance(rising_queries[0], dict):
                trending_df = pd.DataFrame([
                    {
                        'query': item.get('query', item.get('keyword', str(item))),
                        'value': item.get('value', item.get('interest', 0))
                    }
                    for item in rising_queries[:20]  # é™åˆ¶å‰20ä¸ª
                ])
            else:
                # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                trending_df = pd.DataFrame([
                    {'query': str(query)}
                    for query in rising_queries[:20]
                ])
        else:
            trending_df = pd.DataFrame(columns=['query'])

        if trending_df is not None and not trending_df.empty:
            # ä¿å­˜çƒ­é—¨å…³é”®è¯åˆ°ä¸´æ—¶æ–‡ä»¶
            
            # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
            if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                # å¦‚æœæ²¡æœ‰queryåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºå…³é”®è¯
                trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                trending_df.to_csv(f.name, index=False)
                temp_file = f.name
            
            try:
                if not args.quiet:
                    print(f"ğŸ” è·å–åˆ° {len(trending_df)} ä¸ª Rising Queriesï¼Œå¼€å§‹éœ€æ±‚æŒ–æ˜åˆ†æ...")
                
                # æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æï¼Œç¦ç”¨æ–°è¯æ£€æµ‹é¿å…429é”™è¯¯
                manager.new_word_detection_available = False
                result = manager.analyze_keywords(temp_file, args.output, enable_serp=False)
                
                # æ˜¾ç¤ºç»“æœ
                if args.quiet:
                    print_quiet_summary(result)
                else:
                    print(f"\nğŸ‰ éœ€æ±‚æŒ–æ˜åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ª Rising Queries")
                    print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                    print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
                    
                    # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
                    if 'new_word_summary' in result and result['new_word_summary'].get('new_words_detected', 0) > 0:
                        summary = result['new_word_summary']
                        print(f"ğŸ” æ–°è¯æ£€æµ‹: å‘ç° {summary['new_words_detected']} ä¸ªæ–°è¯ ({summary['new_word_percentage']}%)")
                        print(f"   é«˜ç½®ä¿¡åº¦æ–°è¯: {summary['high_confidence_new_words']} ä¸ª")

                    # æ˜¾ç¤ºTop 5æœºä¼šå…³é”®è¯
                    top_keywords = result['market_insights']['top_opportunities'][:5]
                    if top_keywords:
                        print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                        for i, kw in enumerate(top_keywords, 1):
                            intent_desc = kw['intent']['intent_description']
                            score = kw['opportunity_score']
                            new_word_info = ""
                            if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                                new_word_grade = kw['new_word_detection']['new_word_grade']
                                new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                            print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
                    
                    # æ˜¾ç¤ºåŸå§‹Rising Queriesä¿¡æ¯
                    print(f"\nğŸ”¥ åŸå§‹ Rising Queries æ•°æ®:")
                    print(f"   â€¢ æ•°æ®æ¥æº: Google Trends Rising Queries")
                    if 'value' in trending_df.columns:
                        print(f"   â€¢ å¹³å‡çƒ­åº¦: {trending_df['value'].mean():.1f}")
                    
                    # ä¿å­˜åŸå§‹Rising Queries
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    trending_output_file = os.path.join(args.output, f"rising_queries_raw_{timestamp}.csv")
                    os.makedirs(args.output, exist_ok=True)
                    trending_df.to_csv(trending_output_file, index=False, encoding='utf-8')
                    print(f"ğŸ“ åŸå§‹ Rising Queries å·²ä¿å­˜åˆ°: {trending_output_file}")
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file)
                
        else:
            # å½“æ— æ³•è·å–Rising Queriesæ—¶ï¼Œç›´æ¥æŠ¥å‘Šå¤±è´¥
            print("âŒ æ— æ³•è·å– Rising Queriesï¼Œå¯èƒ½çš„åŸå› :")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ç¨åé‡è¯•")
            print("   3. æˆ–ä½¿ç”¨ --input å‚æ•°æŒ‡å®šå…³é”®è¯æ–‡ä»¶è¿›è¡Œåˆ†æ")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ è·å– Rising Queries æˆ–éœ€æ±‚æŒ–æ˜æ—¶å‡ºé”™: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    
    return True


def handle_all_workflow(manager, args):
    """å¤„ç†å®Œæ•´çš„å…³é”®è¯åˆ†æå·¥ä½œæµç¨‹"""
    if not args.all:
        return False
    
    if not args.quiet:
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„å…³é”®è¯åˆ†æå·¥ä½œæµç¨‹...")
        print("   ç¬¬ä¸€æ­¥: æœç´¢çƒ­é—¨å…³é”®è¯")
        print("   ç¬¬äºŒæ­¥: åŸºäºçƒ­é—¨å…³é”®è¯è¿›è¡Œå¤šå¹³å°å‘ç°")
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šè·å–çƒ­é—¨å…³é”®è¯
        from src.collectors.trends_singleton import get_trends_collector
        trends_collector = get_trends_collector()
        
        if not args.quiet:
            print("ğŸ” æ­£åœ¨è·å– Rising Queries...")
        
        rising_queries = trends_collector.fetch_rising_queries()
        
        # å¤„ç†è·å–åˆ°çš„çƒ­é—¨å…³é”®è¯
        if isinstance(rising_queries, pd.DataFrame):
            trending_df = rising_queries.head(20)
            if 'query' not in trending_df.columns:
                if 'title' in trending_df.columns:
                    trending_df = trending_df.rename(columns={'title': 'query'})
                elif len(trending_df.columns) > 0:
                    trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
        elif rising_queries and len(rising_queries) > 0:
            if isinstance(rising_queries[0], str):
                trending_df = pd.DataFrame([
                    {'query': query}
                    for query in rising_queries[:20]
                ])
            elif isinstance(rising_queries[0], dict):
                trending_df = pd.DataFrame([
                    {
                        'query': item.get('query', item.get('keyword', str(item))),
                        'value': item.get('value', item.get('interest', 0))
                    }
                    for item in rising_queries[:20]
                ])
            else:
                trending_df = pd.DataFrame([
                    {'query': str(query)}
                    for query in rising_queries[:20]
                ])
        else:
            trending_df = pd.DataFrame(columns=['query'])
        
        if trending_df is not None and not trending_df.empty:
            # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
            if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
            
            # ç¬¬ä¸€æ­¥ï¼šå¯¹çƒ­é—¨å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                trending_df.to_csv(f.name, index=False)
                temp_file = f.name
            
            try:
                if not args.quiet:
                    print(f"ğŸ” ç¬¬ä¸€æ­¥: å¯¹ {len(trending_df)} ä¸ªçƒ­é—¨å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜...")
                
                # æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
                manager.new_word_detection_available = False
                hot_result = manager.analyze_keywords(temp_file, args.output, enable_serp=False)
                
                if not args.quiet:
                    print(f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ! åˆ†æäº† {hot_result['total_keywords']} ä¸ªçƒ­é—¨å…³é”®è¯")
                    print(f"ğŸ“Š å‘ç° {hot_result['market_insights']['high_opportunity_count']} ä¸ªé«˜æœºä¼šå…³é”®è¯")
                
                # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨çƒ­é—¨å…³é”®è¯ä½œä¸ºç§å­è¿›è¡Œå¤šå¹³å°å‘ç°
                if not args.quiet:
                    print("\nğŸŒ ç¬¬äºŒæ­¥: åŸºäºçƒ­é—¨å…³é”®è¯è¿›è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°...")
                
                # è·å–å‰10ä¸ªçƒ­é—¨å…³é”®è¯ä½œä¸ºç§å­
                seed_keywords = trending_df['query'].head(10).tolist()
                
                # æ‰§è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°
                discovery_tool = MultiPlatformKeywordDiscovery()
                
                all_discovered_keywords = []
                for keyword in seed_keywords:
                    if not args.quiet:
                        print(f"ğŸ” æ­£åœ¨å‘ç°ä¸ '{keyword}' ç›¸å…³çš„å…³é”®è¯...")
                    
                    discovered = discovery_tool.discover_keywords(
                        keyword,
                        platforms=['baidu', 'google', 'bing'],
                        max_keywords_per_platform=20
                    )
                    all_discovered_keywords.extend(discovered)
                
                # å»é‡å¹¶ä¿å­˜å‘ç°çš„å…³é”®è¯
                unique_keywords = list(set(all_discovered_keywords))
                
                if unique_keywords:
                    # åˆ›å»ºå‘ç°å…³é”®è¯çš„CSVæ–‡ä»¶
                    discovered_df = pd.DataFrame([
                        {'keyword': kw} for kw in unique_keywords
                    ])
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    discovered_file = os.path.join(args.output, f"discovered_keywords_{timestamp}.csv")
                    os.makedirs(args.output, exist_ok=True)
                    discovered_df.to_csv(discovered_file, index=False, encoding='utf-8')
                    
                    if not args.quiet:
                        print(f"ğŸ¯ å‘ç°äº† {len(unique_keywords)} ä¸ªç›¸å…³å…³é”®è¯")
                        print(f"ğŸ“ å‘ç°çš„å…³é”®è¯å·²ä¿å­˜åˆ°: {discovered_file}")
                    
                    # å¯¹å‘ç°çš„å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
                    if not args.quiet:
                        print(f"\nğŸ” ç¬¬ä¸‰æ­¥: å¯¹å‘ç°çš„å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ...")
                    
                    discovery_result = manager.analyze_keywords(discovered_file, args.output, enable_serp=False)
                    
                    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
                    if args.quiet:
                        print_quiet_summary(discovery_result)
                    else:
                        print(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆ!")
                        print(f"ğŸ“Š çƒ­é—¨å…³é”®è¯åˆ†æ: {hot_result['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸŒ å¤šå¹³å°å‘ç°: {len(unique_keywords)} ä¸ªç›¸å…³å…³é”®è¯")
                        print(f"ğŸ¯ æœ€ç»ˆåˆ†æ: {discovery_result['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸ† æ€»è®¡é«˜æœºä¼šå…³é”®è¯: {discovery_result['market_insights']['high_opportunity_count']} ä¸ª")
                        print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {discovery_result['market_insights']['avg_opportunity_score']}")
                        
                        # æ˜¾ç¤ºTop 5æœºä¼šå…³é”®è¯
                        top_keywords = discovery_result['market_insights']['top_opportunities'][:5]
                        if top_keywords:
                            print("\nğŸ† Top 5 æœ€ç»ˆæœºä¼šå…³é”®è¯:")
                            for i, kw in enumerate(top_keywords, 1):
                                intent_desc = kw['intent']['intent_description']
                                score = kw['opportunity_score']
                                print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc})")
                
                else:
                    if not args.quiet:
                        print("âš ï¸ æœªå‘ç°ç›¸å…³å…³é”®è¯ï¼Œä»…æ˜¾ç¤ºçƒ­é—¨å…³é”®è¯åˆ†æç»“æœ")
                    
                    if args.quiet:
                        print_quiet_summary(hot_result)
                    else:
                        print(f"\nğŸ‰ çƒ­é—¨å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {hot_result['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {hot_result['market_insights']['high_opportunity_count']} ä¸ª")
                        print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {hot_result['market_insights']['avg_opportunity_score']}")
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file)
        
        else:
            print("âŒ æ— æ³•è·å–çƒ­é—¨å…³é”®è¯ï¼Œå·¥ä½œæµç¨‹ç»ˆæ­¢")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ç¨åé‡è¯•")
            print("   3. æˆ–ä½¿ç”¨å…¶ä»–å‚æ•°è¿›è¡Œåˆ†æ")
            sys.exit(1)
    
    except Exception as e:
        print(f"âŒ å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡Œæ—¶å‡ºé”™: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    
    return True


def handle_demand_validation(manager, args):
    """
    å¤„ç†éœ€æ±‚éªŒè¯æµç¨‹
    
    Args:
        manager: IntegratedDemandMiningManagerå®ä¾‹
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("ğŸ” å¼€å§‹éœ€æ±‚éªŒè¯æµç¨‹...")
    print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–é«˜æœºä¼šå…³é”®è¯")
    
    try:
        from src.collectors.trends_singleton import get_trends_collector
        trends_collector = get_trends_collector()
        rising_queries = trends_collector.fetch_rising_queries()
        
        if isinstance(rising_queries, pd.DataFrame):
            trending_df = rising_queries.head(20)
            if 'query' not in trending_df.columns:
                if 'title' in trending_df.columns:
                    trending_df = trending_df.rename(columns={'title': 'query'})
                elif len(trending_df.columns) > 0:
                    trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
        elif rising_queries and len(rising_queries) > 0:
            if isinstance(rising_queries[0], str):
                trending_df = pd.DataFrame([{'query': query} for query in rising_queries[:20]])
            elif isinstance(rising_queries[0], dict):
                trending_df = pd.DataFrame([{
                    'query': item.get('query', item.get('keyword', str(item))),
                    'value': item.get('value', item.get('interest', 0))
                } for item in rising_queries[:20]])
            else:
                trending_df = pd.DataFrame([{'query': str(query)} for query in rising_queries[:20]])
        else:
            trending_df = pd.DataFrame(columns=['query'])

        if trending_df is not None and not trending_df.empty:
            if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                trending_df.to_csv(f.name, index=False)
                temp_file = f.name
            
            try:
                print(f"ğŸ” è·å–åˆ° {len(trending_df)} ä¸ªå…³é”®è¯ï¼Œå¼€å§‹æœºä¼šåˆ†æ...")
                manager.new_word_detection_available = False
                keywords_result = manager.analyze_keywords(temp_file, args.output, enable_serp=False)
                
                print(f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ! åˆ†æäº† {keywords_result['total_keywords']} ä¸ªå…³é”®è¯")
                
                # ç¬¬äºŒæ­¥ï¼šå¤šå¹³å°éœ€æ±‚éªŒè¯
                print("\nğŸ“‹ ç¬¬äºŒæ­¥ï¼šå¤šå¹³å°éœ€æ±‚éªŒè¯")
                
                try:
                    # ç¡®ä¿èƒ½å¤Ÿå¯¼å…¥æ¨¡å—
                    analyzer_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'src', 'demand_mining', 'analyzers')
                    if analyzer_path not in sys.path:
                        sys.path.insert(0, analyzer_path)
                    
                    from src.demand_mining.analyzers.multi_platform_demand_analyzer import MultiPlatformDemandAnalyzer
                    
                    # åˆ›å»ºå¤šå¹³å°åˆ†æå™¨
                    demand_analyzer = MultiPlatformDemandAnalyzer()
                    
                    # æ‰§è¡Œå¤šå¹³å°éœ€æ±‚åˆ†æ
                    import asyncio
                    demand_results = asyncio.run(demand_analyzer.analyze_high_opportunity_keywords(
                        keywords_result.get('keywords', []),
                        min_opportunity_score=60.0,  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šå…³é”®è¯
                        max_keywords=3  # é™åˆ¶åˆ†ææ•°é‡é¿å…è¯·æ±‚è¿‡å¤š
                    ))
                    
                    # ä¿å­˜éœ€æ±‚éªŒè¯ç»“æœ
                    demand_output_file = demand_analyzer.save_results(demand_results)
                    
                    print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆ! éœ€æ±‚éªŒè¯åˆ†æå®Œæˆ")
                    print(f"ğŸ“Š åˆ†æäº† {demand_results.get('analyzed_keywords', 0)} ä¸ªé«˜æœºä¼šå…³é”®è¯")
                    
                    # æ˜¾ç¤ºéœ€æ±‚éªŒè¯æ‘˜è¦
                    summary = demand_results.get('summary', {})
                    if summary:
                        print(f"\nğŸ¯ éœ€æ±‚éªŒè¯æ‘˜è¦:")
                        print(f"   â€¢ æ€»æœç´¢ç»“æœ: {summary.get('total_search_results', 0)}")
                        print(f"   â€¢ å‘ç°ç—›ç‚¹: {summary.get('total_pain_points_found', 0)} ä¸ª")
                        print(f"   â€¢ åŠŸèƒ½éœ€æ±‚: {summary.get('total_feature_requests_found', 0)} ä¸ª")
                        
                        high_demand = summary.get('high_demand_keywords', [])
                        if high_demand:
                            print(f"   â€¢ é«˜éœ€æ±‚å…³é”®è¯: {', '.join(high_demand)}")
                        
                        top_opportunities = summary.get('top_opportunities', [])[:3]
                        if top_opportunities:
                            print(f"\nğŸ† Top 3 éªŒè¯ç»“æœ:")
                            for i, opp in enumerate(top_opportunities, 1):
                                print(f"   {i}. {opp['keyword']} - {opp['demand_level']} ({opp['pain_points_count']} ä¸ªç—›ç‚¹)")
                    
                    print(f"\nğŸ“ éœ€æ±‚éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {demand_output_file}")
                    
                except ImportError:
                    print("âš ï¸ å¤šå¹³å°éœ€æ±‚åˆ†æå™¨æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ç›¸å…³æ¨¡å—å·²å®‰è£…")
                except Exception as e:
                    print(f"âŒ éœ€æ±‚éªŒè¯å¤±è´¥: {e}")
                    if args.verbose:
                        import traceback
                        traceback.print_exc()
                
            finally:
                os.unlink(temp_file)
                
        else:
            print("âŒ æ— æ³•è·å–å…³é”®è¯è¿›è¡Œéœ€æ±‚éªŒè¯")
            
    except Exception as e:
        print(f"âŒ éœ€æ±‚éªŒè¯æµç¨‹å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()