#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘½ä»¤å¤„ç†å™¨æ¨¡å—
å¤„ç†å„ç§å‘½ä»¤è¡Œå‚æ•°å¯¹åº”çš„åŠŸèƒ½
"""

import os
import sys
import json
from pathlib import Path
import re
import time
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional, Set

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from src.cli_parser import print_quiet_summary, get_reports_dir
from src.utils.enhanced_features import (
    monitor_competitors, predict_keyword_trends, generate_seo_audit,
    batch_build_websites
)
from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery
from src.collectors.suggestion_sources import SuggestionCollector
from src.collectors.rss_hotspot_collector import RSSHotspotCollector


def handle_stats_display(manager, args):
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤º"""
    if args.stats:
        stats = manager.get_manager_stats()
        print("\nğŸ“Š ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯:")
        for manager_name, manager_stats in stats.items():
            if isinstance(manager_stats, dict):
                print(f"\n{manager_name}:")
                for key, value in manager_stats.items():
                    print(f"  {key}: {value}")
            else:
                print(f"{manager_name}: {manager_stats}")
        return True
    return False



STOPWORD_PATTERNS = [
    r'\b(?:news|weather|today|tomorrow|yesterday)\b',
    r'\b(?:login|signup|account|app|download|logo|price|stock|settlement|marketplace)\b',
    r'\b(?:live stream|live score|highlights|match result|match results|score)\b',
    r'\b(?:facebook|instagram|tiktok|youtube|twitter|google|bing|reddit)\b',
]


def _filter_trending_keywords(df):
    """å¯¹çƒ­é—¨å€™é€‰è¯åšåŸºç¡€å»å™ªè¿‡æ»¤"""
    if df is None or df.empty:
        return df

    if 'query' not in df.columns:
        return df

    filtered = df.copy()
    filtered['query'] = filtered['query'].astype(str)
    filtered['query_lower'] = filtered['query'].str.lower().str.strip()

    initial_count = len(filtered)

    # è¿‡æ»¤è¿‡çŸ­æˆ–éå­—æ¯æ•°å­—çš„è¯
    filtered = filtered[filtered['query_lower'].str.len() >= 3]
    filtered = filtered[~filtered['query_lower'].str.match(r'^[\d\W_]+$')]

    # å¸¸è§å“ç‰Œ/æ³›è¯å»å™ª
    for pattern in STOPWORD_PATTERNS:
        filtered = filtered[~filtered['query_lower'].str.contains(pattern, regex=True)]

    # å»é‡
    filtered = filtered.drop_duplicates(subset=['query_lower']).reset_index(drop=True)
    filtered = filtered.drop(columns=['query_lower'])

    removed = initial_count - len(filtered)
    if removed > 0:
        print(f'[Filter] å·²è¿‡æ»¤ {removed} æ¡ä½è´¨é‡çƒ­é—¨è¯')

    return filtered


def _print_new_word_summary(summary: Dict[str, Any]) -> None:
    if not summary or not isinstance(summary, dict):
        return

    total = summary.get('total_analyzed')
    detected = summary.get('new_words_detected')
    breakout = summary.get('breakout_keywords')
    rising = summary.get('rising_keywords')
    high_conf = summary.get('high_confidence_new_words')

    print("\nğŸ” æ–°è¯æ£€æµ‹æ‘˜è¦:")
    print(f"   â€¢ æ£€æµ‹æ€»æ•°: {total}")
    print(f"   â€¢ æ–°è¯æ•°é‡: {detected} / é«˜ç½®ä¿¡åº¦: {high_conf}")
    if breakout is not None or rising is not None:
        print(f"   â€¢ Breakout: {breakout} / Rising: {rising}")
    percentage = summary.get('new_word_percentage')
    if percentage is not None:
        print(f"   â€¢ æ–°è¯å æ¯”: {percentage}%")

    report_files = summary.get('report_files')
    if isinstance(report_files, dict) and report_files:
        print("   â€¢ å¯¼å‡ºæ–‡ä»¶:")
        for label, path in report_files.items():
            print(f"     - {label}: {path}")


def _print_top_new_words(result: Dict[str, Any], limit: int = 5) -> None:
    if not result or 'keywords' not in result:
        return

    candidates = []
    for item in result['keywords']:
        nwd = item.get('new_word_detection') if isinstance(item, dict) else None
        if not nwd or not nwd.get('is_new_word'):
            continue
        candidates.append({
            'keyword': item.get('keyword') or item.get('query'),
            'score': nwd.get('new_word_score', 0.0),
            'momentum': nwd.get('trend_momentum'),
            'delta': nwd.get('avg_7d_delta', 0.0),
            'grade': nwd.get('new_word_grade', 'D'),
            'confidence': nwd.get('confidence_level', 'low')
        })

    if not candidates:
        return

    candidates.sort(key=lambda x: (x['momentum'] == 'breakout', x['score']), reverse=True)
    print("\nğŸ”¥ Top æ–°è¯å€™é€‰:")
    for idx, item in enumerate(candidates[:limit], 1):
        print(
            f"   {idx}. {item['keyword']} | åˆ†æ•° {item['score']:.1f} | åŠ¨é‡ {item['momentum']} "
            f"| Î”7D {item['delta']:.1f} | ç­‰çº§ {item['grade']} | ç½®ä¿¡åº¦ {item['confidence']}"
        )


def _extract_records_from_df(df: Optional[pd.DataFrame], source_label: str, seed: str,
                             limit: int, seen_terms: Set[str], text_fields: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """ä»DataFrameä¸­æå–å€™é€‰å…³é”®è¯è®°å½•"""
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return []

    records: List[Dict[str, Any]] = []
    usable_fields = text_fields or ['query', 'topic_title', 'title']

    for _, row in df.head(max(limit, 0)).iterrows():
        text_value: Optional[str] = None
        for field in usable_fields:
            if field in row and pd.notna(row[field]):
                candidate_text = str(row[field]).strip()
                if candidate_text:
                    text_value = candidate_text
                    break
        if not text_value:
            continue

        normalized = text_value.lower()
        if normalized in seen_terms:
            continue

        record: Dict[str, Any] = {
            'query': text_value,
            'source': source_label,
            'seed': seed
        }

        if 'value' in row and pd.notna(row['value']):
            record['value'] = row['value']
        if 'formattedValue' in row and pd.notna(row['formattedValue']):
            record['growth'] = row['formattedValue']

        records.append(record)
        seen_terms.add(normalized)

    return records


def _collect_trends_related_candidates(trends_collector, seed_keywords: List[str],
                                       geo: str = '', timeframe: str = 'today 12-m',
                                       per_category_limit: int = 5) -> pd.DataFrame:
    """åŸºäºç§å­å…³é”®è¯è¡¥é›† Google Trends ç›¸å…³æŸ¥è¯¢/ä¸»é¢˜å»ºè®®"""
    if not trends_collector or not seed_keywords:
        return pd.DataFrame(columns=['query', 'source'])

    unique_seeds: List[str] = []
    seen_seed: Set[str] = set()
    for keyword in seed_keywords:
        if not keyword:
            continue
        normalized = str(keyword).strip().lower()
        if not normalized or normalized in seen_seed:
            continue
        unique_seeds.append(str(keyword).strip())
        seen_seed.add(normalized)
        if len(unique_seeds) >= 8:
            break

    if not unique_seeds:
        return pd.DataFrame(columns=['query', 'source'])

    seen_terms: Set[str] = set(seen_seed)
    rows: List[Dict[str, Any]] = []

    for seed in unique_seeds:
        if hasattr(trends_collector, 'is_in_cooldown') and trends_collector.is_in_cooldown():
            print("âš ï¸ Google Trends å¤„äºå†·å´æœŸï¼Œç»ˆæ­¢ç›¸å…³æŸ¥è¯¢é‡‡é›†")
            break
        try:
            related_queries = trends_collector.get_related_queries(seed, geo=geo, timeframe=timeframe)
        except Exception as exc:
            print(f"âš ï¸ è·å– {seed} ç›¸å…³æŸ¥è¯¢å¤±è´¥: {exc}")
            related_queries = {}

        seed_map = None
        if isinstance(related_queries, dict):
            seed_map = related_queries.get(seed)
            if seed_map is None and len(related_queries) == 1:
                seed_map = next(iter(related_queries.values()))

        if isinstance(seed_map, dict):
            rows.extend(_extract_records_from_df(
                seed_map.get('rising'),
                'Google Trends Related Rising',
                seed,
                per_category_limit,
                seen_terms
            ))
            rows.extend(_extract_records_from_df(
                seed_map.get('top'),
                'Google Trends Related Top',
                seed,
                per_category_limit,
                seen_terms
            ))
        elif isinstance(related_queries, pd.DataFrame):
            rows.extend(_extract_records_from_df(
                related_queries,
                'Google Trends Related Queries',
                seed,
                per_category_limit,
                seen_terms
            ))

        try:
            related_topics = trends_collector.get_related_topics(seed, geo=geo, timeframe=timeframe)
        except Exception as exc:
            print(f"âš ï¸ è·å– {seed} ç›¸å…³ä¸»é¢˜å¤±è´¥: {exc}")
            related_topics = {}

        if hasattr(trends_collector, 'is_in_cooldown') and trends_collector.is_in_cooldown():
            print("âš ï¸ Google Trends å†·å´ä¸­ï¼Œåœæ­¢ç»§ç»­é‡‡é›†ç›¸å…³ä¸»é¢˜")
            break

        topic_map = None
        if isinstance(related_topics, dict):
            topic_map = related_topics.get(seed)
            if topic_map is None and len(related_topics) == 1:
                topic_map = next(iter(related_topics.values()))

        if isinstance(topic_map, dict):
            rows.extend(_extract_records_from_df(
                topic_map.get('rising'),
                'Google Trends Related Topics Rising',
                seed,
                per_category_limit,
                seen_terms,
                text_fields=['topic_title', 'title', 'query']
            ))
            rows.extend(_extract_records_from_df(
                topic_map.get('top'),
                'Google Trends Related Topics Top',
                seed,
                per_category_limit,
                seen_terms,
                text_fields=['topic_title', 'title', 'query']
            ))
        elif isinstance(related_topics, pd.DataFrame):
            rows.extend(_extract_records_from_df(
                related_topics,
                'Google Trends Related Topics',
                seed,
                per_category_limit,
                seen_terms,
                text_fields=['topic_title', 'title', 'query']
            ))

        try:
            suggestions = trends_collector.get_suggestions(seed) or []
        except Exception as exc:
            print(f"âš ï¸ è·å– {seed} Suggestion å¤±è´¥: {exc}")
            suggestions = []

        for suggestion in suggestions[:max(per_category_limit, 0)]:
            title = suggestion.get('title') if isinstance(suggestion, dict) else None
            if not title:
                continue
            normalized = str(title).strip().lower()
            if not normalized or normalized in seen_terms:
                continue
            rows.append({
                'query': str(title).strip(),
                'source': 'Google Trends Suggestion',
                'seed': seed
            })
            seen_terms.add(normalized)

        time.sleep(0.5)

    if not rows:
        return pd.DataFrame(columns=['query', 'source'])

    return pd.DataFrame(rows)


def _generate_keyword_combinations(seed_keywords: List[str], manager, long_tail_limit: int = 4,
                                   prefix_limit: int = 3, head_limit: int = 3) -> pd.DataFrame:
    """åŸºäºç§å­è¯ç”Ÿæˆç»„åˆå…³é”®è¯"""
    if not seed_keywords or manager is None:
        return pd.DataFrame(columns=['query', 'source'])

    unique_seeds: List[str] = []
    seen_seed: Set[str] = set()
    for keyword in seed_keywords:
        if not keyword:
            continue
        normalized = str(keyword).strip().lower()
        if not normalized or normalized in seen_seed:
            continue
        unique_seeds.append(str(keyword).strip())
        seen_seed.add(normalized)
        if len(unique_seeds) >= 8:
            break

    if not unique_seeds:
        return pd.DataFrame(columns=['query', 'source'])

    long_tail_terms = sorted(getattr(manager, 'long_tail_modifiers', []), key=lambda x: (len(x), x))[:max(long_tail_limit, 0)]
    question_prefixes = list(getattr(manager, 'question_prefixes', ()))[:max(prefix_limit, 0)]
    generic_heads = sorted(getattr(manager, 'generic_head_terms', []), key=lambda x: (len(x), x))[:max(head_limit, 0)]

    rows: List[Dict[str, Any]] = []
    seen_terms: Set[str] = set(seen_seed)

    def _append_candidate(text: str, source_label: str, seed: str) -> None:
        candidate = text.strip()
        if not candidate:
            return
        normalized = candidate.lower()
        if normalized in seen_terms:
            return
        rows.append({'query': candidate, 'source': source_label, 'seed': seed})
        seen_terms.add(normalized)

    for seed in unique_seeds:
        for modifier in long_tail_terms:
            if modifier:
                _append_candidate(f"{seed} {modifier}".strip(), 'Generated Long Tail', seed)
        for prefix in question_prefixes:
            if prefix:
                _append_candidate(f"{prefix} {seed}".strip(), 'Generated Question Prefix', seed)
        for head in generic_heads:
            if head:
                _append_candidate(f"{seed} {head}".strip(), 'Generated Generic Head', seed)

    if not rows:
        return pd.DataFrame(columns=['query', 'source'])

    return pd.DataFrame(rows)

def handle_input_file_analysis(manager, args):
    """å¤„ç†å…³é”®è¯æ–‡ä»¶åˆ†æ"""
    if not args.input:
        return False
        
    # åˆ†æå…³é”®è¯æ–‡ä»¶
    if not args.quiet:
        print("ğŸš€ å¼€å§‹åˆ†æå…³é”®è¯æ–‡ä»¶...")
        if args.serp:
            print("ğŸ” å·²å¯ç”¨SERPåˆ†æåŠŸèƒ½")
    
    result = manager.analyze_keywords(args.input, args.output, enable_serp=args.serp)
    
    # æ˜¾ç¤ºç»“æœ
    if args.quiet:
        print_quiet_summary(result)
    else:
        print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
        print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
        print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")

        # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
        _print_new_word_summary(result.get('new_word_summary'))
        _print_top_new_words(result)

        # æ˜¾ç¤ºSERPåˆ†ææ‘˜è¦
        if 'serp_summary' in result and result['serp_summary'].get('serp_analysis_enabled', False):
            serp_summary = result['serp_summary']
            print(f"ğŸ” SERPåˆ†æ: åˆ†æäº† {serp_summary['total_analyzed']} ä¸ªå…³é”®è¯")
            print(f"   é«˜ç½®ä¿¡åº¦SERP: {serp_summary['high_confidence_serp']} ä¸ª")
            print(f"   å•†ä¸šæ„å›¾å…³é”®è¯: {serp_summary['commercial_intent_keywords']} ä¸ª")

        # æ˜¾ç¤ºTop 5å…³é”®è¯
        top_keywords = result['market_insights']['top_opportunities'][:5]
        if top_keywords:
            print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
            for i, kw in enumerate(top_keywords, 1):
                intent_desc = kw['intent']['intent_description']
                score = kw['opportunity_score']
                new_word_info = ""
                if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                    new_word_grade = kw['new_word_detection']['new_word_grade']
                    new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
    
    return True


def handle_keywords_analysis(manager, args):
    """å¤„ç†å•ä¸ªå…³é”®è¯åˆ†æ"""
    if not args.keywords:
        return False
        
    # åˆ†æå•ä¸ªå…³é”®è¯
    if not args.quiet:
        print("ğŸš€ å¼€å§‹åˆ†æè¾“å…¥çš„å…³é”®è¯...")
    
    keywords = [kw for kw in args.keywords if kw]
    result = manager.analyze_keywords(keywords, args.output, enable_serp=args.serp)

    # æ˜¾ç¤ºç»“æœ
    if args.quiet:
        print_quiet_summary(result)
    else:
        print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {len(args.keywords)} ä¸ªå…³é”®è¯")

        # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„ç»“æœ
        print("\nğŸ“‹ å…³é”®è¯åˆ†æç»“æœ:")
        for kw_result in result['keywords']:
            keyword = kw_result['keyword']
            score = kw_result['opportunity_score']
            intent = kw_result['intent']['intent_description']
            print(f"   â€¢ {keyword}: æœºä¼šåˆ†æ•° {score}, æ„å›¾: {intent}")
    return True


def handle_discover_analysis(manager, args):
    """å¤„ç†å¤šå¹³å°å…³é”®è¯å‘ç°"""
    if not args.discover:
        return False
        
    seed_profile = getattr(args, 'seed_profile', None)
    seed_limit = getattr(args, 'seed_limit', None)
    if isinstance(seed_limit, int) and seed_limit <= 0:
        seed_limit = None
    min_seed_terms = getattr(args, 'min_seed_terms', None)
    if isinstance(min_seed_terms, int) and min_seed_terms <= 0:
        min_seed_terms = None

    # å¤šå¹³å°å…³é”®è¯å‘ç°
    raw_terms = [] if args.discover == ['default'] else args.discover
    
    try:
        # åˆ›å»ºå‘ç°å·¥å…·
        discoverer = MultiPlatformKeywordDiscovery()

        search_terms = discoverer.prepare_search_terms(
            seeds=raw_terms,
            profile=seed_profile,
            limit=seed_limit,
            min_terms=min_seed_terms
        )

        if not search_terms:
            print("âš ï¸ ç¼ºå°‘æœ‰æ•ˆçš„ç§å­å…³é”®è¯ï¼Œæ— æ³•æ‰§è¡Œå¤šå¹³å°å‘ç°")
            return True

        if not args.quiet:
            print("ğŸ” å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
            print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
        
        # æ‰§è¡Œå‘ç°
        df = discoverer.discover_all_platforms(search_terms)

        if not df.empty:
            # åˆ†æè¶‹åŠ¿
            analysis = discoverer.analyze_keyword_trends(df)
            
            # ä¿å­˜ç»“æœ
            output_dir = os.path.join(args.output, 'multi_platform_discovery')
            csv_path, json_path = discoverer.save_results(df, analysis, output_dir)
            
            if args.quiet:
                # é™é»˜æ¨¡å¼æ˜¾ç¤º
                print(f"\nğŸ¯ å¤šå¹³å°å…³é”®è¯å‘ç°ç»“æœ:")
                print(f"   â€¢ å‘ç°å…³é”®è¯: {analysis['total_keywords']} ä¸ª")
                print(f"   â€¢ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                
                # æ˜¾ç¤ºTop 3å…³é”®è¯
                top_keywords = analysis['top_keywords_by_score'][:3]
                if top_keywords:
                    print("\nğŸ† Top 3 çƒ­é—¨å…³é”®è¯:")
                    for i, kw in enumerate(top_keywords, 1):
                        print(f"   {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            else:
                # è¯¦ç»†æ¨¡å¼æ˜¾ç¤º
                print(f"\nğŸ‰ å¤šå¹³å°å…³é”®è¯å‘ç°å®Œæˆ!")
                print(f"ğŸ“Š å‘ç° {analysis['total_keywords']} ä¸ªå…³é”®è¯")
                print(f"ğŸŒ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                
                print(f"\nğŸ† çƒ­é—¨å…³é”®è¯:")
                for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                    print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
            
            print(f"\nğŸ“ ç»“æœå·²ä¿å­˜:")
            print(f"  CSV: {csv_path}")
            print(f"  JSON: {json_path}")
            
            # è¯¢é—®æ˜¯å¦è¦ç«‹å³åˆ†æå‘ç°çš„å…³é”®è¯
            if not args.quiet:
                user_input = input("\nğŸ¤” æ˜¯å¦è¦ç«‹å³åˆ†æè¿™äº›å…³é”®è¯çš„æ„å›¾å’Œå¸‚åœºæœºä¼š? (y/n): ")
                if user_input.lower() in ['y', 'yes', 'æ˜¯']:
                    print("ğŸ”„ å¼€å§‹åˆ†æå‘ç°çš„å…³é”®è¯...")
                    result = manager.analyze_keywords(csv_path, args.output)
                    print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                    print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
        else:
            print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
            
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
    except Exception as e:
        print(f"âŒ å¤šå¹³å°å…³é”®è¯å‘ç°å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    
    return True


def handle_enhanced_features(manager, args):
    """å¤„ç†å¢å¼ºåŠŸèƒ½ (ç«å“ç›‘æ§ã€è¶‹åŠ¿é¢„æµ‹ã€SEOå®¡è®¡ã€æ‰¹é‡å»ºç«™)"""
    # ç«å“ç›‘æ§
    if args.monitor_competitors:
        sites = args.sites or ['canva.com', 'midjourney.com', 'openai.com']
        if not args.quiet:
            print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(sites)} ä¸ªç«å“ç½‘ç«™...")
        
        try:
            result = monitor_competitors(sites, args.output)
        except Exception as exc:
            print(f"âŒ ç«å“ç›‘æ§å¤±è´¥: {exc}")
            return True

        print(f"âœ… ç«å“ç›‘æ§å®Œæˆ: åˆ†æäº† {len(result['competitors'])} ä¸ªç«å“")

        if not args.quiet:
            print("\nğŸ“Š ç›‘æ§ç»“æœæ‘˜è¦:")
            for comp in result['competitors'][:3]:
                print(f"  â€¢ {comp['site']}: {comp['new_keywords_count']} ä¸ªæ–°å…³é”®è¯")
        return True
    
    # è¶‹åŠ¿é¢„æµ‹
    if args.predict_trends:
        if not args.quiet:
            print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {args.timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
        
        # è·å–å…³é”®è¯åˆ—è¡¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        keywords_for_prediction = getattr(args, 'keywords', None)
        
        try:
            result = predict_keyword_trends(
                timeframe=args.timeframe, 
                output_dir=args.output,
                keywords=keywords_for_prediction,
                use_real_data=True
            )
        except Exception as exc:
            print(f"âŒ è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {exc}")
            return True
        
        # æ˜¾ç¤ºç»“æœ
        data_source = result.get('data_source', 'unknown')
        if data_source == 'google_trends_real_data':
            print(f"âœ… çœŸå®æ•°æ®è¶‹åŠ¿é¢„æµ‹å®Œæˆ: {len(result['rising_keywords'])} ä¸Šå‡, {len(result['stable_keywords'])} ç¨³å®š, {len(result['declining_keywords'])} ä¸‹é™")
        else:
            print(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆ: é¢„æµ‹äº† {len(result['rising_keywords'])} ä¸ªä¸Šå‡å…³é”®è¯ (æ•°æ®æº: {data_source})")
        
        if not args.quiet:
            print("\nğŸ“ˆ è¶‹åŠ¿é¢„æµ‹æ‘˜è¦:")
            
            # æ˜¾ç¤ºä¸Šå‡å…³é”®è¯
            if result['rising_keywords']:
                print("\nğŸ”¥ ä¸Šå‡è¶‹åŠ¿å…³é”®è¯:")
                for kw in result['rising_keywords'][:3]:
                    growth = kw.get('predicted_growth', kw.get('growth_rate', 'N/A'))
                    confidence = kw.get('confidence', 0)
                    print(f"  ğŸ“ˆ {kw['keyword']}: {growth} (ç½®ä¿¡åº¦: {confidence:.0%})")
            
            # æ˜¾ç¤ºç¨³å®šå…³é”®è¯
            if result['stable_keywords']:
                print("\nğŸ“Š ç¨³å®šè¶‹åŠ¿å…³é”®è¯:")
                for kw in result['stable_keywords'][:2]:
                    growth = kw.get('predicted_change', kw.get('growth_rate', 'N/A'))
                    confidence = kw.get('confidence', 0)
                    print(f"  ğŸ“Š {kw['keyword']}: {growth} (ç½®ä¿¡åº¦: {confidence:.0%})")
            
            # æ˜¾ç¤ºä¸‹é™å…³é”®è¯
            if result['declining_keywords']:
                print("\nğŸ“‰ ä¸‹é™è¶‹åŠ¿å…³é”®è¯:")
                for kw in result['declining_keywords'][:2]:
                    decline = kw.get('predicted_decline', kw.get('growth_rate', 'N/A'))
                    confidence = kw.get('confidence', 0)
                    print(f"  ğŸ“‰ {kw['keyword']}: {decline} (ç½®ä¿¡åº¦: {confidence:.0%})")
        
        return True
    
    # SEOå®¡è®¡
    if args.seo_audit:
        if not args.domain:
            print("âŒ è¯·æŒ‡å®šè¦å®¡è®¡çš„åŸŸå (--domain)")
            return True
        
        if not args.quiet:
            print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {args.domain}")
        
        try:
            result = generate_seo_audit(args.domain, args.keywords)
        except Exception as exc:
            print(f"âŒ SEOå®¡è®¡å¤±è´¥: {exc}")
            return True

        print(f"âœ… SEOå®¡è®¡å®Œæˆ: å‘ç° {len(result['keyword_opportunities'])} ä¸ªå…³é”®è¯æœºä¼š")

        if not args.quiet:
            print("\nğŸ¯ SEOä¼˜åŒ–å»ºè®®:")
            for gap in result['content_gaps'][:3]:
                print(f"  â€¢ {gap}")
        return True
    
    # æ‰¹é‡å»ºç«™
    if args.build_websites:
        if not args.quiet:
            print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {args.top_keywords} ä¸ªç½‘ç«™...")
        
        try:
            result = batch_build_websites(args.top_keywords, args.output)
        except Exception as exc:
            print(f"âŒ æ‰¹é‡å»ºç«™å¤±è´¥: {exc}")
            return True

        print(f"âœ… æ‰¹é‡å»ºç«™å®Œæˆ: æˆåŠŸæ„å»º {result['successful_builds']} ä¸ªç½‘ç«™")

        if not args.quiet:
            print("\nğŸŒ æ„å»ºçš„ç½‘ç«™:")
            for site in result['websites'][:3]:
                print(f"  â€¢ {site['keyword']}: {site['domain_suggestion']}")
        return True
    
    return False


def handle_hot_keywords(manager, args):
    """å¤„ç†çƒ­é—¨å…³é”®è¯æœç´¢å’Œéœ€æ±‚æŒ–æ˜"""
    if not args.hotkeywords:
        return False
    
    # æœç´¢çƒ­é—¨å…³é”®è¯ï¼šä½¿ç”¨ fetch_rising_queries è·å–å…³é”®è¯å¹¶è¿›è¡Œéœ€æ±‚æŒ–æ˜
    if not args.quiet:
        print("ğŸ”¥ å¼€å§‹æœç´¢çƒ­é—¨å…³é”®è¯å¹¶è¿›è¡Œéœ€æ±‚æŒ–æ˜...")
    
    try:
        # ä½¿ç”¨å•ä¾‹è·å– TrendsCollector
        from src.collectors.trends_singleton import get_trends_collector
        
        # è·å– TrendsCollector å•ä¾‹å®ä¾‹
        trends_collector = get_trends_collector()
        
        # ä½¿ç”¨ fetch_rising_queries è·å–çƒ­é—¨å…³é”®è¯
        if not args.quiet:
            print("ğŸ” æ­£åœ¨è·å– Rising Queries...")
        
        rising_queries = trends_collector.fetch_rising_queries()
        
        # å°† rising queries è½¬æ¢ä¸ºDataFrameæ ¼å¼
        if isinstance(rising_queries, pd.DataFrame):
            # å¦‚æœå·²ç»æ˜¯DataFrameï¼Œç›´æ¥ä½¿ç”¨
            trending_df = rising_queries.head(20)  # é™åˆ¶å‰20ä¸ª
            # ç¡®ä¿æœ‰queryåˆ—
            if 'query' not in trending_df.columns:
                if 'title' in trending_df.columns:
                    trending_df = trending_df.rename(columns={'title': 'query'})
                elif len(trending_df.columns) > 0:
                    trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
        elif rising_queries and len(rising_queries) > 0:
            # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            if isinstance(rising_queries[0], str):
                trending_df = pd.DataFrame([
                    {'query': query}
                    for query in rising_queries[:20]  # é™åˆ¶å‰20ä¸ª
                ])
            # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸åˆ—è¡¨
            elif isinstance(rising_queries[0], dict):
                trending_df = pd.DataFrame([
                    {
                        'query': item.get('query', item.get('keyword', str(item))),
                        'value': item.get('value', item.get('interest', 0))
                    }
                    for item in rising_queries[:20]  # é™åˆ¶å‰20ä¸ª
                ])
            else:
                # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                trending_df = pd.DataFrame([
                    {'query': str(query)}
                    for query in rising_queries[:20]
                ])
        else:
            trending_df = pd.DataFrame(columns=['query'])

        if trending_df is not None and not trending_df.empty:
            all_trending_keywords = [trending_df]

            seed_pool = trending_df['query'].dropna().astype(str).tolist()

            # RSS çƒ­ç‚¹
            try:
                rss_df = RSSHotspotCollector().collect(max_items=20)
                if not rss_df.empty:
                    all_trending_keywords.append(rss_df)
                    if not args.quiet:
                        print(f"âœ… RSS çƒ­ç‚¹: è·å–åˆ° {len(rss_df)} ä¸ªçƒ­ç‚¹è¯")
            except Exception as exc:
                if not args.quiet:
                    print(f"âš ï¸ RSS çƒ­ç‚¹æŠ“å–å¤±è´¥: {exc}")

            # Google Trends ç›¸å…³è¯
            try:
                related_df = _collect_trends_related_candidates(trends_collector, seed_pool)
                if not related_df.empty:
                    all_trending_keywords.append(related_df)
                    if not args.quiet:
                        print(f"âœ… Google Trends å…³è”æ‰©å±•: æ–°å¢ {len(related_df)} ä¸ªå€™é€‰å…³é”®è¯")
            except Exception as exc:
                if not args.quiet:
                    print(f"âš ï¸ Google Trends å…³è”æ‰©å±•å¤±è´¥: {exc}")

            # ç»„åˆç”Ÿæˆ
            try:
                combo_df = _generate_keyword_combinations(seed_pool, manager)
                if not combo_df.empty:
                    all_trending_keywords.append(combo_df)
                    if not args.quiet:
                        print(f"âœ… ç»„åˆç”Ÿæˆ: æ–°å¢ {len(combo_df)} ä¸ªå€™é€‰å…³é”®è¯")
            except Exception as exc:
                if not args.quiet:
                    print(f"âš ï¸ ç»„åˆç”Ÿæˆå¤±è´¥: {exc}")

            # Suggestion æ¥æº
            suggestion_records: List = []
            try:
                suggestion_collector = SuggestionCollector()
                suggestion_records = suggestion_collector.collect(seed_pool, per_seed_limit=4)
            except Exception as exc:
                if not args.quiet:
                    print(f"âš ï¸ Suggestion æ”¶é›†å¤±è´¥: {exc}")

            if suggestion_records:
                suggestion_df = pd.DataFrame([
                    {
                        'query': item.term,
                        'source': f"Suggestions/{item.source}",
                        'seed': item.seed
                    }
                    for item in suggestion_records if item.term
                ])
                if not suggestion_df.empty:
                    all_trending_keywords.append(suggestion_df)
                    if not args.quiet:
                        print(f"âœ… çƒ­é—¨è”æƒ³: æ–°å¢ {len(suggestion_df)} ä¸ªå€™é€‰å…³é”®è¯")
                        stats = suggestion_collector.get_last_stats()
                        print(
                            f"   â€¢ è”æƒ³æ‘˜è¦: ç§å­ {stats['seeds_processed']}/{stats['seeds_total']} | è¯·æ±‚ {stats['requests_sent']} | å»ºè®® {stats['suggestions_collected']}"
                        )

            trending_df = pd.concat(all_trending_keywords, ignore_index=True)
            trending_df = trending_df.drop_duplicates(subset=['query'], keep='first')
            trending_df = trending_df.head(50)
            filtered = _filter_trending_keywords(trending_df)
            if not filtered.empty:
                trending_df = filtered
            else:
                print("[Filter] çƒ­é—¨å…³é”®è¯è¿‡æ»¤åä¸ºç©ºï¼Œç»§ç»­ä½¿ç”¨æœªè¿‡æ»¤ç»“æœ")

            # ä¿å­˜çƒ­é—¨å…³é”®è¯åˆ°ä¸´æ—¶æ–‡ä»¶
            
            # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
            if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                # å¦‚æœæ²¡æœ‰queryåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºå…³é”®è¯
                trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
            
            # æ¸…æ´—å¹¶ç›´æ¥åœ¨å†…å­˜ä¸­è§¦å‘éœ€æ±‚æŒ–æ˜
            try:
                from src.pipeline.cleaning.cleaner import clean_terms, CleaningConfig
                if 'query' in trending_df.columns:
                    cleaned = clean_terms(
                        trending_df['query'].astype(str).tolist(),
                        CleaningConfig()
                    )
                    trending_df = pd.DataFrame({'query': cleaned})
            except Exception:
                pass

            if not args.quiet:
                print(f"ğŸ” è·å–åˆ° {len(trending_df)} ä¸ª Rising Queriesï¼Œå¼€å§‹éœ€æ±‚æŒ–æ˜åˆ†æ...")

            # æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æï¼Œç¦ç”¨æ–°è¯æ£€æµ‹é¿å…429é”™è¯¯
            original_new_word_flag = getattr(manager, 'new_word_detection_available', True)
            manager.new_word_detection_available = False
            try:
                result = manager.analyze_keywords(trending_df, args.output, enable_serp=False)

                # æ˜¾ç¤ºç»“æœ
                if args.quiet:
                    print_quiet_summary(result)
                else:
                    print(f"\nğŸ‰ éœ€æ±‚æŒ–æ˜åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ª Rising Queries")
                    print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                    print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")

                    # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
                    _print_new_word_summary(result.get('new_word_summary'))
                    _print_top_new_words(result)

                    # æ˜¾ç¤ºTop 5æœºä¼šå…³é”®è¯
                    top_keywords = result['market_insights']['top_opportunities'][:5]
                    if top_keywords:
                        print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                        for i, kw in enumerate(top_keywords, 1):
                            intent_desc = kw['intent']['intent_description']
                            score = kw['opportunity_score']
                            new_word_info = ""
                            if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                                new_word_grade = kw['new_word_detection']['new_word_grade']
                                new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                            print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
                    
                    # æ˜¾ç¤ºåŸå§‹Rising Queriesä¿¡æ¯
                    print(f"\nğŸ”¥ åŸå§‹ Rising Queries æ•°æ®:")
                    print(f"   â€¢ æ•°æ®æ¥æº: Google Trends Rising Queries")
                    if 'value' in trending_df.columns:
                        print(f"   â€¢ å¹³å‡çƒ­åº¦: {trending_df['value'].mean():.1f}")
                    
                    # ä¿å­˜åŸå§‹Rising Queries
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    trending_output_file = os.path.join(args.output, f"rising_queries_raw_{timestamp}.csv")
                    os.makedirs(args.output, exist_ok=True)
                    trending_df.to_csv(trending_output_file, index=False, encoding='utf-8')
                    print(f"ğŸ“ åŸå§‹ Rising Queries å·²ä¿å­˜åˆ°: {trending_output_file}")
            finally:
                manager.new_word_detection_available = original_new_word_flag

        else:
            # å½“æ— æ³•è·å–Rising Queriesæ—¶ï¼Œç›´æ¥æŠ¥å‘Šå¤±è´¥
            print("âŒ æ— æ³•è·å– Rising Queriesï¼Œå¯èƒ½çš„åŸå› :")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ç¨åé‡è¯•")
            print("   3. æˆ–ä½¿ç”¨ --input å‚æ•°æŒ‡å®šå…³é”®è¯æ–‡ä»¶è¿›è¡Œåˆ†æ")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ è·å– Rising Queries æˆ–éœ€æ±‚æŒ–æ˜æ—¶å‡ºé”™: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    
    return True


def handle_all_workflow(manager, args):
    """å¤„ç†å®Œæ•´çš„å…³é”®è¯åˆ†æå·¥ä½œæµç¨‹"""
    if not args.all:
        return False
    
    if not args.quiet:
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„å…³é”®è¯åˆ†æå·¥ä½œæµç¨‹...")
        print("   ç¬¬ä¸€æ­¥: æœç´¢çƒ­é—¨å…³é”®è¯ (Google Trends + TrendingKeywords.net)")
        print("   ç¬¬äºŒæ­¥: åŸºäºçƒ­é—¨å…³é”®è¯è¿›è¡Œå¤šå¹³å°å‘ç°")
    
    max_seed_keywords = max(1, getattr(args, 'max_seed_keywords', 10))
    max_discovered_keywords = max(10, getattr(args, 'max_discovered_keywords', 150))
    hot_result = None
    trends_collector = None

    try:
        # ç¬¬ä¸€æ­¥ï¼šè·å–çƒ­é—¨å…³é”®è¯ - æ•´åˆå¤šä¸ªæ•°æ®æº
        all_trending_keywords = []
        
        # 1.1 è·å– Google Trends Rising Queries
        if not args.quiet:
            print("ğŸ” æ­£åœ¨è·å– Google Trends Rising Queries...")
        
        try:
            from src.collectors.trends_singleton import get_trends_collector
            trends_collector = get_trends_collector()
            rising_queries = trends_collector.fetch_rising_queries()
            
            # å¤„ç† Google Trends æ•°æ®
            if isinstance(rising_queries, pd.DataFrame):
                trending_df = rising_queries.head(15)  # å‡å°‘åˆ°15ä¸ªä¸ºTrendingKeywordsç•™ç©ºé—´
                if 'query' not in trending_df.columns:
                    if 'title' in trending_df.columns:
                        trending_df = trending_df.rename(columns={'title': 'query'})
                    elif len(trending_df.columns) > 0:
                        trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
            elif rising_queries and len(rising_queries) > 0:
                if isinstance(rising_queries[0], str):
                    trending_df = pd.DataFrame([
                        {'query': query, 'source': 'Google Trends'}
                        for query in rising_queries[:15]
                    ])
                elif isinstance(rising_queries[0], dict):
                    trending_df = pd.DataFrame([
                        {
                            'query': item.get('query', item.get('keyword', str(item))),
                            'value': item.get('value', item.get('interest', 0)),
                            'source': 'Google Trends'
                        }
                        for item in rising_queries[:15]
                    ])
                else:
                    trending_df = pd.DataFrame([
                        {'query': str(query), 'source': 'Google Trends'}
                        for query in rising_queries[:15]
                    ])
            else:
                trending_df = pd.DataFrame(columns=['query', 'source'])
            
            if not trending_df.empty:
                all_trending_keywords.append(trending_df)
                if not args.quiet:
                    print(f"âœ… Google Trends: è·å–åˆ° {len(trending_df)} ä¸ªå…³é”®è¯")
            
        except Exception as e:
            if not args.quiet:
                print(f"âš ï¸ Google Trends è·å–å¤±è´¥: {e}")
        
        # 1.2 è·å– TrendingKeywords.net æ•°æ®
        if not args.quiet:
            print("ğŸ” æ­£åœ¨è·å– TrendingKeywords.net æ•°æ®...")

        try:
            from src.collectors.trending_keywords_collector import TrendingKeywordsCollector
            
            tk_collector = TrendingKeywordsCollector()
            tk_df = tk_collector.get_trending_keywords_for_analysis(max_keywords=15)
            
            if not tk_df.empty:
                # æ·»åŠ æ•°æ®æºæ ‡è¯†
                tk_df['source'] = 'TrendingKeywords.net'
                all_trending_keywords.append(tk_df)
                if not args.quiet:
                    print(f"âœ… TrendingKeywords.net: è·å–åˆ° {len(tk_df)} ä¸ªå…³é”®è¯")
            
        except Exception as e:
            if not args.quiet:
                print(f"âš ï¸ TrendingKeywords.net è·å–å¤±è´¥: {e}")

        # 1.3 è·å– RSS çƒ­ç‚¹æ•°æ®
        if not args.quiet:
            print("ğŸ” æ­£åœ¨æŠ“å– RSS çƒ­ç‚¹...")

        try:
            rss_collector = RSSHotspotCollector()
            rss_df = rss_collector.collect(max_items=20)
            if not rss_df.empty:
                all_trending_keywords.append(rss_df)
                if not args.quiet:
                    print(f"âœ… RSS çƒ­ç‚¹: è·å–åˆ° {len(rss_df)} ä¸ªçƒ­ç‚¹è¯")
        except Exception as e:
            if not args.quiet:
                print(f"âš ï¸ RSS çƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")

        # åŸºäºç°æœ‰æ•°æ®æºæ‰©å±•ç›¸å…³è¯ä¸ç»„åˆè¯
        seed_pool: List[str] = []
        for df_candidate in all_trending_keywords:
            if isinstance(df_candidate, pd.DataFrame) and 'query' in df_candidate.columns:
                seed_pool.extend(df_candidate['query'].dropna().astype(str).tolist())

        if seed_pool and trends_collector:
            related_candidates = _collect_trends_related_candidates(trends_collector, seed_pool)
            if not related_candidates.empty:
                all_trending_keywords.append(related_candidates)
                if not args.quiet:
                    print(f"âœ… Google Trends å…³è”æ‰©å±•: æ–°å¢ {len(related_candidates)} ä¸ªå€™é€‰å…³é”®è¯")

        suggestion_collector: Optional[SuggestionCollector] = None
        if seed_pool:
            combo_candidates = _generate_keyword_combinations(seed_pool, manager)
            if not combo_candidates.empty:
                all_trending_keywords.append(combo_candidates)
                if not args.quiet:
                    print(f"âœ… ç»„åˆç”Ÿæˆ: æ–°å¢ {len(combo_candidates)} ä¸ªå€™é€‰å…³é”®è¯")

            try:
                suggestion_collector = SuggestionCollector()
                suggestion_records = suggestion_collector.collect(seed_pool, per_seed_limit=4)
            except Exception as exc:
                if not args.quiet:
                    print(f"âš ï¸ Suggestion æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {exc}")
                suggestion_records = []

            if suggestion_records:
                suggestion_df = pd.DataFrame([
                    {
                        'query': item.term,
                        'source': f"Suggestions/{item.source}",
                        'seed': item.seed
                    }
                    for item in suggestion_records
                    if item.term
                ])
                if not suggestion_df.empty:
                    all_trending_keywords.append(suggestion_df)
                    if not args.quiet:
                        print(f"âœ… çƒ­é—¨è”æƒ³: æ–°å¢ {len(suggestion_df)} ä¸ªå€™é€‰å…³é”®è¯")
                        if suggestion_collector is not None:
                            stats = suggestion_collector.get_last_stats()
                            print(
                                f"   â€¢ è”æƒ³æ‘˜è¦: ç§å­ {stats['seeds_processed']}/{stats['seeds_total']} | è¯·æ±‚ {stats['requests_sent']} | å»ºè®® {stats['suggestions_collected']}"
                            )

        # åˆå¹¶æ‰€æœ‰æ•°æ®æº
        if all_trending_keywords:
            trending_df = pd.concat(all_trending_keywords, ignore_index=True)
            
            # å»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„
            trending_df = trending_df.drop_duplicates(subset=['query'], keep='first')
            
            # é™åˆ¶æ€»æ•°é‡
            trending_df = trending_df.head(60)
            filtered_trending = _filter_trending_keywords(trending_df)
            if not filtered_trending.empty:
                trending_df = filtered_trending
            else:
                print("[Filter] çƒ­é—¨å…³é”®è¯è¿‡æ»¤åä¸ºç©ºï¼Œç»§ç»­ä½¿ç”¨æœªè¿‡æ»¤ç»“æœ")
            
            if not args.quiet:
                print(f"ğŸ¯ åˆå¹¶åæ€»è®¡: {len(trending_df)} ä¸ªå…³é”®è¯")
                
                # æ˜¾ç¤ºæ•°æ®æºåˆ†å¸ƒ
                if 'source' in trending_df.columns:
                    source_counts = trending_df['source'].value_counts()
                    print("ğŸ“Š æ•°æ®æºåˆ†å¸ƒ:")
                    for source, count in source_counts.items():
                        print(f"   â€¢ {source}: {count} ä¸ª")
        else:
            trending_df = pd.DataFrame(columns=['query'])
        
        if trending_df is not None and not trending_df.empty:
            # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
            if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
            
            # ç¬¬ä¸€æ­¥ï¼šå¯¹çƒ­é—¨å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜
            cleaned_terms = None
            try:
                from src.pipeline.cleaning.cleaner import clean_terms, CleaningConfig
                if 'query' in trending_df.columns:
                    cleaned_terms = clean_terms(
                        trending_df['query'].astype(str).tolist(),
                        CleaningConfig()
                    )
            except Exception as exc:
                if not args.quiet:
                    print(f"âš ï¸ æ¸…æ´—çƒ­é—¨å…³é”®è¯æ—¶å‡ºç°å¼‚å¸¸: {exc}")
                cleaned_terms = trending_df['query'].dropna().astype(str).tolist() if 'query' in trending_df.columns else []
            if cleaned_terms is not None:
                if not cleaned_terms:
                    if not args.quiet:
                        print("âš ï¸ æ¸…æ´—åçš„çƒ­é—¨å…³é”®è¯ä¸ºç©ºï¼Œå·²ç»ˆæ­¢å®Œæ•´æµç¨‹ã€‚")
                    print("ğŸ’¡ è¯·ç¨åé‡è¯•ï¼Œæˆ–ä½¿ç”¨ --input æŒ‡å®šæœ¬åœ°å…³é”®è¯æ–‡ä»¶ã€‚")
                    return True
                trending_df = pd.DataFrame({'query': cleaned_terms})

            if not args.quiet:
                print(f"ğŸ” ç¬¬ä¸€æ­¥: å¯¹ {len(trending_df)} ä¸ªçƒ­é—¨å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜...")

            # æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
            original_new_word_flag = getattr(manager, 'new_word_detection_available', True)
            hot_result = None
            try:
                manager.new_word_detection_available = False
                hot_result = manager.analyze_keywords(trending_df, args.output, enable_serp=False)
            finally:
                manager.new_word_detection_available = original_new_word_flag

            if not hot_result:
                print("âš ï¸ çƒ­é—¨å…³é”®è¯åˆ†ææœªè¿”å›ç»“æœï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
                return True

            if not args.quiet:
                print(f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ! åˆ†æäº† {hot_result['total_keywords']} ä¸ªçƒ­é—¨å…³é”®è¯")
                print(f"ğŸ“Š å‘ç° {hot_result['market_insights']['high_opportunity_count']} ä¸ªé«˜æœºä¼šå…³é”®è¯")
                _print_new_word_summary(hot_result.get('new_word_summary'))
                _print_top_new_words(hot_result)
                print("\nğŸŒ ç¬¬äºŒæ­¥: åŸºäºçƒ­é—¨å…³é”®è¯è¿›è¡Œå¤šå¹³å°å…³é”®è¯å‘ç°...")
                
                # é€‰å–æœºä¼šåˆ†æœ€é«˜çš„å…³é”®è¯ä½œä¸ºç§å­
                seed_keywords: List[str] = []
                if isinstance(hot_result, dict) and hot_result.get('keywords'):
                    sorted_keywords = sorted(
                        (kw for kw in hot_result['keywords'] if kw.get('keyword')),
                        key=lambda kw: kw.get('opportunity_score', 0),
                        reverse=True
                    )
                    seed_keywords = [kw['keyword'] for kw in sorted_keywords[:max_seed_keywords]]

                if len(seed_keywords) < max_seed_keywords and 'query' in trending_df.columns:
                    fallback_candidates: List[str] = [
                        kw for kw in trending_df['query'].tolist()
                        if kw and kw not in seed_keywords
                    ]
                    seed_keywords.extend(fallback_candidates[:max_seed_keywords - len(seed_keywords)])

                seed_keywords = [kw for kw in seed_keywords if kw][:max_seed_keywords]
                if not seed_keywords:
                    if not args.quiet:
                        print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç§å­å…³é”®è¯ï¼Œè·³è¿‡å¤šå¹³å°å…³é”®è¯å‘ç°ã€‚")
                        print("ğŸ’¡ å»ºè®®æ£€æŸ¥ç¬¬ä¸€æ­¥ç»“æœï¼Œæˆ–ä½¿ç”¨ --input æŒ‡å®šæœ¬åœ°å…³é”®è¯æ–‡ä»¶ã€‚")
                    return True

                discovery_tool = MultiPlatformKeywordDiscovery()
                seed_profile = getattr(args, 'seed_profile', None)
                seed_limit_arg = getattr(args, 'seed_limit', None)
                if isinstance(seed_limit_arg, int) and seed_limit_arg <= 0:
                    seed_limit_arg = None
                min_seed_terms = getattr(args, 'min_seed_terms', None)
                if isinstance(min_seed_terms, int) and min_seed_terms <= 0:
                    min_seed_terms = None
                effective_limit = seed_limit_arg or max_seed_keywords
                prepared_seeds = discovery_tool.prepare_search_terms(
                    seeds=seed_keywords,
                    profile=seed_profile,
                    limit=effective_limit,
                    min_terms=min_seed_terms
                )

                if not prepared_seeds:
                    if not args.quiet:
                        print("âš ï¸ æ— æœ‰æ•ˆç§å­å…³é”®è¯å¯ç”¨äºå¤šå¹³å°å‘ç°ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
                    return True

                if not args.quiet:
                    extra_seed_count = len([kw for kw in prepared_seeds if kw not in seed_keywords])
                    if extra_seed_count > 0:
                        print(f"â„¹ï¸ å·²è¿½åŠ  {extra_seed_count} ä¸ªé…ç½®ç§å­å…³é”®è¯ä»¥æ»¡è¶³å‘ç°éœ€æ±‚")
                    print(f"ğŸ” æ­£åœ¨å‘ç°ä¸ {len(prepared_seeds)} ä¸ªå…³é”®è¯ç›¸å…³çš„å…³é”®è¯...")

                df = discovery_tool.discover_all_platforms(prepared_seeds)
                
                unique_keywords = []
                prioritized_df = None
                if not df.empty and 'keyword' in df.columns:
                    keyword_series = df['keyword'].dropna().astype(str)
                    if 'score' in df.columns:
                        prioritized_df = df[['keyword', 'score']].dropna(subset=['keyword'])
                        prioritized_df = prioritized_df.sort_values('score', ascending=False)
                        prioritized_df = prioritized_df.drop_duplicates(subset=['keyword'], keep='first')
                    else:
                        counts = keyword_series.value_counts().reset_index()
                        counts.columns = ['keyword', 'score']
                        prioritized_df = counts
                    prioritized_df['score'] = prioritized_df['score'].fillna(0)
                    prioritized_df['keyword'] = prioritized_df['keyword'].astype(str)
                    unique_keywords = prioritized_df['keyword'].head(max_discovered_keywords).tolist()

                if unique_keywords:
                    if not args.quiet and prioritized_df is not None and len(prioritized_df) > len(unique_keywords):
                        print(f"âš–ï¸ å·²æŒ‰å¾—åˆ†ç­›é€‰å‰ {len(unique_keywords)} ä¸ªå…³é”®è¯ç”¨äºæœ€ç»ˆåˆ†æ (ä¸Šé™ {max_discovered_keywords})")
                    # åˆ›å»ºå‘ç°å…³é”®è¯çš„CSVæ–‡ä»¶
                    discovered_df = pd.DataFrame([
                        {'keyword': kw} for kw in unique_keywords
                    ])
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    discovered_file = os.path.join(args.output, f"discovered_keywords_{timestamp}.csv")
                    os.makedirs(args.output, exist_ok=True)
                    discovered_df.to_csv(discovered_file, index=False, encoding='utf-8')
                    
                    if not args.quiet:
                        print(f"ğŸ¯ å‘ç°äº† {len(unique_keywords)} ä¸ªç›¸å…³å…³é”®è¯")
                        print(f"ğŸ“ å‘ç°çš„å…³é”®è¯å·²ä¿å­˜åˆ°: {discovered_file}")
                    
                    # å¯¹å‘ç°çš„å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
                    if not args.quiet:
                        print(f"\nğŸ” ç¬¬ä¸‰æ­¥: å¯¹å‘ç°çš„å…³é”®è¯è¿›è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ...")
                    
                    discovery_result = manager.analyze_keywords(discovered_file, args.output, enable_serp=False)
                    
                    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
                    if args.quiet:
                        print_quiet_summary(discovery_result)
                    else:
                        print(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆ!")
                        print(f"ğŸ“Š çƒ­é—¨å…³é”®è¯åˆ†æ: {hot_result['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸŒ å¤šå¹³å°å‘ç°: {len(unique_keywords)} ä¸ªç›¸å…³å…³é”®è¯")
                        print(f"ğŸ¯ æœ€ç»ˆåˆ†æ: {discovery_result['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸ† æ€»è®¡é«˜æœºä¼šå…³é”®è¯: {discovery_result['market_insights']['high_opportunity_count']} ä¸ª")
                        print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {discovery_result['market_insights']['avg_opportunity_score']}")
                        
                        # æ˜¾ç¤ºTop 5æœºä¼šå…³é”®è¯
                        top_keywords = discovery_result['market_insights']['top_opportunities'][:5]
                        if top_keywords:
                            print("\nğŸ† Top 5 æœ€ç»ˆæœºä¼šå…³é”®è¯:")
                            for i, kw in enumerate(top_keywords, 1):
                                intent_desc = kw['intent']['intent_description']
                                score = kw['opportunity_score']
                                print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc})")
                
                else:
                    if not args.quiet:
                        print("âš ï¸ æœªå‘ç°ç›¸å…³å…³é”®è¯ï¼Œä»…æ˜¾ç¤ºçƒ­é—¨å…³é”®è¯åˆ†æç»“æœ")
                    
                    if args.quiet:
                        print_quiet_summary(hot_result)
                    else:
                        print(f"\nğŸ‰ çƒ­é—¨å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {hot_result['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {hot_result['market_insights']['high_opportunity_count']} ä¸ª")
                        print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {hot_result['market_insights']['avg_opportunity_score']}")
        
        else:
            if not args.quiet:
                print("âŒ æ— æ³•è·å–çƒ­é—¨å…³é”®è¯ï¼Œå·¥ä½œæµç¨‹ç»ˆæ­¢")
                print("ğŸ’¡ å»ºè®®:")
                print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                print("   2. ç¨åé‡è¯•")
                print("   3. æˆ–ä½¿ç”¨å…¶ä»–å‚æ•°è¿›è¡Œåˆ†æ")
            return True
    
    except Exception as e:
        print(f"âŒ å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡Œæ—¶å‡ºé”™: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    
    return True


def handle_demand_validation(manager, args):
    """
    å¤„ç†éœ€æ±‚éªŒè¯æµç¨‹
    
    Args:
        manager: IntegratedDemandMiningManagerå®ä¾‹
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("ğŸ” å¼€å§‹éœ€æ±‚éªŒè¯æµç¨‹...")
    print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–é«˜æœºä¼šå…³é”®è¯")
    
    try:
        from src.collectors.trends_singleton import get_trends_collector
        trends_collector = get_trends_collector()
        rising_queries = trends_collector.fetch_rising_queries()
        
        if isinstance(rising_queries, pd.DataFrame):
            trending_df = rising_queries.head(20)
            if 'query' not in trending_df.columns:
                if 'title' in trending_df.columns:
                    trending_df = trending_df.rename(columns={'title': 'query'})
                elif len(trending_df.columns) > 0:
                    trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
        elif rising_queries and len(rising_queries) > 0:
            if isinstance(rising_queries[0], str):
                trending_df = pd.DataFrame([{'query': query} for query in rising_queries[:20]])
            elif isinstance(rising_queries[0], dict):
                trending_df = pd.DataFrame([{
                    'query': item.get('query', item.get('keyword', str(item))),
                    'value': item.get('value', item.get('interest', 0))
                } for item in rising_queries[:20]])
            else:
                trending_df = pd.DataFrame([{'query': str(query)} for query in rising_queries[:20]])
        else:
            trending_df = pd.DataFrame(columns=['query'])

        if trending_df is not None and not trending_df.empty:
            if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})

            try:
                from src.pipeline.cleaning.cleaner import clean_terms, CleaningConfig
                cleaned = clean_terms(
                    trending_df['query'].astype(str).tolist(),
                    CleaningConfig()
                )
                trending_df = pd.DataFrame({'query': cleaned})
            except Exception:
                trending_df = pd.DataFrame({'query': trending_df['query'].astype(str).tolist()})

            original_new_word_flag = getattr(manager, 'new_word_detection_available', True)
            keywords_result = None
            try:
                print(f"ğŸ” è·å–åˆ° {len(trending_df)} ä¸ªå…³é”®è¯ï¼Œå¼€å§‹æœºä¼šåˆ†æ...")
                manager.new_word_detection_available = False
                keywords_result = manager.analyze_keywords(trending_df, args.output, enable_serp=False)
            finally:
                manager.new_word_detection_available = original_new_word_flag

            if not keywords_result:
                print("âš ï¸ å…³é”®è¯åˆ†ææœªè¿”å›ç»“æœï¼Œç»ˆæ­¢éœ€æ±‚éªŒè¯æµç¨‹ã€‚")
                return True

            print(f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ! åˆ†æäº† {keywords_result['total_keywords']} ä¸ªå…³é”®è¯")

            # ç¬¬äºŒæ­¥ï¼šå¤šå¹³å°éœ€æ±‚éªŒè¯
            print("\nğŸ“‹ ç¬¬äºŒæ­¥ï¼šå¤šå¹³å°éœ€æ±‚éªŒè¯")

            try:
                # ç¡®ä¿èƒ½å¤Ÿå¯¼å…¥æ¨¡å—
                analyzer_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'src', 'demand_mining', 'analyzers')
                if analyzer_path not in sys.path:
                    sys.path.insert(0, analyzer_path)

                from src.demand_mining.analyzers.multi_platform_demand_analyzer import MultiPlatformDemandAnalyzer

                # åˆ›å»ºå¤šå¹³å°åˆ†æå™¨
                demand_analyzer = MultiPlatformDemandAnalyzer()

                # æ‰§è¡Œå¤šå¹³å°éœ€æ±‚åˆ†æ
                import asyncio
                demand_results = asyncio.run(demand_analyzer.analyze_high_opportunity_keywords(
                    keywords_result.get('keywords', []),
                    min_opportunity_score=60.0,  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šå…³é”®è¯
                    max_keywords=3  # é™åˆ¶åˆ†ææ•°é‡é¿å…è¯·æ±‚è¿‡å¤š
                ))

                # ä¿å­˜éœ€æ±‚éªŒè¯ç»“æœ
                demand_output_file = demand_analyzer.save_results(demand_results)

                print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆ! éœ€æ±‚éªŒè¯åˆ†æå®Œæˆ")
                print(f"ğŸ“Š åˆ†æäº† {demand_results.get('analyzed_keywords', 0)} ä¸ªé«˜æœºä¼šå…³é”®è¯")

                # æ˜¾ç¤ºéœ€æ±‚éªŒè¯æ‘˜è¦
                summary = demand_results.get('summary', {})
                if summary:
                    print(f"\nğŸ¯ éœ€æ±‚éªŒè¯æ‘˜è¦:")
                    print(f"   â€¢ æ€»æœç´¢ç»“æœ: {summary.get('total_search_results', 0)}")
                    print(f"   â€¢ å‘ç°ç—›ç‚¹: {summary.get('total_pain_points_found', 0)} ä¸ª")
                    print(f"   â€¢ åŠŸèƒ½éœ€æ±‚: {summary.get('total_feature_requests_found', 0)} ä¸ª")

                    high_demand = summary.get('high_demand_keywords', [])
                    if high_demand:
                        print(f"   â€¢ é«˜éœ€æ±‚å…³é”®è¯: {', '.join(high_demand)}")

                    top_opportunities = summary.get('top_opportunities', [])[:3]
                    if top_opportunities:
                        print(f"\nğŸ† Top 3 éªŒè¯ç»“æœ:")
                        for i, opp in enumerate(top_opportunities, 1):
                            print(f"   {i}. {opp['keyword']} - {opp['demand_level']} ({opp['pain_points_count']} ä¸ªç—›ç‚¹)")

                print(f"\nğŸ“ éœ€æ±‚éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {demand_output_file}")

            except ImportError:
                print("âš ï¸ å¤šå¹³å°éœ€æ±‚åˆ†æå™¨æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ç›¸å…³æ¨¡å—å·²å®‰è£…")
            except Exception as e:
                print(f"âŒ éœ€æ±‚éªŒè¯å¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
                
        else:
            print("âŒ æ— æ³•è·å–å…³é”®è¯è¿›è¡Œéœ€æ±‚éªŒè¯")
            
    except Exception as e:
        print(f"âŒ éœ€æ±‚éªŒè¯æµç¨‹å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

def refresh_dashboard_data(output_dir: str, history_size: int = 20) -> None:
    """Refresh aggregated dashboard data after primary workflows finish."""
    try:
        from src.utils.dashboard_data_builder import generate_dashboard_payload
        from src.utils.file_utils import ensure_directory_exists

        target_dir = Path(output_dir or get_reports_dir()).resolve()
        ensure_directory_exists(str(target_dir))

        payload = generate_dashboard_payload(target_dir, history_size=history_size)
        dashboard_path = target_dir / "dashboard_data.json"
        with dashboard_path.open("w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)

        if payload.get("last_updated"):
            print(f"[Dashboard] æ•°æ®å·²åˆ·æ–°: {dashboard_path}")
        else:
            print("[Dashboard] ä»ªè¡¨ç›˜å·²æ›´æ–°ï¼Œä½†å°šæœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„åˆ†æç»“æœã€‚")
    except Exception as exc:
        print(f"[Dashboard] åˆ·æ–°ä»ªè¡¨ç›˜æ•°æ®å¤±è´¥: {exc}")
