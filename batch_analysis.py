#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å…³é”®è¯åˆ†æå·¥å…·
ç”¨äºåˆ†æAIå·¥å…·ç›¸å…³çš„å¤šä¸ªç§å­å…³é”®è¯
"""

import sys
import os
import json
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.core.market_analyzer import MarketAnalyzer
from src.utils import Logger, safe_print

def load_seed_keywords(file_path):
    """ä»æ–‡ä»¶åŠ è½½ç§å­å…³é”®è¯"""
    keywords = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                if line and not line.startswith('#'):
                    keywords.append(line)
        
        safe_print(f"å·²åŠ è½½ {len(keywords)} ä¸ªç§å­å…³é”®è¯")
        return keywords
        
    except Exception as e:
        safe_print(f"åŠ è½½å…³é”®è¯æ–‡ä»¶å¤±è´¥: {e}")
        return []

def analyze_ai_tools_keywords():
    """åˆ†æAIå·¥å…·ç›¸å…³å…³é”®è¯"""
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = Logger()
    logger.setup_console_encoding()
    
    safe_print("AIå·¥å…·å…³é”®è¯æ‰¹é‡åˆ†æ")
    safe_print("=" * 50)
    
    # åŠ è½½ç§å­å…³é”®è¯
    seed_file = 'data/ai_tools_seed_keywords.txt'
    keywords = load_seed_keywords(seed_file)
    
    if not keywords:
        safe_print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç§å­å…³é”®è¯")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MarketAnalyzer('data/ai_tools_analysis')
    
    # åˆ†æå‚æ•°
    analysis_params = {
        'geo': 'US',  # ä¸»è¦é’ˆå¯¹ç¾å›½å¸‚åœº
        'timeframe': 'today 3-m',
        'volume_weight': 0.4,
        'growth_weight': 0.4,
        'kd_weight': 0.2,
        'min_score': 20,  # é™ä½æœ€ä½åˆ†æ•°ä»¥è·å–æ›´å¤šå…³é”®è¯
        'enrich': True
    }
    
    safe_print(f"åˆ†æå‚æ•°: {analysis_params}")
    safe_print("")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    successful_analyses = 0
    failed_analyses = 0
    
    # åˆ†æ‰¹å¤„ç†å…³é”®è¯ï¼ˆæ¯æ‰¹3-5ä¸ªï¼‰
    batch_size = 3
    total_batches = (len(keywords) + batch_size - 1) // batch_size
    
    for batch_idx in range(0, len(keywords), batch_size):
        batch_keywords = keywords[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        
        safe_print(f"æ­£åœ¨åˆ†æç¬¬ {batch_num}/{total_batches} æ‰¹å…³é”®è¯: {batch_keywords}")
        
        try:
            # è¿è¡Œåˆ†æ
            result = analyzer.run_analysis(
                keywords=batch_keywords,
                **analysis_params
            )
            
            if 'error' in result:
                safe_print(f"  âŒ æ‰¹æ¬¡ {batch_num} åˆ†æå¤±è´¥: {result['error']}")
                failed_analyses += 1
            else:
                safe_print(f"  âœ… æ‰¹æ¬¡ {batch_num} åˆ†ææˆåŠŸ: {result['å…³é”®è¯æ€»æ•°']} ä¸ªå…³é”®è¯")
                
                # æ·»åŠ æ‰¹æ¬¡ä¿¡æ¯
                result['æ‰¹æ¬¡'] = batch_num
                result['ç§å­å…³é”®è¯'] = batch_keywords
                all_results.append(result)
                successful_analyses += 1
                
        except Exception as e:
            safe_print(f"  âŒ æ‰¹æ¬¡ {batch_num} åˆ†æå‡ºé”™: {e}")
            failed_analyses += 1
        
        safe_print("")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    if all_results:
        generate_comprehensive_report(all_results, analysis_params)
    
    # æ˜¾ç¤ºæ€»ç»“
    safe_print("æ‰¹é‡åˆ†æå®Œæˆ!")
    safe_print(f"æˆåŠŸåˆ†æ: {successful_analyses} æ‰¹")
    safe_print(f"å¤±è´¥åˆ†æ: {failed_analyses} æ‰¹")
    safe_print(f"æ€»å…³é”®è¯æ•°: {sum(r['å…³é”®è¯æ€»æ•°'] for r in all_results)}")
    safe_print(f"é«˜åˆ†å…³é”®è¯: {sum(r['é«˜åˆ†å…³é”®è¯æ•°'] for r in all_results)}")

def generate_comprehensive_report(results, params):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    safe_print("æ­£åœ¨ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    # æ±‡æ€»ç»Ÿè®¡
    total_keywords = sum(r['å…³é”®è¯æ€»æ•°'] for r in results)
    total_high_score = sum(r['é«˜åˆ†å…³é”®è¯æ•°'] for r in results)
    avg_score = sum(r['å¹³å‡æŒ‡æ ‡']['å¹³å‡è¯„åˆ†'] for r in results) / len(results)
    avg_volume = sum(r['å¹³å‡æŒ‡æ ‡']['å¹³å‡æœç´¢é‡'] for r in results) / len(results)
    
    # æ”¶é›†æ‰€æœ‰Topå…³é”®è¯
    all_top_keywords = []
    for result in results:
        for kw in result['Top5å…³é”®è¯']:
            kw['æ‰¹æ¬¡'] = result['æ‰¹æ¬¡']
            kw['ç§å­å…³é”®è¯'] = ', '.join(result['ç§å­å…³é”®è¯'])
            all_top_keywords.append(kw)
    
    # æŒ‰åˆ†æ•°æ’åºï¼Œå–Top 20
    top_keywords = sorted(all_top_keywords, key=lambda x: x['score'], reverse=True)[:20]
    
    # æ±‡æ€»æ„å›¾åˆ†å¸ƒ
    intent_summary = {}
    for result in results:
        for intent, count in result['æ„å›¾åˆ†å¸ƒ'].items():
            intent_summary[intent] = intent_summary.get(intent, 0) + count
    
    # åˆ›å»ºç»¼åˆæŠ¥å‘Š
    comprehensive_report = {
        'åˆ†ææ¦‚è§ˆ': {
            'åˆ†ææ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'åˆ†ææ‰¹æ¬¡æ•°': len(results),
            'æ€»ç§å­å…³é”®è¯æ•°': sum(len(r['ç§å­å…³é”®è¯']) for r in results),
            'åˆ†æå‚æ•°': params
        },
        'å…³é”®æŒ‡æ ‡': {
            'æ€»å…³é”®è¯æ•°': total_keywords,
            'é«˜åˆ†å…³é”®è¯æ•°': total_high_score,
            'é«˜åˆ†å…³é”®è¯æ¯”ä¾‹': f"{total_high_score/total_keywords*100:.1f}%" if total_keywords > 0 else "0%",
            'å¹³å‡è¯„åˆ†': round(avg_score, 2),
            'å¹³å‡æœç´¢é‡': round(avg_volume, 0)
        },
        'Top20é«˜ä»·å€¼å…³é”®è¯': top_keywords,
        'æ•´ä½“æ„å›¾åˆ†å¸ƒ': intent_summary,
        'å„æ‰¹æ¬¡è¯¦æƒ…': [
            {
                'æ‰¹æ¬¡': r['æ‰¹æ¬¡'],
                'ç§å­å…³é”®è¯': r['ç§å­å…³é”®è¯'],
                'å…³é”®è¯æ€»æ•°': r['å…³é”®è¯æ€»æ•°'],
                'é«˜åˆ†å…³é”®è¯æ•°': r['é«˜åˆ†å…³é”®è¯æ•°'],
                'å¹³å‡è¯„åˆ†': r['å¹³å‡æŒ‡æ ‡']['å¹³å‡è¯„åˆ†']
            }
            for r in results
        ]
    }
    
    # ä¿å­˜ç»¼åˆæŠ¥å‘Š
    import json
    report_file = f"data/ai_tools_analysis/comprehensive_report_{datetime.now().strftime('%Y-%m-%d')}.json"
    os.makedirs(os.path.dirname(report_file), exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
    
    safe_print(f"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # æ˜¾ç¤ºå…³é”®å‘ç°
    safe_print("\nğŸ¯ å…³é”®å‘ç°:")
    safe_print(f"   â€¢ æ€»å…±å‘ç° {total_keywords} ä¸ªç›¸å…³å…³é”®è¯")
    safe_print(f"   â€¢ å…¶ä¸­ {total_high_score} ä¸ªä¸ºé«˜ä»·å€¼å…³é”®è¯")
    safe_print(f"   â€¢ å¹³å‡è¯„åˆ†: {avg_score:.1f}")
    safe_print(f"   â€¢ å¹³å‡æœç´¢é‡: {avg_volume:.0f}")
    
    safe_print("\nğŸ† Top 5 é«˜ä»·å€¼å…³é”®è¯:")
    for i, kw in enumerate(top_keywords[:5]):
        safe_print(f"   {i+1}. {kw['query']} (åˆ†æ•°: {kw['score']}, {kw['intent']}, æœç´¢é‡: {kw['volume']})")
    
    safe_print("\nğŸ“Š æ„å›¾åˆ†å¸ƒ:")
    for intent, count in sorted(intent_summary.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_keywords * 100
        safe_print(f"   â€¢ {intent}: {count} ä¸ªå…³é”®è¯ ({percentage:.1f}%)")

if __name__ == "__main__":
    analyze_ai_tools_keywords()