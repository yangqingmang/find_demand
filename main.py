#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - ä¸»å…¥å£æ–‡ä»¶
æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„ç»Ÿä¸€æ‰§è¡Œå…¥å£
"""

import argparse
import sys
import os
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥ç»Ÿä¸€çš„éœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
from src.demand_mining.unified_main import UnifiedDemandMiningManager as DemandMiningManager

# å¯¼å…¥å¢å¼ºåŠŸèƒ½æ¨¡å—
try:
    from src.utils.enhanced_features import (
        monitor_competitors, predict_keyword_trends, generate_seo_audit,
        batch_build_websites, setup_scheduler, run_scheduler
    )
    ENHANCED_FEATURES_AVAILABLE = True
except ImportError:
    ENHANCED_FEATURES_AVAILABLE = False
    print("âš ï¸ å¢å¼ºåŠŸèƒ½æ¨¡å—æœªæ‰¾åˆ°ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸‚åœºéœ€æ±‚åˆ†æå·¥å…· - ä¸»å…¥å£æ–‡ä»¶
Market Demand Analysis Toolkit - Main Entry Point
"""


def print_quiet_summary(result):
    """é™é»˜æ¨¡å¼ä¸‹çš„ç®€è¦ç»“æœæ˜¾ç¤º"""
    print("\nğŸ¯ éœ€æ±‚æŒ–æ˜åˆ†æç»“æœæ‘˜è¦:")
    print(f"   â€¢ å…³é”®è¯æ€»æ•°: {result.get('total_keywords', 0)}")
    print(f"   â€¢ é«˜æœºä¼šå…³é”®è¯: {result.get('market_insights', {}).get('high_opportunity_count', 0)}")
    print(f"   â€¢ å¹³å‡æœºä¼šåˆ†æ•°: {result.get('market_insights', {}).get('avg_opportunity_score', 0)}")
    
    # æ˜¾ç¤ºTop 3å…³é”®è¯
    top_keywords = result.get('market_insights', {}).get('top_opportunities', [])[:3]
    if top_keywords:
        print("\nğŸ† Top 3 æœºä¼šå…³é”®è¯:")
        for i, kw in enumerate(top_keywords):
            intent_desc = kw.get('intent', {}).get('intent_description', 'æœªçŸ¥')
            score = kw.get('opportunity_score', 0)
            print(f"   {i+1}. {kw['keyword']} (æœºä¼šåˆ†æ•°: {score}, æ„å›¾: {intent_desc})")

def main():
    """ä¸»å‡½æ•° - æä¾›ç»Ÿä¸€çš„æ‰§è¡Œå…¥å£"""
    
    print("ğŸ” éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· v2.0")
    print("æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„æ™ºèƒ½åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - æ•´åˆå…­å¤§æŒ–æ˜æ–¹æ³•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ å…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•:
  1. åŸºäºè¯æ ¹å…³é”®è¯æ‹“å±• (52ä¸ªæ ¸å¿ƒè¯æ ¹)
  2. åŸºäºSEOå¤§ç«™æµé‡åˆ†æ (8ä¸ªç«å“ç½‘ç«™)
  3. æœç´¢å¼•æ“ä¸‹æ‹‰æ¨è
  4. å¾ªç¯æŒ–æ˜æ³•
  5. ä»˜è´¹å¹¿å‘Šå…³é”®è¯åˆ†æ
  6. æ”¶å…¥æ’è¡Œæ¦œåˆ†æ

ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹:
  # åˆ†æå…³é”®è¯æ–‡ä»¶
  python main.py --input data/keywords.csv
  
  # åˆ†æå•ä¸ªå…³é”®è¯
  python main.py --keywords "ai generator" "ai converter"
  
  # å¤šå¹³å°å…³é”®è¯å‘ç°
  python main.py --discover "AI image generator" "AI writing tool"
  
  # ä½¿ç”¨é»˜è®¤æœç´¢è¯è¿›è¡Œå¤šå¹³å°å‘ç°
  python main.py --discover default
  
  # ç”Ÿæˆåˆ†ææŠ¥å‘Š
  python main.py --report

  # é™é»˜æ¨¡å¼åˆ†æ
  python main.py --input data/keywords.csv --quiet

ğŸš€ å¢å¼ºåŠŸèƒ½ç¤ºä¾‹:
  # ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–
  python main.py --monitor-competitors --sites canva.com midjourney.com

  # é¢„æµ‹å…³é”®è¯è¶‹åŠ¿
  python main.py --predict-trends --timeframe 30d

  # SEOå®¡è®¡
  python main.py --seo-audit --domain your-site.com --keywords "ai tool" "ai generator"

  # æ‰¹é‡ç”Ÿæˆç½‘ç«™
  python main.py --build-websites --top-keywords 5

  # è®¾ç½®å®šæ—¶ä»»åŠ¡
  python main.py --schedule daily --time "09:00" --action discover --run-scheduler
        """
    )
    
    # è¾“å…¥æ–¹å¼é€‰æ‹© - ä¿®æ”¹ä¸ºéå¿…éœ€ï¼Œæ”¯æŒé»˜è®¤è¯æ ¹åˆ†æ
    input_group = parser.add_mutually_exclusive_group(required=False)
    input_group.add_argument('--input', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    input_group.add_argument('--keywords', nargs='+', help='ç›´æ¥è¾“å…¥å…³é”®è¯ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªï¼‰')
    input_group.add_argument('--discover', nargs='+', help='å¤šå¹³å°å…³é”®è¯å‘ç°ï¼ˆå¯æŒ‡å®šæœç´¢è¯æ±‡ï¼‰')
    input_group.add_argument('--report', action='store_true', help='ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š')
    
    # å¢å¼ºåŠŸèƒ½ç»„
    enhanced_group = parser.add_argument_group('å¢å¼ºåŠŸèƒ½')
    enhanced_group.add_argument('--monitor-competitors', action='store_true', help='ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–')
    enhanced_group.add_argument('--sites', nargs='+', help='ç«å“ç½‘ç«™åˆ—è¡¨')
    enhanced_group.add_argument('--predict-trends', action='store_true', help='é¢„æµ‹å…³é”®è¯è¶‹åŠ¿')
    enhanced_group.add_argument('--timeframe', default='30d', help='é¢„æµ‹æ—¶é—´èŒƒå›´')
    enhanced_group.add_argument('--seo-audit', action='store_true', help='ç”ŸæˆSEOä¼˜åŒ–å»ºè®®')
    enhanced_group.add_argument('--domain', help='è¦å®¡è®¡çš„åŸŸå')
    enhanced_group.add_argument('--build-websites', action='store_true', help='æ‰¹é‡ç”Ÿæˆç½‘ç«™')
    enhanced_group.add_argument('--top-keywords', type=int, default=10, help='ä½¿ç”¨å‰Nä¸ªå…³é”®è¯')
    
    # è°ƒåº¦åŠŸèƒ½ç»„
    schedule_group = parser.add_argument_group('å®šæ—¶ä»»åŠ¡')
    schedule_group.add_argument('--schedule', choices=['daily', 'weekly', 'hourly'], help='è®¾ç½®å®šæ—¶ä»»åŠ¡')
    schedule_group.add_argument('--time', help='æ‰§è¡Œæ—¶é—´ (HH:MM)')
    schedule_group.add_argument('--action', help='å®šæ—¶æ‰§è¡Œçš„åŠ¨ä½œ')
    schedule_group.add_argument('--run-scheduler', action='store_true', help='è¿è¡Œè°ƒåº¦å™¨')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--output', default='src/demand_mining/reports', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæœ€ç»ˆç»“æœ')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ¨¡å¼ï¼Œæ˜¾ç¤ºæ‰€æœ‰ä¸­é—´è¿‡ç¨‹')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºåˆ†æå‚æ•°
    if not args.quiet:
        if args.input:
            print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input}")
        elif args.keywords:
            print(f"ğŸ”¤ åˆ†æå…³é”®è¯: {', '.join(args.keywords)}")
        elif args.report:
            print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
        print("")
    
    try:
        # åˆ›å»ºéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
        manager = DemandMiningManager(args.config)
        
        if args.input:
            # åˆ†æå…³é”®è¯æ–‡ä»¶
            if not args.quiet:
                print("ğŸš€ å¼€å§‹åˆ†æå…³é”®è¯æ–‡ä»¶...")
            
            result = manager.analyze_keywords(args.input, args.output)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
                
                # æ˜¾ç¤ºTop 5å…³é”®è¯
                top_keywords = result['market_insights']['top_opportunities'][:5]
                if top_keywords:
                    print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                    for i, kw in enumerate(top_keywords, 1):
                        intent_desc = kw['intent']['intent_description']
                        score = kw['opportunity_score']
                        print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc})")
        
        elif args.keywords:
            # åˆ†æå•ä¸ªå…³é”®è¯
            if not args.quiet:
                print("ğŸš€ å¼€å§‹åˆ†æè¾“å…¥çš„å…³é”®è¯...")
            
            # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶
            import pandas as pd
            import tempfile
            
            temp_df = pd.DataFrame([{'query': kw} for kw in args.keywords])
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                temp_df.to_csv(f.name, index=False)
                temp_file = f.name
            
            try:
                result = manager.analyze_keywords(temp_file, args.output)
                
                # æ˜¾ç¤ºç»“æœ
                if args.quiet:
                    print_quiet_summary(result)
                else:
                    print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {len(args.keywords)} ä¸ªå…³é”®è¯")
                    
                    # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„ç»“æœ
                    print("\nğŸ“‹ å…³é”®è¯åˆ†æç»“æœ:")
                    for kw_result in result['keywords']:
                        keyword = kw_result['keyword']
                        score = kw_result['opportunity_score']
                        intent = kw_result['intent']['intent_description']
                        print(f"   â€¢ {keyword}: æœºä¼šåˆ†æ•° {score}, æ„å›¾: {intent}")
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file)
        
        elif args.discover:
            # å¤šå¹³å°å…³é”®è¯å‘ç°
            search_terms = args.discover if args.discover != ['default'] else ['AI tool', 'AI generator', 'AI assistant']
            
            if not args.quiet:
                print("ğŸ” å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
                print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
            
            try:
                # å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·
                from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery
                
                # åˆ›å»ºå‘ç°å·¥å…·
                discoverer = MultiPlatformKeywordDiscovery()
                
                # æ‰§è¡Œå‘ç°
                df = discoverer.discover_all_platforms(search_terms)
                
                if not df.empty:
                    # åˆ†æè¶‹åŠ¿
                    analysis = discoverer.analyze_keyword_trends(df)
                    
                    # ä¿å­˜ç»“æœ
                    output_dir = os.path.join(args.output, 'multi_platform_discovery')
                    csv_path, json_path = discoverer.save_results(df, analysis, output_dir)
                    
                    if args.quiet:
                        # é™é»˜æ¨¡å¼æ˜¾ç¤º
                        print(f"\nğŸ¯ å¤šå¹³å°å…³é”®è¯å‘ç°ç»“æœ:")
                        print(f"   â€¢ å‘ç°å…³é”®è¯: {analysis['total_keywords']} ä¸ª")
                        print(f"   â€¢ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                        
                        # æ˜¾ç¤ºTop 3å…³é”®è¯
                        top_keywords = analysis['top_keywords_by_score'][:3]
                        if top_keywords:
                            print("\nğŸ† Top 3 çƒ­é—¨å…³é”®è¯:")
                            for i, kw in enumerate(top_keywords, 1):
                                print(f"   {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
                    else:
                        # è¯¦ç»†æ¨¡å¼æ˜¾ç¤º
                        print(f"\nğŸ‰ å¤šå¹³å°å…³é”®è¯å‘ç°å®Œæˆ!")
                        print(f"ğŸ“Š å‘ç° {analysis['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸŒ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                        
                        print(f"\nğŸ† çƒ­é—¨å…³é”®è¯:")
                        for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                            print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
                    
                    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜:")
                    print(f"  CSV: {csv_path}")
                    print(f"  JSON: {json_path}")
                    
                    # è¯¢é—®æ˜¯å¦è¦ç«‹å³åˆ†æå‘ç°çš„å…³é”®è¯
                    if not args.quiet:
                        user_input = input("\nğŸ¤” æ˜¯å¦è¦ç«‹å³åˆ†æè¿™äº›å…³é”®è¯çš„æ„å›¾å’Œå¸‚åœºæœºä¼š? (y/n): ")
                        if user_input.lower() in ['y', 'yes', 'æ˜¯']:
                            print("ğŸ”„ å¼€å§‹åˆ†æå‘ç°çš„å…³é”®è¯...")
                            result = manager.analyze_keywords(csv_path, args.output)
                            print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                            print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                else:
                    print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
                    
            except ImportError as e:
                print(f"âŒ å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·å¤±è´¥: {e}")
                print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
            except Exception as e:
                print(f"âŒ å¤šå¹³å°å…³é”®è¯å‘ç°å¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
        
        elif args.monitor_competitors and ENHANCED_FEATURES_AVAILABLE:
            # ç«å“ç›‘æ§
            sites = args.sites or ['canva.com', 'midjourney.com', 'openai.com']
            if not args.quiet:
                print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(sites)} ä¸ªç«å“ç½‘ç«™...")
            
            result = monitor_competitors(sites, args.output)
            print(f"âœ… ç«å“ç›‘æ§å®Œæˆ: åˆ†æäº† {len(result['competitors'])} ä¸ªç«å“")
            
            if not args.quiet:
                print("\nğŸ“Š ç›‘æ§ç»“æœæ‘˜è¦:")
                for comp in result['competitors'][:3]:
                    print(f"  â€¢ {comp['site']}: {comp['new_keywords_count']} ä¸ªæ–°å…³é”®è¯")
        
        elif args.predict_trends and ENHANCED_FEATURES_AVAILABLE:
            # è¶‹åŠ¿é¢„æµ‹
            if not args.quiet:
                print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {args.timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
            
            result = predict_keyword_trends(args.timeframe, args.output)
            print(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆ: é¢„æµ‹äº† {len(result['rising_keywords'])} ä¸ªä¸Šå‡å…³é”®è¯")
            
            if not args.quiet:
                print("\nğŸ“ˆ è¶‹åŠ¿é¢„æµ‹æ‘˜è¦:")
                for kw in result['rising_keywords'][:3]:
                    print(f"  ğŸ“ˆ {kw['keyword']}: {kw['predicted_growth']} (ç½®ä¿¡åº¦: {kw['confidence']:.0%})")
        
        elif args.seo_audit and ENHANCED_FEATURES_AVAILABLE:
            # SEOå®¡è®¡
            if not args.domain:
                print("âŒ è¯·æŒ‡å®šè¦å®¡è®¡çš„åŸŸå (--domain)")
                return
            
            if not args.quiet:
                print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {args.domain}")
            
            result = generate_seo_audit(args.domain, args.keywords)
            print(f"âœ… SEOå®¡è®¡å®Œæˆ: å‘ç° {len(result['keyword_opportunities'])} ä¸ªå…³é”®è¯æœºä¼š")
            
            if not args.quiet:
                print("\nğŸ¯ SEOä¼˜åŒ–å»ºè®®:")
                for gap in result['content_gaps'][:3]:
                    print(f"  â€¢ {gap}")
        
        elif args.build_websites and ENHANCED_FEATURES_AVAILABLE:
            # æ‰¹é‡å»ºç«™
            if not args.quiet:
                print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {args.top_keywords} ä¸ªç½‘ç«™...")
            
            result = batch_build_websites(args.top_keywords, args.output)
            print(f"âœ… æ‰¹é‡å»ºç«™å®Œæˆ: æˆåŠŸæ„å»º {result['successful_builds']} ä¸ªç½‘ç«™")
            
            if not args.quiet:
                print("\nğŸŒ æ„å»ºçš„ç½‘ç«™:")
                for site in result['websites'][:3]:
                    print(f"  â€¢ {site['keyword']}: {site['domain_suggestion']}")
        
        elif args.schedule and ENHANCED_FEATURES_AVAILABLE:
            # è®¾ç½®å®šæ—¶ä»»åŠ¡
            if not args.time or not args.action:
                print("âŒ è¯·æŒ‡å®šæ‰§è¡Œæ—¶é—´ (--time) å’ŒåŠ¨ä½œ (--action)")
                return
            
            setup_scheduler(args.schedule, args.time, args.action)
            
            if args.run_scheduler:
                run_scheduler()
        
        elif args.run_scheduler and ENHANCED_FEATURES_AVAILABLE:
            # ä»…è¿è¡Œè°ƒåº¦å™¨
            run_scheduler()
        
        elif args.report:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if not args.quiet:
                print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š...")
            
            report_path = manager.generate_daily_report(args.date)
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        else:
            # é»˜è®¤ï¼šä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
            if not args.quiet:
                print("ğŸŒ± å¼€å§‹ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
            
            result = manager.analyze_root_words(args.output)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ è¯æ ¹è¶‹åŠ¿åˆ†æå®Œæˆ! å…±åˆ†æ {result.get('total_root_words', 0)} ä¸ªè¯æ ¹")
                print(f"ğŸ“Š æˆåŠŸåˆ†æ: {result.get('successful_analyses', 0)} ä¸ª")
                print(f"ğŸ“ˆ ä¸Šå‡è¶‹åŠ¿è¯æ ¹: {len(result.get('top_trending_words', []))}")
                
                # æ˜¾ç¤ºTop 5è¯æ ¹
                top_words = result.get('top_trending_words', [])[:5]
                if top_words:
                    print("\nğŸ† Top 5 çƒ­é—¨è¯æ ¹:")
                    for i, word_data in enumerate(top_words, 1):
                        print(f"   {i}. {word_data['word']}: å¹³å‡å…´è¶£åº¦ {word_data['average_interest']:.1f}")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {args.output} ç›®å½•")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()