#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - ç»Ÿä¸€ä¸»å…¥å£æ–‡ä»¶
æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„å®Œæ•´æ‰§è¡Œå…¥å£
"""

import argparse
import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Any

def get_reports_dir() -> str:
    """ä»é…ç½®æ–‡ä»¶è·å–æŠ¥å‘Šè¾“å‡ºç›®å½•"""
    try:
        config_path = os.path.join(os.path.dirname(__file__), 'config/integrated_workflow_config.json')
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('output_settings', {}).get('reports_dir', 'output/reports')
    except:
        pass
    return 'output/reports'
from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery
from src.utils.enhanced_features import (
        monitor_competitors, predict_keyword_trends, generate_seo_audit,
        batch_build_websites
    )
# ç›´æ¥å¯¼å…¥éœ€æ±‚æŒ–æ˜ç®¡ç†å™¨ç»„ä»¶
from src.demand_mining.managers import KeywordManager, DiscoveryManager, TrendManager
from src.utils.logger import setup_logger

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


class IntegratedDemandMiningManager:
    """é›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨ - ç»Ÿä¸€æ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path
        self.logger = setup_logger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        self.keyword_manager = KeywordManager(config_path)
        self.discovery_manager = DiscoveryManager(config_path)
        self.trend_manager = TrendManager(config_path)
        
        # åˆå§‹åŒ–æ–°è¯æ£€æµ‹å™¨
        try:
            from src.demand_mining.analyzers.new_word_detector import NewWordDetector
            self.new_word_detector = NewWordDetector()
            self.new_word_detection_available = True
            print("âœ… æ–°è¯æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            self.new_word_detector = None
            self.new_word_detection_available = False
            print(f"âš ï¸ æ–°è¯æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        print("ğŸš€ é›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“Š å·²åŠ è½½å…³é”®è¯ç®¡ç†å™¨ã€å‘ç°ç®¡ç†å™¨ã€è¶‹åŠ¿ç®¡ç†å™¨")
        if self.new_word_detection_available:
            print("ğŸ” æ–°è¯æ£€æµ‹åŠŸèƒ½å·²å¯ç”¨")

    def analyze_keywords(self, input_file: str, output_dir: str = None, enable_serp: bool = False) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯æ–‡ä»¶ï¼ˆåŒ…å«æ–°è¯æ£€æµ‹å’Œå¯é€‰çš„SERPåˆ†æï¼‰"""
        # æ‰§è¡ŒåŸºç¡€å…³é”®è¯åˆ†æ
        result = self.keyword_manager.analyze(input_file, 'file', output_dir)

        # å¦‚æœå¯ç”¨SERPåˆ†æï¼Œæ‰§è¡ŒSERPåˆ†æ
        if enable_serp:
            try:
                print("ğŸ” æ­£åœ¨è¿›è¡ŒSERPåˆ†æ...")
                result = self._perform_serp_analysis(result, input_file)
                print("âœ… SERPåˆ†æå®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ SERPåˆ†æå¤±è´¥: {e}")
                result['serp_analysis_error'] = str(e)

        # æ·»åŠ æ–°è¯æ£€æµ‹
        if self.new_word_detection_available:
            try:
                print("ğŸ” æ­£åœ¨è¿›è¡Œæ–°è¯æ£€æµ‹...")

                # è¯»å–å…³é”®è¯æ•°æ®
                import pandas as pd
                df = pd.read_csv(input_file)

                # æ‰§è¡Œæ–°è¯æ£€æµ‹
                new_word_results = self.new_word_detector.detect_new_words(df)

                # å°†æ–°è¯æ£€æµ‹ç»“æœåˆå¹¶åˆ°åˆ†æç»“æœä¸­
                if 'keywords' in result:
                    for i, keyword_data in enumerate(result['keywords']):
                        if i < len(new_word_results):
                            # æ·»åŠ æ–°è¯æ£€æµ‹ä¿¡æ¯
                            row = new_word_results.iloc[i]
                            keyword_data['new_word_detection'] = {
                                'is_new_word': bool(row.get('is_new_word', False)),
                                'new_word_score': float(row.get('new_word_score', 0)),
                                'new_word_grade': str(row.get('new_word_grade', 'D')),
                                'growth_rate_7d': float(row.get('growth_rate_7d', 0)),
                                'confidence_level': str(row.get('confidence_level', 'low'))
                            }

                            # å¦‚æœæ˜¯æ–°è¯ï¼Œå¢åŠ æœºä¼šåˆ†æ•°åŠ æˆ
                            if row.get('is_new_word', False):
                                original_score = keyword_data.get('opportunity_score', 0)
                                new_word_bonus = row.get('new_word_score', 0) * 0.1  # 10%åŠ æˆ
                                keyword_data['opportunity_score'] = min(100, original_score + new_word_bonus)
                                keyword_data['new_word_bonus'] = new_word_bonus

                # ç”Ÿæˆæ–°è¯æ£€æµ‹æ‘˜è¦
                new_words_count = len(new_word_results[new_word_results['is_new_word'] == True])
                high_confidence_count = len(new_word_results[new_word_results['confidence_level'] == 'high'])

                result['new_word_summary'] = {
                    'total_analyzed': len(new_word_results),
                    'new_words_detected': new_words_count,
                    'high_confidence_new_words': high_confidence_count,
                    'new_word_percentage': round(new_words_count / len(new_word_results) * 100, 1) if len(new_word_results) > 0 else 0
                }

                print(f"âœ… æ–°è¯æ£€æµ‹å®Œæˆ: å‘ç° {new_words_count} ä¸ªæ–°è¯ ({high_confidence_count} ä¸ªé«˜ç½®ä¿¡åº¦)")

            except Exception as e:
                print(f"âš ï¸ æ–°è¯æ£€æµ‹å¤±è´¥: {e}")
                result['new_word_summary'] = {
                    'error': str(e),
                    'new_words_detected': 0
                }

        return result

    def _perform_serp_analysis(self, result: Dict[str, Any], input_file: str) -> Dict[str, Any]:
        """æ‰§è¡ŒSERPåˆ†æ"""
        try:
            from src.demand_mining.analyzers.serp_analyzer import SerpAnalyzer
            
            # åˆå§‹åŒ–SERPåˆ†æå™¨
            serp_analyzer = SerpAnalyzer()
            
            # è¯»å–å…³é”®è¯æ•°æ®
            import pandas as pd
            df = pd.read_csv(input_file)
            
            # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡ŒSERPåˆ†æ
            serp_results = []
            total_keywords = len(df)
            
            for i, row in df.iterrows():
                keyword = row.get('query', row.get('keyword', ''))
                if keyword:
                    print(f"  åˆ†æå…³é”®è¯ {i+1}/{total_keywords}: {keyword}")
                    serp_result = serp_analyzer.analyze_keyword_serp(keyword)
                    serp_results.append({
                        'keyword': keyword,
                        'serp_features': serp_result.get('serp_features', {}),
                        'serp_intent': serp_result.get('intent', 'I'),
                        'serp_confidence': serp_result.get('confidence', 0.0),
                        'serp_secondary_intent': serp_result.get('secondary_intent', None)
                    })
            
            # å°†SERPåˆ†æç»“æœåˆå¹¶åˆ°åŸç»“æœä¸­
            if 'keywords' in result and serp_results:
                for i, keyword_data in enumerate(result['keywords']):
                    if i < len(serp_results):
                        serp_data = serp_results[i]
                        keyword_data['serp_analysis'] = {
                            'features': serp_data['serp_features'],
                            'intent': serp_data['serp_intent'],
                            'confidence': serp_data['serp_confidence'],
                            'secondary_intent': serp_data['serp_secondary_intent']
                        }
                        
                        # å¦‚æœSERPåˆ†æç½®ä¿¡åº¦é«˜ï¼Œå¯ä»¥è°ƒæ•´æœºä¼šåˆ†æ•°
                        if serp_data['serp_confidence'] > 0.8:
                            original_score = keyword_data.get('opportunity_score', 0)
                            serp_bonus = serp_data['serp_confidence'] * 5  # æœ€å¤š5åˆ†åŠ æˆ
                            keyword_data['opportunity_score'] = min(100, original_score + serp_bonus)
                            keyword_data['serp_bonus'] = serp_bonus
            
            # ç”ŸæˆSERPåˆ†ææ‘˜è¦
            high_confidence_serp = len([r for r in serp_results if r['serp_confidence'] > 0.8])
            commercial_intent = len([r for r in serp_results if r['serp_intent'] in ['C', 'T']])
            
            result['serp_summary'] = {
                'total_analyzed': len(serp_results),
                'high_confidence_serp': high_confidence_serp,
                'commercial_intent_keywords': commercial_intent,
                'serp_analysis_enabled': True
            }
            
            return result
            
        except ImportError:
            print("âš ï¸ SERPåˆ†æå™¨æœªæ‰¾åˆ°ï¼Œè·³è¿‡SERPåˆ†æ")
            result['serp_summary'] = {
                'error': 'SERPåˆ†æå™¨æœªæ‰¾åˆ°',
                'serp_analysis_enabled': False
            }
            return result
        except Exception as e:
            print(f"âš ï¸ SERPåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            result['serp_summary'] = {
                'error': str(e),
                'serp_analysis_enabled': False
            }
            return result
    
    def analyze_root_words(self, output_dir: str = None) -> Dict[str, Any]:
        """åˆ†æè¯æ ¹è¶‹åŠ¿"""
        try:
            # ä½¿ç”¨å·²æœ‰çš„è¶‹åŠ¿ç®¡ç†å™¨ï¼Œé¿å…åˆ›å»ºæ–°çš„åˆ†æå™¨å®ä¾‹
            analyzer_output_dir = output_dir or f"{get_reports_dir()}/root_word_trends"
            
            # ç›´æ¥ä½¿ç”¨è¶‹åŠ¿ç®¡ç†å™¨è¿›è¡Œåˆ†æ
            results = self.trend_manager.analyze(
                'root_trends',
                timeframe="now 7-d",
                batch_size=5,
                output_dir=analyzer_output_dir
            )
            
            # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºç©º
            if results is None:
                results = {
                    'total_root_words': 0,
                    'summary': {
                        'successful_analyses': 0,
                        'failed_analyses': 0,
                        'top_trending_words': [],
                        'declining_words': [],
                        'stable_words': []
                    }
                }
            
            # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
            return {
                'total_root_words': results.get('total_root_words', 0),
                'successful_analyses': results.get('summary', {}).get('successful_analyses', 0),
                'failed_analyses': results.get('summary', {}).get('failed_analyses', 0),
                'top_trending_words': results.get('summary', {}).get('top_trending_words', []),
                'declining_words': results.get('summary', {}).get('declining_words', []),
                'stable_words': results.get('summary', {}).get('stable_words', []),
                'output_path': analyzer_output_dir
            }
            
        except Exception as e:
            self.logger.error(f"è¯æ ¹è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {
                'error': f'è¯æ ¹è¶‹åŠ¿åˆ†æå¤±è´¥: {e}',
                'total_root_words': 0,
                'successful_analyses': 0,
                'top_trending_words': []
            }
    
    
    def generate_daily_report(self, date: str = None) -> str:
        """ç”Ÿæˆæ—¥æŠ¥"""
        report_date = date or datetime.now().strftime("%Y-%m-%d")
        report_path = f"{get_reports_dir()}/daily_report_{report_date}.txt"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"éœ€æ±‚æŒ–æ˜æ—¥æŠ¥ - {report_date}\n")
                f.write("=" * 50 + "\n\n")
                
                # è·å–å„ç®¡ç†å™¨ç»Ÿè®¡
                stats = self.get_manager_stats()
                for manager_name, manager_stats in stats.items():
                    f.write(f"{manager_name}:\n")
                    if isinstance(manager_stats, dict):
                        for key, value in manager_stats.items():
                            f.write(f"  {key}: {value}\n")
                    else:
                        f.write(f"  çŠ¶æ€: {manager_stats}\n")
                    f.write("\n")
            
            return report_path
        except Exception as e:
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"

    def get_manager_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ç®¡ç†å™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'keyword_manager': self.keyword_manager.get_stats(),
            'discovery_manager': self.discovery_manager.get_discovery_stats(),
            'trend_manager': self.trend_manager.get_stats(),
        }


def print_quiet_summary(result):
    """é™é»˜æ¨¡å¼ä¸‹çš„ç®€è¦ç»“æœæ˜¾ç¤º"""
    print("\nğŸ¯ éœ€æ±‚æŒ–æ˜åˆ†æç»“æœæ‘˜è¦:")
    print(f"   â€¢ å…³é”®è¯æ€»æ•°: {result.get('total_keywords', 0)}")
    print(f"   â€¢ é«˜æœºä¼šå…³é”®è¯: {result.get('market_insights', {}).get('high_opportunity_count', 0)}")
    print(f"   â€¢ å¹³å‡æœºä¼šåˆ†æ•°: {result.get('market_insights', {}).get('avg_opportunity_score', 0)}")
    
    # æ˜¾ç¤ºTop 3å…³é”®è¯
    top_keywords = result.get('market_insights', {}).get('top_opportunities', [])[:3]
    if top_keywords:
        print("\nğŸ† Top 3 æœºä¼šå…³é”®è¯:")
        for i, kw in enumerate(top_keywords):
            intent_desc = kw.get('intent', {}).get('intent_description', 'æœªçŸ¥')
            score = kw.get('opportunity_score', 0)
            print(f"   {i+1}. {kw['keyword']} (æœºä¼šåˆ†æ•°: {score}, æ„å›¾: {intent_desc})")


def main():
    """ä¸»å‡½æ•° - æä¾›ç»Ÿä¸€çš„æ‰§è¡Œå…¥å£"""
    
    print("ğŸ” éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· v2.0")
    print("æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„æ™ºèƒ½åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - æ•´åˆå…­å¤§æŒ–æ˜æ–¹æ³•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ å…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•:
  1. åŸºäºè¯æ ¹å…³é”®è¯æ‹“å±• (52ä¸ªæ ¸å¿ƒè¯æ ¹)
  2. åŸºäºSEOå¤§ç«™æµé‡åˆ†æ (8ä¸ªç«å“ç½‘ç«™)
  3. æœç´¢å¼•æ“ä¸‹æ‹‰æ¨è
  4. å¾ªç¯æŒ–æ˜æ³•
  5. ä»˜è´¹å¹¿å‘Šå…³é”®è¯åˆ†æ
  6. æ”¶å…¥æ’è¡Œæ¦œåˆ†æ

ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹:
  # åˆ†æå…³é”®è¯æ–‡ä»¶
  python main.py --input data/keywords.csv
  
  # åˆ†æå…³é”®è¯æ–‡ä»¶å¹¶å¯ç”¨SERPåˆ†æ
  python main.py --input data/keywords.csv --serp
  
  # åˆ†æå•ä¸ªå…³é”®è¯
  python main.py --keywords "ai generator" "ai converter"
  
  # åˆ†æå•ä¸ªå…³é”®è¯å¹¶å¯ç”¨SERPåˆ†æ
  python main.py --keywords "AI" --serp
  
  # å¤šå¹³å°å…³é”®è¯å‘ç°
  python main.py --discover "AI image generator" "AI writing tool"
  
  # ä½¿ç”¨é»˜è®¤æœç´¢è¯è¿›è¡Œå¤šå¹³å°å‘ç°
  python main.py --discover default
  
  # ç”Ÿæˆåˆ†ææŠ¥å‘Š
  python main.py --report

  # ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
  python main.py --use-root-words

  # é™é»˜æ¨¡å¼åˆ†æ
  python main.py --input data/keywords.csv --quiet

ğŸš€ å¢å¼ºåŠŸèƒ½ç¤ºä¾‹:
  # ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–
  python main.py --monitor-competitors --sites canva.com midjourney.com

  # é¢„æµ‹å…³é”®è¯è¶‹åŠ¿
  python main.py --predict-trends --timeframe 30d

  # SEOå®¡è®¡
  python main.py --seo-audit --domain your-site.com --keywords "ai tool" "ai generator"

  # æ‰¹é‡ç”Ÿæˆç½‘ç«™
  python main.py --build-websites --top-keywords 5
        """
    )
    
    # è¾“å…¥æ–¹å¼é€‰æ‹© - ä¿®æ”¹ä¸ºéå¿…éœ€ï¼Œæ”¯æŒé»˜è®¤è¯æ ¹åˆ†æ
    input_group = parser.add_mutually_exclusive_group(required=False)
    input_group.add_argument('--input', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    input_group.add_argument('--keywords', nargs='+', help='ç›´æ¥è¾“å…¥å…³é”®è¯ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªï¼‰')
    input_group.add_argument('--discover', nargs='+', help='å¤šå¹³å°å…³é”®è¯å‘ç°ï¼ˆå¯æŒ‡å®šæœç´¢è¯æ±‡ï¼‰')
    input_group.add_argument('--report', action='store_true', help='ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š')
    
    # å¢å¼ºåŠŸèƒ½ç»„
    enhanced_group = parser.add_argument_group('å¢å¼ºåŠŸèƒ½')
    enhanced_group.add_argument('--monitor-competitors', action='store_true', help='ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–')
    enhanced_group.add_argument('--sites', nargs='+', help='ç«å“ç½‘ç«™åˆ—è¡¨')
    enhanced_group.add_argument('--predict-trends', action='store_true', help='é¢„æµ‹å…³é”®è¯è¶‹åŠ¿')
    enhanced_group.add_argument('--timeframe', default='30d', help='é¢„æµ‹æ—¶é—´èŒƒå›´')
    enhanced_group.add_argument('--seo-audit', action='store_true', help='ç”ŸæˆSEOä¼˜åŒ–å»ºè®®')
    enhanced_group.add_argument('--domain', help='è¦å®¡è®¡çš„åŸŸå')
    enhanced_group.add_argument('--build-websites', action='store_true', help='æ‰¹é‡ç”Ÿæˆç½‘ç«™')
    enhanced_group.add_argument('--top-keywords', type=int, default=10, help='ä½¿ç”¨å‰Nä¸ªå…³é”®è¯')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--output', default=get_reports_dir(), help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæœ€ç»ˆç»“æœ')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ¨¡å¼ï¼Œæ˜¾ç¤ºæ‰€æœ‰ä¸­é—´è¿‡ç¨‹')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--use-root-words', action='store_true', help='ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ')
    parser.add_argument('--serp', action='store_true', help='å¯ç”¨SERPåˆ†æåŠŸèƒ½')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºåˆ†æå‚æ•°
    if not args.quiet:
        if args.input:
            print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input}")
        elif args.keywords:
            print(f"ğŸ”¤ åˆ†æå…³é”®è¯: {', '.join(args.keywords)}")
        elif args.report:
            print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
        print("")
    
    try:
        # åˆ›å»ºé›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
        manager = IntegratedDemandMiningManager(args.config)
        
        # æ˜¾ç¤ºç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
        if args.stats:
            stats = manager.get_manager_stats()
            print("\nğŸ“Š ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯:")
            for manager_name, manager_stats in stats.items():
                if isinstance(manager_stats, dict):
                    print(f"\n{manager_name}:")
                    for key, value in manager_stats.items():
                        print(f"  {key}: {value}")
                else:
                    print(f"{manager_name}: {manager_stats}")
            return
        
        if args.input:
            # åˆ†æå…³é”®è¯æ–‡ä»¶
            if not args.quiet:
                print("ğŸš€ å¼€å§‹åˆ†æå…³é”®è¯æ–‡ä»¶...")
                if args.serp:
                    print("ğŸ” å·²å¯ç”¨SERPåˆ†æåŠŸèƒ½")
            
            result = manager.analyze_keywords(args.input, args.output, enable_serp=args.serp)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
                
                # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
                if 'new_word_summary' in result and result['new_word_summary'].get('new_words_detected', 0) > 0:
                    summary = result['new_word_summary']
                    print(f"ğŸ” æ–°è¯æ£€æµ‹: å‘ç° {summary['new_words_detected']} ä¸ªæ–°è¯ ({summary['new_word_percentage']}%)")
                    print(f"   é«˜ç½®ä¿¡åº¦æ–°è¯: {summary['high_confidence_new_words']} ä¸ª")

                # æ˜¾ç¤ºSERPåˆ†ææ‘˜è¦
                if 'serp_summary' in result and result['serp_summary'].get('serp_analysis_enabled', False):
                    serp_summary = result['serp_summary']
                    print(f"ğŸ” SERPåˆ†æ: åˆ†æäº† {serp_summary['total_analyzed']} ä¸ªå…³é”®è¯")
                    print(f"   é«˜ç½®ä¿¡åº¦SERP: {serp_summary['high_confidence_serp']} ä¸ª")
                    print(f"   å•†ä¸šæ„å›¾å…³é”®è¯: {serp_summary['commercial_intent_keywords']} ä¸ª")

                # æ˜¾ç¤ºTop 5å…³é”®è¯
                top_keywords = result['market_insights']['top_opportunities'][:5]
                if top_keywords:
                    print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                    for i, kw in enumerate(top_keywords, 1):
                        intent_desc = kw['intent']['intent_description']
                        score = kw['opportunity_score']
                        new_word_info = ""
                        if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                            new_word_grade = kw['new_word_detection']['new_word_grade']
                            new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                        print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
        
        elif args.keywords:
            # åˆ†æå•ä¸ªå…³é”®è¯
            if not args.quiet:
                print("ğŸš€ å¼€å§‹åˆ†æè¾“å…¥çš„å…³é”®è¯...")
            
            # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶
            import pandas as pd
            import tempfile
            
            temp_df = pd.DataFrame([{'query': kw} for kw in args.keywords])
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                temp_df.to_csv(f.name, index=False)
                temp_file = f.name
            
            try:
                result = manager.analyze_keywords(temp_file, args.output, enable_serp=args.serp)
                
                # æ˜¾ç¤ºç»“æœ
                if args.quiet:
                    print_quiet_summary(result)
                else:
                    print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {len(args.keywords)} ä¸ªå…³é”®è¯")
                    
                    # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„ç»“æœ
                    print("\nğŸ“‹ å…³é”®è¯åˆ†æç»“æœ:")
                    for kw_result in result['keywords']:
                        keyword = kw_result['keyword']
                        score = kw_result['opportunity_score']
                        intent = kw_result['intent']['intent_description']
                        print(f"   â€¢ {keyword}: æœºä¼šåˆ†æ•° {score}, æ„å›¾: {intent}")
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file)
        
        elif args.discover:
            # å¤šå¹³å°å…³é”®è¯å‘ç°
            search_terms = args.discover if args.discover != ['default'] else ['AI tool', 'AI generator', 'AI assistant']
            
            if not args.quiet:
                print("ğŸ” å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
                print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
            
            try:
                # å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·

                
                # åˆ›å»ºå‘ç°å·¥å…·
                discoverer = MultiPlatformKeywordDiscovery()
                
                # æ‰§è¡Œå‘ç°
                df = discoverer.discover_all_platforms(search_terms)


                if not df.empty:
                    # åˆ†æè¶‹åŠ¿
                    analysis = discoverer.analyze_keyword_trends(df)
                    
                    # ä¿å­˜ç»“æœ
                    output_dir = os.path.join(args.output, 'multi_platform_discovery')
                    csv_path, json_path = discoverer.save_results(df, analysis, output_dir)
                    
                    if args.quiet:
                        # é™é»˜æ¨¡å¼æ˜¾ç¤º
                        print(f"\nğŸ¯ å¤šå¹³å°å…³é”®è¯å‘ç°ç»“æœ:")
                        print(f"   â€¢ å‘ç°å…³é”®è¯: {analysis['total_keywords']} ä¸ª")
                        print(f"   â€¢ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                        
                        # æ˜¾ç¤ºTop 3å…³é”®è¯
                        top_keywords = analysis['top_keywords_by_score'][:3]
                        if top_keywords:
                            print("\nğŸ† Top 3 çƒ­é—¨å…³é”®è¯:")
                            for i, kw in enumerate(top_keywords, 1):
                                print(f"   {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
                    else:
                        # è¯¦ç»†æ¨¡å¼æ˜¾ç¤º
                        print(f"\nğŸ‰ å¤šå¹³å°å…³é”®è¯å‘ç°å®Œæˆ!")
                        print(f"ğŸ“Š å‘ç° {analysis['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸŒ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                        
                        print(f"\nğŸ† çƒ­é—¨å…³é”®è¯:")
                        for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                            print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
                    
                    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜:")
                    print(f"  CSV: {csv_path}")
                    print(f"  JSON: {json_path}")
                    
                    # è¯¢é—®æ˜¯å¦è¦ç«‹å³åˆ†æå‘ç°çš„å…³é”®è¯
                    if not args.quiet:
                        user_input = input("\nğŸ¤” æ˜¯å¦è¦ç«‹å³åˆ†æè¿™äº›å…³é”®è¯çš„æ„å›¾å’Œå¸‚åœºæœºä¼š? (y/n): ")
                        if user_input.lower() in ['y', 'yes', 'æ˜¯']:
                            print("ğŸ”„ å¼€å§‹åˆ†æå‘ç°çš„å…³é”®è¯...")
                            result = manager.analyze_keywords(csv_path, args.output)
                            print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                            print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                else:
                    print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
                    
            except ImportError as e:
                print(f"âŒ å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·å¤±è´¥: {e}")
                print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
            except Exception as e:
                print(f"âŒ å¤šå¹³å°å…³é”®è¯å‘ç°å¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
        
        elif args.monitor_competitors:
            # ç«å“ç›‘æ§
            sites = args.sites or ['canva.com', 'midjourney.com', 'openai.com']
            if not args.quiet:
                print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(sites)} ä¸ªç«å“ç½‘ç«™...")
            
            result = monitor_competitors(sites, args.output)
            print(f"âœ… ç«å“ç›‘æ§å®Œæˆ: åˆ†æäº† {len(result['competitors'])} ä¸ªç«å“")
            
            if not args.quiet:
                print("\nğŸ“Š ç›‘æ§ç»“æœæ‘˜è¦:")
                for comp in result['competitors'][:3]:
                    print(f"  â€¢ {comp['site']}: {comp['new_keywords_count']} ä¸ªæ–°å…³é”®è¯")
        
        elif args.predict_trends:
            # è¶‹åŠ¿é¢„æµ‹
            if not args.quiet:
                print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {args.timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
            
            result = predict_keyword_trends(args.timeframe, args.output)
            print(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆ: é¢„æµ‹äº† {len(result['rising_keywords'])} ä¸ªä¸Šå‡å…³é”®è¯")
            
            if not args.quiet:
                print("\nğŸ“ˆ è¶‹åŠ¿é¢„æµ‹æ‘˜è¦:")
                for kw in result['rising_keywords'][:3]:
                    print(f"  ğŸ“ˆ {kw['keyword']}: {kw['predicted_growth']} (ç½®ä¿¡åº¦: {kw['confidence']:.0%})")
        
        elif args.seo_audit:
            # SEOå®¡è®¡
            if not args.domain:
                print("âŒ è¯·æŒ‡å®šè¦å®¡è®¡çš„åŸŸå (--domain)")
                return
            
            if not args.quiet:
                print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {args.domain}")
            
            result = generate_seo_audit(args.domain, args.keywords)
            print(f"âœ… SEOå®¡è®¡å®Œæˆ: å‘ç° {len(result['keyword_opportunities'])} ä¸ªå…³é”®è¯æœºä¼š")
            
            if not args.quiet:
                print("\nğŸ¯ SEOä¼˜åŒ–å»ºè®®:")
                for gap in result['content_gaps'][:3]:
                    print(f"  â€¢ {gap}")
        
        elif args.build_websites:
            # æ‰¹é‡å»ºç«™
            if not args.quiet:
                print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {args.top_keywords} ä¸ªç½‘ç«™...")
            
            result = batch_build_websites(args.top_keywords, args.output)
            print(f"âœ… æ‰¹é‡å»ºç«™å®Œæˆ: æˆåŠŸæ„å»º {result['successful_builds']} ä¸ªç½‘ç«™")
            
            if not args.quiet:
                print("\nğŸŒ æ„å»ºçš„ç½‘ç«™:")
                for site in result['websites'][:3]:
                    print(f"  â€¢ {site['keyword']}: {site['domain_suggestion']}")

        elif args.report:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if not args.quiet:
                print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š...")
            
            report_path = manager.generate_daily_report()
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        elif args.use_root_words:
            # ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
            if not args.quiet:
                print("ğŸŒ± å¼€å§‹ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
            
            result = manager.analyze_root_words(args.output)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ è¯æ ¹è¶‹åŠ¿åˆ†æå®Œæˆ! å…±åˆ†æ {result.get('total_root_words', 0)} ä¸ªè¯æ ¹")
                print(f"ğŸ“Š æˆåŠŸåˆ†æ: {result.get('successful_analyses', 0)} ä¸ª")
                print(f"ğŸ“ˆ ä¸Šå‡è¶‹åŠ¿è¯æ ¹: {len(result.get('top_trending_words', []))}")
                
                # æ˜¾ç¤ºTop 5è¯æ ¹
                top_words = result.get('top_trending_words', [])[:5]
                if top_words:
                    print("\nğŸ† Top 5 çƒ­é—¨è¯æ ¹:")
                    for i, word_data in enumerate(top_words, 1):
                        print(f"   {i}. {word_data['word']}: å¹³å‡å…´è¶£åº¦ {word_data['average_interest']:.1f}")
        
        else:
            # é»˜è®¤ï¼šä½¿ç”¨ fetch_rising_queries è·å–å…³é”®è¯å¹¶è¿›è¡Œéœ€æ±‚æŒ–æ˜
            if not args.quiet:
                print("ğŸ”¥ æœªæŒ‡å®šå‚æ•°ï¼Œä½¿ç”¨ Rising Queries è·å–çƒ­é—¨å…³é”®è¯å¹¶è¿›è¡Œéœ€æ±‚æŒ–æ˜...")
            
            try:
                # å¯¼å…¥ TrendsCollector
                from src.collectors.trends_collector import TrendsCollector
                
                # åˆ›å»º TrendsCollector å®ä¾‹
                trends_collector = TrendsCollector()
                
                # ä½¿ç”¨ fetch_rising_queries è·å–çƒ­é—¨å…³é”®è¯
                if not args.quiet:
                    print("ğŸ” æ­£åœ¨è·å– Rising Queries...")
                
                rising_queries = trends_collector.fetch_rising_queries()
                
                # å°† rising queries è½¬æ¢ä¸ºDataFrameæ ¼å¼
                import pandas as pd
                if rising_queries and len(rising_queries) > 0:
                    # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                    if isinstance(rising_queries[0], str):
                        trending_df = pd.DataFrame([
                            {'query': query}
                            for query in rising_queries[:20]  # é™åˆ¶å‰20ä¸ª
                        ])
                    # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸åˆ—è¡¨
                    elif isinstance(rising_queries[0], dict):
                        trending_df = pd.DataFrame([
                            {
                                'query': item.get('query', item.get('keyword', str(item))),
                                'value': item.get('value', item.get('interest', 0))
                            }
                            for item in rising_queries[:20]  # é™åˆ¶å‰20ä¸ª
                        ])
                    else:
                        # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        trending_df = pd.DataFrame([
                            {'query': str(query)}
                            for query in rising_queries[:20]
                        ])
                else:
                    trending_df = pd.DataFrame(columns=['query'])

                if not trending_df.empty:
                    # ä¿å­˜çƒ­é—¨å…³é”®è¯åˆ°ä¸´æ—¶æ–‡ä»¶
                    import tempfile
                    from datetime import datetime
                    
                    # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
                    if 'query' not in trending_df.columns and len(trending_df.columns) > 0:
                        # å¦‚æœæ²¡æœ‰queryåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºå…³é”®è¯
                        trending_df = trending_df.rename(columns={trending_df.columns[0]: 'query'})
                    
                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                        trending_df.to_csv(f.name, index=False)
                        temp_file = f.name
                    
                    try:
                        if not args.quiet:
                            print(f"ğŸ” è·å–åˆ° {len(trending_df)} ä¸ª Rising Queriesï¼Œå¼€å§‹éœ€æ±‚æŒ–æ˜åˆ†æ...")
                        
                        # æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æï¼Œç¦ç”¨æ–°è¯æ£€æµ‹é¿å…429é”™è¯¯
                        manager.new_word_detection_available = False
                        result = manager.analyze_keywords(temp_file, args.output, enable_serp=False)
                        
                        # æ˜¾ç¤ºç»“æœ
                        if args.quiet:
                            print_quiet_summary(result)
                        else:
                            print(f"\nğŸ‰ éœ€æ±‚æŒ–æ˜åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ª Rising Queries")
                            print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                            print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
                            
                            # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
                            if 'new_word_summary' in result and result['new_word_summary'].get('new_words_detected', 0) > 0:
                                summary = result['new_word_summary']
                                print(f"ğŸ” æ–°è¯æ£€æµ‹: å‘ç° {summary['new_words_detected']} ä¸ªæ–°è¯ ({summary['new_word_percentage']}%)")
                                print(f"   é«˜ç½®ä¿¡åº¦æ–°è¯: {summary['high_confidence_new_words']} ä¸ª")

                            # æ˜¾ç¤ºTop 5æœºä¼šå…³é”®è¯
                            top_keywords = result['market_insights']['top_opportunities'][:5]
                            if top_keywords:
                                print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                                for i, kw in enumerate(top_keywords, 1):
                                    intent_desc = kw['intent']['intent_description']
                                    score = kw['opportunity_score']
                                    new_word_info = ""
                                    if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                                        new_word_grade = kw['new_word_detection']['new_word_grade']
                                        new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                                    print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
                            
                            # æ˜¾ç¤ºåŸå§‹Rising Queriesä¿¡æ¯
                            print(f"\nğŸ”¥ åŸå§‹ Rising Queries æ•°æ®:")
                            print(f"   â€¢ æ•°æ®æ¥æº: Google Trends Rising Queries")
                            if 'value' in trending_df.columns:
                                print(f"   â€¢ å¹³å‡çƒ­åº¦: {trending_df['value'].mean():.1f}")
                            
                            # ä¿å­˜åŸå§‹Rising Queries
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            trending_output_file = os.path.join(args.output, f"rising_queries_raw_{timestamp}.csv")
                            os.makedirs(args.output, exist_ok=True)
                            trending_df.to_csv(trending_output_file, index=False, encoding='utf-8')
                            print(f"ğŸ“ åŸå§‹ Rising Queries å·²ä¿å­˜åˆ°: {trending_output_file}")
                        
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        os.unlink(temp_file)
                        
                else:
                    # å½“æ— æ³•è·å–Rising Queriesæ—¶ï¼Œç›´æ¥æŠ¥å‘Šå¤±è´¥
                    print("âŒ æ— æ³•è·å– Rising Queriesï¼Œå¯èƒ½çš„åŸå› :")
                    print("ğŸ’¡ å»ºè®®:")
                    print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    print("   2. ç¨åé‡è¯•")
                    print("   3. æˆ–ä½¿ç”¨ --input å‚æ•°æŒ‡å®šå…³é”®è¯æ–‡ä»¶è¿›è¡Œåˆ†æ")
                    sys.exit(1)
                    
            except Exception as e:
                print(f"âŒ è·å– Rising Queries æˆ–éœ€æ±‚æŒ–æ˜æ—¶å‡ºé”™: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {args.output} ç›®å½•")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()