#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - ç»Ÿä¸€ä¸»å…¥å£æ–‡ä»¶
æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„å®Œæ•´æ‰§è¡Œå…¥å£
"""

import sys
import os
import time

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from src.cli_parser import setup_argument_parser, display_analysis_parameters, print_quiet_summary
from src.demand_mining_manager import IntegratedDemandMiningManager
from src.command_handlers import (
    handle_stats_display, handle_input_file_analysis, handle_keywords_analysis,
    handle_discover_analysis, handle_enhanced_features, handle_hot_keywords,
    handle_all_workflow, handle_demand_validation
)


def main():
    """ä¸»å‡½æ•° - æä¾›ç»Ÿä¸€çš„æ‰§è¡Œå…¥å£"""
    print("ğŸ” éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· v2.0")
    print("æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„æ™ºèƒ½åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    # æ˜¾ç¤ºåˆ†æå‚æ•°
    display_analysis_parameters(args)
    
    try:
        # åˆ›å»ºé›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
        manager = IntegratedDemandMiningManager(args.config)
        
        # æ˜¾ç¤ºç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
        if handle_stats_display(manager, args):
            return
        
        if handle_input_file_analysis(manager, args):
            return
        
        elif handle_keywords_analysis(manager, args):
            # å¦‚æœåŒæ—¶æŒ‡å®šäº†predict-trendsï¼Œåœ¨å…³é”®è¯åˆ†æå®Œæˆåæ‰§è¡Œè¶‹åŠ¿é¢„æµ‹
            if args.predict_trends:
                print("\nğŸ“ˆ å…³é”®è¯åˆ†æå®Œæˆï¼Œç°åœ¨æ‰§è¡Œè¶‹åŠ¿é¢„æµ‹...")
                time.sleep(3)  # æ·»åŠ é—´éš”é¿å…APIå†²çª
                handle_enhanced_features(manager, args)
            return
        
        elif handle_discover_analysis(manager, args):
            return
        
        elif handle_enhanced_features(manager, args):
            return

        elif args.report:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if not args.quiet:
                print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š...")
            
            report_path = manager.generate_daily_report()
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        elif args.use_root_words:
            # ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
            if not args.quiet:
                print("ğŸŒ± å¼€å§‹ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
            
            result = manager.analyze_root_words(args.output)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ è¯æ ¹è¶‹åŠ¿åˆ†æå®Œæˆ! å…±åˆ†æ {result.get('total_root_words', 0)} ä¸ªè¯æ ¹")
                print(f"ğŸ“Š æˆåŠŸåˆ†æ: {result.get('successful_analyses', 0)} ä¸ª")
                print(f"ğŸ“ˆ ä¸Šå‡è¶‹åŠ¿è¯æ ¹: {len(result.get('top_trending_words', []))}")
                
                # æ˜¾ç¤ºTop 5è¯æ ¹
                top_words = result.get('top_trending_words', [])[:5]
                if top_words:
                    print("\nğŸ† Top 5 çƒ­é—¨è¯æ ¹:")
                    for i, word_data in enumerate(top_words, 1):
                        print(f"   {i}. {word_data['word']}: å¹³å‡å…´è¶£åº¦ {word_data['average_interest']:.1f}")
        
        elif handle_hot_keywords(manager, args):
            return
        
        elif args.trending_keywords:
            # TrendingKeywords.net çƒ­é—¨å…³é”®è¯è·å–å’Œåˆ†æ
            if not args.quiet:
                print("ğŸ”¥ å¼€å§‹è·å– TrendingKeywords.net çƒ­é—¨å…³é”®è¯...")
            
            try:
                from src.collectors.trending_keywords_collector import TrendingKeywordsCollector
                import tempfile
                
                # åˆ›å»ºæ”¶é›†å™¨
                tk_collector = TrendingKeywordsCollector()
                
                # è·å–å…³é”®è¯
                tk_df = tk_collector.get_trending_keywords_for_analysis(max_keywords=30)
                
                if not tk_df.empty:
                    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶è¿›è¡Œåˆ†æ
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                        tk_df.to_csv(f.name, index=False)
                        temp_file = f.name
                    
                    try:
                        if not args.quiet:
                            print(f"ğŸ” è·å–åˆ° {len(tk_df)} ä¸ªå…³é”®è¯ï¼Œå¼€å§‹éœ€æ±‚æŒ–æ˜åˆ†æ...")
                        
                        # æ‰§è¡Œéœ€æ±‚æŒ–æ˜åˆ†æ
                        manager.new_word_detection_available = False
                        result = manager.analyze_keywords(temp_file, args.output, enable_serp=False)
                        
                        # æ˜¾ç¤ºç»“æœ
                        if args.quiet:
                            print_quiet_summary(result)
                        else:
                            print(f"\nğŸ‰ TrendingKeywords.net åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                            print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                            print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
                            
                            # æ˜¾ç¤ºTop 5æœºä¼šå…³é”®è¯
                            top_keywords = result['market_insights']['top_opportunities'][:5]
                            if top_keywords:
                                print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                                for i, kw in enumerate(top_keywords, 1):
                                    intent_desc = kw['intent']['intent_description']
                                    score = kw['opportunity_score']
                                    print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc})")
                        
                        # ä¿å­˜åŸå§‹æ•°æ®
                        from datetime import datetime
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        raw_output_file = os.path.join(args.output, f"trending_keywords_raw_{timestamp}.csv")
                        os.makedirs(args.output, exist_ok=True)
                        tk_df.to_csv(raw_output_file, index=False, encoding='utf-8')
                        print(f"ğŸ“ åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {raw_output_file}")
                        
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        os.unlink(temp_file)
                else:
                    print("âŒ æœªè·å–åˆ° TrendingKeywords.net æ•°æ®")
                    
            except Exception as e:
                print(f"âŒ TrendingKeywords.net åˆ†æå¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
            return
        
        elif handle_all_workflow(manager, args):
            return
        
        elif args.expand:
            # å¢å¼ºå…³é”®è¯æ‰©å±•
            if not args.quiet:
                print("ğŸš€ å¼€å§‹å¢å¼ºå…³é”®è¯æ‰©å±•...")
                print(f"ğŸŒ± ç§å­å…³é”®è¯: {', '.join(args.expand)}")
            
            result = manager.expand_keywords_comprehensive(
                args.expand, 
                output_dir=args.output,
                max_expanded=200,
                max_longtails=100
            )
            
            if 'error' in result:
                print(f"âŒ å…³é”®è¯æ‰©å±•å¤±è´¥: {result['error']}")
            else:
                if args.quiet:
                    print(f"ğŸ¯ å…³é”®è¯æ‰©å±•ç»“æœ:")
                    print(f"   â€¢ ç§å­å…³é”®è¯: {result['summary']['seed_count']}")
                    print(f"   â€¢ æ‰©å±•å…³é”®è¯: {result['summary']['expanded_count']}")
                    print(f"   â€¢ é•¿å°¾å…³é”®è¯: {result['summary']['longtail_count']}")
                    print(f"   â€¢ æ€»å…³é”®è¯æ•°: {result['summary']['total_count']}")
                    print(f"   â€¢ æ‰©å±•å€æ•°: {result['summary']['expansion_ratio']:.1f}x")
                    
                    # æ˜¾ç¤ºTop 5æ‰©å±•å…³é”®è¯
                    if result['expanded_keywords']:
                        print(f"ğŸ† Top 5 æ‰©å±•å…³é”®è¯:")
                        for i, kw in enumerate(result['expanded_keywords'][:5], 1):
                            print(f"   {i}. {kw}")
                    
                    # æ˜¾ç¤ºTop 5é•¿å°¾å…³é”®è¯
                    if result['longtail_keywords']:
                        print(f"ğŸ”— Top 5 é•¿å°¾å…³é”®è¯:")
                        for i, kw in enumerate(result['longtail_keywords'][:5], 1):
                            print(f"   {i}. {kw}")
                else:
                    print(f"ğŸ‰ å¢å¼ºå…³é”®è¯æ‰©å±•å®Œæˆ!")
                    print(f"ğŸ“Š æ‰©å±•ç»Ÿè®¡:")
                    for key, value in result['summary'].items():
                        print(f"   {key}: {value}")
                    
                    if 'output_files' in result:
                        print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
                        for file_type, file_path in result['output_files'].items():
                            print(f"   {file_type}: {file_path}")
        
        elif args.demand_validation:
            handle_demand_validation(manager, args)
        
        else:
            # æ— å‚æ•°æ—¶æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            parser.print_help()
            return
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {args.output} ç›®å½•")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()