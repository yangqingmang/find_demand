#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - ç»Ÿä¸€ä¸»å…¥å£æ–‡ä»¶
æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„å®Œæ•´æ‰§è¡Œå…¥å£
"""

import argparse
import sys
import os
from datetime import datetime
from typing import Dict, List, Any
from src.demand_mining.tools.multi_platform_keyword_discovery import MultiPlatformKeywordDiscovery
from src.utils.enhanced_features import (
        monitor_competitors, predict_keyword_trends, generate_seo_audit,
        batch_build_websites
    )
# ç›´æ¥å¯¼å…¥éœ€æ±‚æŒ–æ˜ç®¡ç†å™¨ç»„ä»¶
from src.demand_mining.managers import KeywordManager, DiscoveryManager, TrendManager
from src.utils.logger import setup_logger

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


class IntegratedDemandMiningManager:
    """é›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨ - ç»Ÿä¸€æ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path
        self.logger = setup_logger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        self.keyword_manager = KeywordManager(config_path)
        self.discovery_manager = DiscoveryManager(config_path)
        self.trend_manager = TrendManager(config_path)
        
        # åˆå§‹åŒ–æ–°è¯æ£€æµ‹å™¨
        try:
            from src.demand_mining.analyzers.new_word_detector import NewWordDetector
            self.new_word_detector = NewWordDetector()
            self.new_word_detection_available = True
            print("âœ… æ–°è¯æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            self.new_word_detector = None
            self.new_word_detection_available = False
            print(f"âš ï¸ æ–°è¯æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        print("ğŸš€ é›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“Š å·²åŠ è½½å…³é”®è¯ç®¡ç†å™¨ã€å‘ç°ç®¡ç†å™¨ã€è¶‹åŠ¿ç®¡ç†å™¨")
        if self.new_word_detection_available:
            print("ğŸ” æ–°è¯æ£€æµ‹åŠŸèƒ½å·²å¯ç”¨")

    def analyze_keywords(self, input_file: str, output_dir: str = None) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯æ–‡ä»¶ï¼ˆåŒ…å«æ–°è¯æ£€æµ‹ï¼‰"""
        # æ‰§è¡ŒåŸºç¡€å…³é”®è¯åˆ†æ
        result = self.keyword_manager.analyze(input_file, 'file', output_dir)

        # æ·»åŠ æ–°è¯æ£€æµ‹
        if self.new_word_detection_available:
            try:
                print("ğŸ” æ­£åœ¨è¿›è¡Œæ–°è¯æ£€æµ‹...")

                # è¯»å–å…³é”®è¯æ•°æ®
                import pandas as pd
                df = pd.read_csv(input_file)

                # æ‰§è¡Œæ–°è¯æ£€æµ‹
                new_word_results = self.new_word_detector.detect_new_words(df)

                # å°†æ–°è¯æ£€æµ‹ç»“æœåˆå¹¶åˆ°åˆ†æç»“æœä¸­
                if 'keywords' in result:
                    for i, keyword_data in enumerate(result['keywords']):
                        if i < len(new_word_results):
                            # æ·»åŠ æ–°è¯æ£€æµ‹ä¿¡æ¯
                            row = new_word_results.iloc[i]
                            keyword_data['new_word_detection'] = {
                                'is_new_word': bool(row.get('is_new_word', False)),
                                'new_word_score': float(row.get('new_word_score', 0)),
                                'new_word_grade': str(row.get('new_word_grade', 'D')),
                                'growth_rate_7d': float(row.get('growth_rate_7d', 0)),
                                'confidence_level': str(row.get('confidence_level', 'low'))
                            }

                            # å¦‚æœæ˜¯æ–°è¯ï¼Œå¢åŠ æœºä¼šåˆ†æ•°åŠ æˆ
                            if row.get('is_new_word', False):
                                original_score = keyword_data.get('opportunity_score', 0)
                                new_word_bonus = row.get('new_word_score', 0) * 0.1  # 10%åŠ æˆ
                                keyword_data['opportunity_score'] = min(100, original_score + new_word_bonus)
                                keyword_data['new_word_bonus'] = new_word_bonus

                # ç”Ÿæˆæ–°è¯æ£€æµ‹æ‘˜è¦
                new_words_count = len(new_word_results[new_word_results['is_new_word'] == True])
                high_confidence_count = len(new_word_results[new_word_results['confidence_level'] == 'high'])

                result['new_word_summary'] = {
                    'total_analyzed': len(new_word_results),
                    'new_words_detected': new_words_count,
                    'high_confidence_new_words': high_confidence_count,
                    'new_word_percentage': round(new_words_count / len(new_word_results) * 100, 1) if len(new_word_results) > 0 else 0
                }

                print(f"âœ… æ–°è¯æ£€æµ‹å®Œæˆ: å‘ç° {new_words_count} ä¸ªæ–°è¯ ({high_confidence_count} ä¸ªé«˜ç½®ä¿¡åº¦)")

            except Exception as e:
                print(f"âš ï¸ æ–°è¯æ£€æµ‹å¤±è´¥: {e}")
                result['new_word_summary'] = {
                    'error': str(e),
                    'new_words_detected': 0
                }

        return result
    
    def analyze_root_words(self, output_dir: str = None) -> Dict[str, Any]:
        """åˆ†æè¯æ ¹è¶‹åŠ¿"""
        try:
            from src.demand_mining.root_word_trends_analyzer import RootWordTrendsAnalyzer
            
            analyzer_output_dir = output_dir or "src/demand_mining/reports/root_word_trends"
            analyzer = RootWordTrendsAnalyzer(analyzer_output_dir)
            
            # æ‰§è¡Œåˆ†æ
            results = analyzer.analyze_all_root_words(timeframe="now 7-d", batch_size=5)
            
            # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
            return {
                'total_root_words': results.get('total_root_words', 0),
                'successful_analyses': results['summary'].get('successful_analyses', 0),
                'failed_analyses': results['summary'].get('failed_analyses', 0),
                'top_trending_words': results['summary'].get('top_trending_words', []),
                'declining_words': results['summary'].get('declining_words', []),
                'stable_words': results['summary'].get('stable_words', []),
                'output_path': analyzer_output_dir
            }
            
        except Exception as e:
            self.logger.error(f"è¯æ ¹è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {
                'error': f'è¯æ ¹è¶‹åŠ¿åˆ†æå¤±è´¥: {e}',
                'total_root_words': 0,
                'successful_analyses': 0,
                'top_trending_words': []
            }
    
    def discover_keywords(self, search_terms: List[str], output_dir: str = None) -> Dict[str, Any]:
        """å¤šå¹³å°å…³é”®è¯å‘ç°"""
        return self.discovery_manager.analyze(search_terms, output_dir)
    
    def run_unified_analysis(self, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œç»Ÿä¸€åˆ†ææµç¨‹"""
        analysis_type = kwargs.get('analysis_type', 'keywords')
        
        if analysis_type == 'keywords':
            return self._run_keyword_analysis(**kwargs)
        elif analysis_type == 'discovery':
            return self._run_keyword_discovery(**kwargs)
        elif analysis_type == 'root_trends':
            return self._run_root_trends_analysis(**kwargs)
        elif analysis_type == 'competitor':
            return self._run_competitor_analysis(**kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
    
    def _run_keyword_analysis(self, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå…³é”®è¯åˆ†æ"""
        input_file = kwargs.get('input_file')
        keywords = kwargs.get('keywords')
        output_dir = kwargs.get('output_dir')
        
        if input_file:
            return self.keyword_manager.analyze(input_file, 'file', output_dir)
        elif keywords:
            return self.keyword_manager.analyze(keywords, 'keywords', output_dir)
        else:
            raise ValueError("è¯·æä¾›è¾“å…¥æ–‡ä»¶æˆ–å…³é”®è¯åˆ—è¡¨")
    
    def _run_keyword_discovery(self, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå…³é”®è¯å‘ç°"""
        search_terms = kwargs.get('search_terms', ['AI tool', 'AI generator'])
        output_dir = kwargs.get('output_dir')
        
        return self.discovery_manager.analyze(search_terms, output_dir)
    
    def _run_root_trends_analysis(self, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œè¯æ ¹è¶‹åŠ¿åˆ†æ"""
        output_dir = kwargs.get('output_dir')
        from src.utils.constants import GOOGLE_TRENDS_CONFIG
        timeframe = kwargs.get('timeframe', GOOGLE_TRENDS_CONFIG['default_timeframe'].replace('today ', ''))
        batch_size = kwargs.get('batch_size', 5)
        
        return self.trend_manager.analyze(
            'root_trends',
            timeframe=timeframe,
            batch_size=batch_size,
            output_dir=output_dir
        )
    
    @staticmethod
    def _run_competitor_analysis(**kwargs) -> Dict[str, Any]:
        """è¿è¡Œç«å“åˆ†æ"""
        
        try:
            sites = kwargs.get('sites', ['canva.com', 'midjourney.com'])
            output_dir = kwargs.get('output_dir')
            
            return monitor_competitors(sites, output_dir)
        except Exception as e:
            return {'error': f'ç«å“åˆ†æå¤±è´¥: {e}'}
    
    def generate_daily_report(self, date: str = None) -> str:
        """ç”Ÿæˆæ—¥æŠ¥"""
        report_date = date or datetime.now().strftime("%Y-%m-%d")
        report_path = f"src/demand_mining/reports/daily_report_{report_date}.txt"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"éœ€æ±‚æŒ–æ˜æ—¥æŠ¥ - {report_date}\n")
                f.write("=" * 50 + "\n\n")
                
                # è·å–å„ç®¡ç†å™¨ç»Ÿè®¡
                stats = self.get_manager_stats()
                for manager_name, manager_stats in stats.items():
                    f.write(f"{manager_name}:\n")
                    if isinstance(manager_stats, dict):
                        for key, value in manager_stats.items():
                            f.write(f"  {key}: {value}\n")
                    else:
                        f.write(f"  çŠ¶æ€: {manager_stats}\n")
                    f.write("\n")
            
            return report_path
        except Exception as e:
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"

    def get_manager_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ç®¡ç†å™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'keyword_manager': self.keyword_manager.get_stats(),
            'discovery_manager': self.discovery_manager.get_discovery_stats(),
            'trend_manager': self.trend_manager.get_stats(),
        }


def print_quiet_summary(result):
    """é™é»˜æ¨¡å¼ä¸‹çš„ç®€è¦ç»“æœæ˜¾ç¤º"""
    print("\nğŸ¯ éœ€æ±‚æŒ–æ˜åˆ†æç»“æœæ‘˜è¦:")
    print(f"   â€¢ å…³é”®è¯æ€»æ•°: {result.get('total_keywords', 0)}")
    print(f"   â€¢ é«˜æœºä¼šå…³é”®è¯: {result.get('market_insights', {}).get('high_opportunity_count', 0)}")
    print(f"   â€¢ å¹³å‡æœºä¼šåˆ†æ•°: {result.get('market_insights', {}).get('avg_opportunity_score', 0)}")
    
    # æ˜¾ç¤ºTop 3å…³é”®è¯
    top_keywords = result.get('market_insights', {}).get('top_opportunities', [])[:3]
    if top_keywords:
        print("\nğŸ† Top 3 æœºä¼šå…³é”®è¯:")
        for i, kw in enumerate(top_keywords):
            intent_desc = kw.get('intent', {}).get('intent_description', 'æœªçŸ¥')
            score = kw.get('opportunity_score', 0)
            print(f"   {i+1}. {kw['keyword']} (æœºä¼šåˆ†æ•°: {score}, æ„å›¾: {intent_desc})")


def main():
    """ä¸»å‡½æ•° - æä¾›ç»Ÿä¸€çš„æ‰§è¡Œå…¥å£"""
    
    print("ğŸ” éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· v2.0")
    print("æ•´åˆå…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•çš„æ™ºèƒ½åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='éœ€æ±‚æŒ–æ˜åˆ†æå·¥å…· - æ•´åˆå…­å¤§æŒ–æ˜æ–¹æ³•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ å…­å¤§éœ€æ±‚æŒ–æ˜æ–¹æ³•:
  1. åŸºäºè¯æ ¹å…³é”®è¯æ‹“å±• (52ä¸ªæ ¸å¿ƒè¯æ ¹)
  2. åŸºäºSEOå¤§ç«™æµé‡åˆ†æ (8ä¸ªç«å“ç½‘ç«™)
  3. æœç´¢å¼•æ“ä¸‹æ‹‰æ¨è
  4. å¾ªç¯æŒ–æ˜æ³•
  5. ä»˜è´¹å¹¿å‘Šå…³é”®è¯åˆ†æ
  6. æ”¶å…¥æ’è¡Œæ¦œåˆ†æ

ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹:
  # åˆ†æå…³é”®è¯æ–‡ä»¶
  python main.py --input data/keywords.csv
  
  # åˆ†æå•ä¸ªå…³é”®è¯
  python main.py --keywords "ai generator" "ai converter"
  
  # å¤šå¹³å°å…³é”®è¯å‘ç°
  python main.py --discover "AI image generator" "AI writing tool"
  
  # ä½¿ç”¨é»˜è®¤æœç´¢è¯è¿›è¡Œå¤šå¹³å°å‘ç°
  python main.py --discover default
  
  # ç”Ÿæˆåˆ†ææŠ¥å‘Š
  python main.py --report

  # ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
  python main.py --use-root-words

  # é™é»˜æ¨¡å¼åˆ†æ
  python main.py --input data/keywords.csv --quiet

ğŸš€ å¢å¼ºåŠŸèƒ½ç¤ºä¾‹:
  # ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–
  python main.py --monitor-competitors --sites canva.com midjourney.com

  # é¢„æµ‹å…³é”®è¯è¶‹åŠ¿
  python main.py --predict-trends --timeframe 30d

  # SEOå®¡è®¡
  python main.py --seo-audit --domain your-site.com --keywords "ai tool" "ai generator"

  # æ‰¹é‡ç”Ÿæˆç½‘ç«™
  python main.py --build-websites --top-keywords 5
        """
    )
    
    # è¾“å…¥æ–¹å¼é€‰æ‹© - ä¿®æ”¹ä¸ºéå¿…éœ€ï¼Œæ”¯æŒé»˜è®¤è¯æ ¹åˆ†æ
    input_group = parser.add_mutually_exclusive_group(required=False)
    input_group.add_argument('--input', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    input_group.add_argument('--keywords', nargs='+', help='ç›´æ¥è¾“å…¥å…³é”®è¯ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªï¼‰')
    input_group.add_argument('--discover', nargs='+', help='å¤šå¹³å°å…³é”®è¯å‘ç°ï¼ˆå¯æŒ‡å®šæœç´¢è¯æ±‡ï¼‰')
    input_group.add_argument('--report', action='store_true', help='ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š')
    
    # å¢å¼ºåŠŸèƒ½ç»„
    enhanced_group = parser.add_argument_group('å¢å¼ºåŠŸèƒ½')
    enhanced_group.add_argument('--monitor-competitors', action='store_true', help='ç›‘æ§ç«å“å…³é”®è¯å˜åŒ–')
    enhanced_group.add_argument('--sites', nargs='+', help='ç«å“ç½‘ç«™åˆ—è¡¨')
    enhanced_group.add_argument('--predict-trends', action='store_true', help='é¢„æµ‹å…³é”®è¯è¶‹åŠ¿')
    enhanced_group.add_argument('--timeframe', default='30d', help='é¢„æµ‹æ—¶é—´èŒƒå›´')
    enhanced_group.add_argument('--seo-audit', action='store_true', help='ç”ŸæˆSEOä¼˜åŒ–å»ºè®®')
    enhanced_group.add_argument('--domain', help='è¦å®¡è®¡çš„åŸŸå')
    enhanced_group.add_argument('--build-websites', action='store_true', help='æ‰¹é‡ç”Ÿæˆç½‘ç«™')
    enhanced_group.add_argument('--top-keywords', type=int, default=10, help='ä½¿ç”¨å‰Nä¸ªå…³é”®è¯')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--output', default='src/demand_mining/reports', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæœ€ç»ˆç»“æœ')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ¨¡å¼ï¼Œæ˜¾ç¤ºæ‰€æœ‰ä¸­é—´è¿‡ç¨‹')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--use-root-words', action='store_true', help='ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºåˆ†æå‚æ•°
    if not args.quiet:
        if args.input:
            print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input}")
        elif args.keywords:
            print(f"ğŸ”¤ åˆ†æå…³é”®è¯: {', '.join(args.keywords)}")
        elif args.report:
            print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
        print("")
    
    try:
        # åˆ›å»ºé›†æˆéœ€æ±‚æŒ–æ˜ç®¡ç†å™¨
        manager = IntegratedDemandMiningManager(args.config)
        
        # æ˜¾ç¤ºç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
        if args.stats:
            stats = manager.get_manager_stats()
            print("\nğŸ“Š ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯:")
            for manager_name, manager_stats in stats.items():
                if isinstance(manager_stats, dict):
                    print(f"\n{manager_name}:")
                    for key, value in manager_stats.items():
                        print(f"  {key}: {value}")
                else:
                    print(f"{manager_name}: {manager_stats}")
            return
        
        if args.input:
            # åˆ†æå…³é”®è¯æ–‡ä»¶
            if not args.quiet:
                print("ğŸš€ å¼€å§‹åˆ†æå…³é”®è¯æ–‡ä»¶...")
            
            result = manager.analyze_keywords(args.input, args.output)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                print(f"ğŸ“ˆ å¹³å‡æœºä¼šåˆ†æ•°: {result['market_insights']['avg_opportunity_score']}")
                
                # æ˜¾ç¤ºæ–°è¯æ£€æµ‹æ‘˜è¦
                if 'new_word_summary' in result and result['new_word_summary'].get('new_words_detected', 0) > 0:
                    summary = result['new_word_summary']
                    print(f"ğŸ” æ–°è¯æ£€æµ‹: å‘ç° {summary['new_words_detected']} ä¸ªæ–°è¯ ({summary['new_word_percentage']}%)")
                    print(f"   é«˜ç½®ä¿¡åº¦æ–°è¯: {summary['high_confidence_new_words']} ä¸ª")

                # æ˜¾ç¤ºTop 5å…³é”®è¯
                top_keywords = result['market_insights']['top_opportunities'][:5]
                if top_keywords:
                    print("\nğŸ† Top 5 æœºä¼šå…³é”®è¯:")
                    for i, kw in enumerate(top_keywords, 1):
                        intent_desc = kw['intent']['intent_description']
                        score = kw['opportunity_score']
                        new_word_info = ""
                        if 'new_word_detection' in kw and kw['new_word_detection']['is_new_word']:
                            new_word_grade = kw['new_word_detection']['new_word_grade']
                            new_word_info = f" [æ–°è¯-{new_word_grade}çº§]"
                        print(f"   {i}. {kw['keyword']} (åˆ†æ•°: {score}, æ„å›¾: {intent_desc}){new_word_info}")
        
        elif args.keywords:
            # åˆ†æå•ä¸ªå…³é”®è¯
            if not args.quiet:
                print("ğŸš€ å¼€å§‹åˆ†æè¾“å…¥çš„å…³é”®è¯...")
            
            # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶
            import pandas as pd
            import tempfile
            
            temp_df = pd.DataFrame([{'query': kw} for kw in args.keywords])
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as f:
                temp_df.to_csv(f.name, index=False)
                temp_file = f.name
            
            try:
                result = manager.analyze_keywords(temp_file, args.output)
                
                # æ˜¾ç¤ºç»“æœ
                if args.quiet:
                    print_quiet_summary(result)
                else:
                    print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±åˆ†æ {len(args.keywords)} ä¸ªå…³é”®è¯")
                    
                    # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„ç»“æœ
                    print("\nğŸ“‹ å…³é”®è¯åˆ†æç»“æœ:")
                    for kw_result in result['keywords']:
                        keyword = kw_result['keyword']
                        score = kw_result['opportunity_score']
                        intent = kw_result['intent']['intent_description']
                        print(f"   â€¢ {keyword}: æœºä¼šåˆ†æ•° {score}, æ„å›¾: {intent}")
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file)
        
        elif args.discover:
            # å¤šå¹³å°å…³é”®è¯å‘ç°
            search_terms = args.discover if args.discover != ['default'] else ['AI tool', 'AI generator', 'AI assistant']
            
            if not args.quiet:
                print("ğŸ” å¼€å§‹å¤šå¹³å°å…³é”®è¯å‘ç°...")
                print(f"ğŸ“Š æœç´¢è¯æ±‡: {', '.join(search_terms)}")
            
            try:
                # å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·

                
                # åˆ›å»ºå‘ç°å·¥å…·
                discoverer = MultiPlatformKeywordDiscovery()
                
                # æ‰§è¡Œå‘ç°
                df = discoverer.discover_all_platforms(search_terms)


                if not df.empty:
                    # åˆ†æè¶‹åŠ¿
                    analysis = discoverer.analyze_keyword_trends(df)
                    
                    # ä¿å­˜ç»“æœ
                    output_dir = os.path.join(args.output, 'multi_platform_discovery')
                    csv_path, json_path = discoverer.save_results(df, analysis, output_dir)
                    
                    if args.quiet:
                        # é™é»˜æ¨¡å¼æ˜¾ç¤º
                        print(f"\nğŸ¯ å¤šå¹³å°å…³é”®è¯å‘ç°ç»“æœ:")
                        print(f"   â€¢ å‘ç°å…³é”®è¯: {analysis['total_keywords']} ä¸ª")
                        print(f"   â€¢ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                        
                        # æ˜¾ç¤ºTop 3å…³é”®è¯
                        top_keywords = analysis['top_keywords_by_score'][:3]
                        if top_keywords:
                            print("\nğŸ† Top 3 çƒ­é—¨å…³é”®è¯:")
                            for i, kw in enumerate(top_keywords, 1):
                                print(f"   {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
                    else:
                        # è¯¦ç»†æ¨¡å¼æ˜¾ç¤º
                        print(f"\nğŸ‰ å¤šå¹³å°å…³é”®è¯å‘ç°å®Œæˆ!")
                        print(f"ğŸ“Š å‘ç° {analysis['total_keywords']} ä¸ªå…³é”®è¯")
                        print(f"ğŸŒ å¹³å°åˆ†å¸ƒ: {analysis['platform_distribution']}")
                        
                        print(f"\nğŸ† çƒ­é—¨å…³é”®è¯:")
                        for i, kw in enumerate(analysis['top_keywords_by_score'][:5], 1):
                            print(f"  {i}. {kw['keyword']} (è¯„åˆ†: {kw['score']}, æ¥æº: {kw['platform']})")
                    
                    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜:")
                    print(f"  CSV: {csv_path}")
                    print(f"  JSON: {json_path}")
                    
                    # è¯¢é—®æ˜¯å¦è¦ç«‹å³åˆ†æå‘ç°çš„å…³é”®è¯
                    if not args.quiet:
                        user_input = input("\nğŸ¤” æ˜¯å¦è¦ç«‹å³åˆ†æè¿™äº›å…³é”®è¯çš„æ„å›¾å’Œå¸‚åœºæœºä¼š? (y/n): ")
                        if user_input.lower() in ['y', 'yes', 'æ˜¯']:
                            print("ğŸ”„ å¼€å§‹åˆ†æå‘ç°çš„å…³é”®è¯...")
                            result = manager.analyze_keywords(csv_path, args.output)
                            print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆ! å…±åˆ†æ {result['total_keywords']} ä¸ªå…³é”®è¯")
                            print(f"ğŸ“Š é«˜æœºä¼šå…³é”®è¯: {result['market_insights']['high_opportunity_count']} ä¸ª")
                else:
                    print("âš ï¸ æœªå‘ç°ä»»ä½•å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å‚æ•°")
                    
            except ImportError as e:
                print(f"âŒ å¯¼å…¥å¤šå¹³å°å‘ç°å·¥å…·å¤±è´¥: {e}")
                print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
            except Exception as e:
                print(f"âŒ å¤šå¹³å°å…³é”®è¯å‘ç°å¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
        
        elif args.monitor_competitors:
            # ç«å“ç›‘æ§
            sites = args.sites or ['canva.com', 'midjourney.com', 'openai.com']
            if not args.quiet:
                print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(sites)} ä¸ªç«å“ç½‘ç«™...")
            
            result = monitor_competitors(sites, args.output)
            print(f"âœ… ç«å“ç›‘æ§å®Œæˆ: åˆ†æäº† {len(result['competitors'])} ä¸ªç«å“")
            
            if not args.quiet:
                print("\nğŸ“Š ç›‘æ§ç»“æœæ‘˜è¦:")
                for comp in result['competitors'][:3]:
                    print(f"  â€¢ {comp['site']}: {comp['new_keywords_count']} ä¸ªæ–°å…³é”®è¯")
        
        elif args.predict_trends:
            # è¶‹åŠ¿é¢„æµ‹
            if not args.quiet:
                print(f"ğŸ“ˆ å¼€å§‹é¢„æµ‹æœªæ¥ {args.timeframe} çš„å…³é”®è¯è¶‹åŠ¿...")
            
            result = predict_keyword_trends(args.timeframe, args.output)
            print(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆ: é¢„æµ‹äº† {len(result['rising_keywords'])} ä¸ªä¸Šå‡å…³é”®è¯")
            
            if not args.quiet:
                print("\nğŸ“ˆ è¶‹åŠ¿é¢„æµ‹æ‘˜è¦:")
                for kw in result['rising_keywords'][:3]:
                    print(f"  ğŸ“ˆ {kw['keyword']}: {kw['predicted_growth']} (ç½®ä¿¡åº¦: {kw['confidence']:.0%})")
        
        elif args.seo_audit:
            # SEOå®¡è®¡
            if not args.domain:
                print("âŒ è¯·æŒ‡å®šè¦å®¡è®¡çš„åŸŸå (--domain)")
                return
            
            if not args.quiet:
                print(f"ğŸ” å¼€å§‹SEOå®¡è®¡: {args.domain}")
            
            result = generate_seo_audit(args.domain, args.keywords)
            print(f"âœ… SEOå®¡è®¡å®Œæˆ: å‘ç° {len(result['keyword_opportunities'])} ä¸ªå…³é”®è¯æœºä¼š")
            
            if not args.quiet:
                print("\nğŸ¯ SEOä¼˜åŒ–å»ºè®®:")
                for gap in result['content_gaps'][:3]:
                    print(f"  â€¢ {gap}")
        
        elif args.build_websites:
            # æ‰¹é‡å»ºç«™
            if not args.quiet:
                print(f"ğŸ—ï¸ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {args.top_keywords} ä¸ªç½‘ç«™...")
            
            result = batch_build_websites(args.top_keywords, args.output)
            print(f"âœ… æ‰¹é‡å»ºç«™å®Œæˆ: æˆåŠŸæ„å»º {result['successful_builds']} ä¸ªç½‘ç«™")
            
            if not args.quiet:
                print("\nğŸŒ æ„å»ºçš„ç½‘ç«™:")
                for site in result['websites'][:3]:
                    print(f"  â€¢ {site['keyword']}: {site['domain_suggestion']}")

        elif args.report:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if not args.quiet:
                print("ğŸ“Š ç”Ÿæˆä»Šæ—¥åˆ†ææŠ¥å‘Š...")
            
            report_path = manager.generate_daily_report()
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        elif args.use_root_words:
            # ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
            if not args.quiet:
                print("ğŸŒ± å¼€å§‹ä½¿ç”¨51ä¸ªè¯æ ¹è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
            
            result = manager.analyze_root_words(args.output)
            
            # æ˜¾ç¤ºç»“æœ
            if args.quiet:
                print_quiet_summary(result)
            else:
                print(f"\nğŸ‰ è¯æ ¹è¶‹åŠ¿åˆ†æå®Œæˆ! å…±åˆ†æ {result.get('total_root_words', 0)} ä¸ªè¯æ ¹")
                print(f"ğŸ“Š æˆåŠŸåˆ†æ: {result.get('successful_analyses', 0)} ä¸ª")
                print(f"ğŸ“ˆ ä¸Šå‡è¶‹åŠ¿è¯æ ¹: {len(result.get('top_trending_words', []))}")
                
                # æ˜¾ç¤ºTop 5è¯æ ¹
                top_words = result.get('top_trending_words', [])[:5]
                if top_words:
                    print("\nğŸ† Top 5 çƒ­é—¨è¯æ ¹:")
                    for i, word_data in enumerate(top_words, 1):
                        print(f"   {i}. {word_data['word']}: å¹³å‡å…´è¶£åº¦ {word_data['average_interest']:.1f}")
        
        else:
            # é»˜è®¤ï¼šæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            parser.print_help()
            return
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {args.output} ç›®å½•")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()